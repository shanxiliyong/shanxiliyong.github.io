{"meta":{"title":"李永的博客","subtitle":null,"description":null,"author":"李永","url":"https://liyong.ac.cn","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2017-05-07T01:34:43.000Z","updated":"2019-09-10T11:57:17.000Z","comments":false,"path":"/404.html","permalink":"https://liyong.ac.cn//404.html","excerpt":"","text":".article-header { padding: 0; padding-top: 26px; border-left: none; text-align: center; } .article-header:hover { border-left: none; } .article-title { font-size: 2.1em; } strong a { color: #747474; } .article-meta { display: none; } .share { display: none; } .ds-meta { display: none; } .player { margin-left: -10px; } .sign { text-align: right; font-style: italic; } #page-visit { display: none; } .center { text-align: center; height: 2.5em; font-weight: bold; } .article-entry hr { margin: 0; } .pic { text-align: center; margin: 0; } .pic br { display: none; } #container .article-info-post.article-info { display: none; } #container .article .article-title { padding: 0; }"},{"title":"关于我","date":"2019-09-10T12:31:37.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"about/index.html","permalink":"https://liyong.ac.cn/about/index.html","excerpt":"","text":"基本信息 热爱技术，热爱编码，IT界的郭靖。 联系方式 手机：18635581189邮箱：shanxi_liyong@163.com微信：18635581189（或者二维码） 我的社区 GitHub &nbsp;&nbsp;&nbsp;&nbsp; oschina 研究技术心得 悟性 学习 思考 坚持 英语，新技术必备 广度，技术栈需要铺的比较广 源码，深度学习的必要方式 深度，一门技术特别精通 实战，学以致用 学习之路 刚参加工作主要以见着拆招，搜索为主 搜索的多了，发现了一些大牛的博客，然后就膜拜大牛的博客 博客看多了，觉得还是不够系统，开始了看书之旅 在看书的同时，开始死磕源码，比如se 、mybatis 、zuul、apache commons、spring spring源码阅读之旅 读了三个多月以后，发现太难了，转战mybatis mybatis读懂以后，再战spring，spring mvc 算是看了个差不多，spring整体还是没有磕下来，之所以没有磕下来是由于我不知道需求，怎么能知道为什么这么写 开始看spirng官方文档，看了一段时间，发现英语太差 于是转战英语，可可英语、走遍美国、流利说，最终找到成年人学习英语的途径，就是背单词 官网也能看懂了，但是发现好多基础技术点吃的不透，还是无法深刻理解，这些技术点大致有 算法 jvm 在漫长的学习之旅中总结了一些方法论 大部分博客针对的是博主的痛点，博主理解了的那部分通常会被忽略，很难看到知识的全貌 书相对比较系统，但是通常比较滞后，通过看书可以快速理解知识的核心思想，但是和实际使用的知识可能有细微的出入 官网较书来说有版本的概念，所以实时性较好。 推荐先通过书、博客、视频教程等理解了知识的核心思想整体脉轮，再去官网校验准确性（英语不是很好，直接看官网很难理解核心思想）。 最好通过查看源码验证知识的准确性。博客、书、官网 都很难表达代码的逻辑 已读书单 《Spring Cloud微服务实战》 《微服务设计》 《实战Java高并发程序设计》 《深入理解Java虚拟机：JVM高级特性与最佳实践（第2版》 《高效团队开发工具与方法》 《大型网站技术架构》 《Spring攻略（第2版)》 《Oracle PL/SQL从入门到精通》 《深入分析Java Web技术内幕》 《火球:UML大战需求分析》 《JAVA核心技术（卷1）》 《Java与模式》 《经典Java EE企业应用实战》 《时间管理 如何充分利用你的24小时》 阅读中书单 《Spring实战(第4版)》 《Spring源码深度解析》 《Redis设计与实现》 《揭秘Java虚拟机》 《揭秘Java虚拟机》 《MySQL技术内幕》 划书单 《Netty实战》 《HTTP权威指南》 《Head First 设计模式》 《Maven实战》 《Java性能权威指南》 《Spring Boot实战JavaEE开发的颠覆者》 《性能之巅》 《数据结构与算法分析》 《Java并发编程实战》 《分布式服务框架》 《基于Oracle的SQL优化》 《Linux内核设计与实现(原书第3版)》 《Hadoop实战》 ​"},{"title":"IT123","date":"2019-09-10T12:31:37.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"it123/index.html","permalink":"https://liyong.ac.cn/it123/index.html","excerpt":"","text":"常用 掘金 开源中国 infoq 简书 并发编程网 博客园 segmentfault importnew stackoverflow The Central Repository MavenRepository github mkyong 教程 runoob 易百教程 慕课网 Linux命令大全 实验楼 前端 iviewui xw素材 极客标签 Html580 jquery-plugins 大牛 阮一峰 陈皓 廖学峰 咖啡兔 纯洁的微笑 deepinmind Harries skywang12345 翟永超 周立 brianway 优优码 不蒜子 阿里中间件团队 梁桂钊 泥瓦匠BYSocket cmsblogs 松然聊技术 芋道源码 框架 jodd oauth Disruptor akka apache curator 国外 javaworld jaxenter onjava javaspecialists Java Code Geeks dzone thoughts-on-java 在线算法 jcp.org leetcode wiki.openjdk asm ucla hannesdorfmann gee.cs.oswego vlkan hg.openjdk psy-lob-saw openjdk 金融 中投在线 析金 其他 pathfinding it-ebooks overapi devdocs oreilly"},{"title":"开源项目","date":"2019-09-13T06:36:50.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"projects/index.html","permalink":"https://liyong.ac.cn/projects/index.html","excerpt":"","text":"博客 TaleTale 使用了轻量级mvc框架 Blade 开发，默认主题使用了漂亮的 pinghsu演示站点：https://tale.biezhi.me CMS CicadasCMSspringboot+mybatis+beetl开发的一款CMS，支持自定义内容模型、模板标签、全站静态化等功能 电商 goshopJava分布式多店铺电商系统，使用技术：spring 、springmvc、mybatis、maven、html5、jquery、freemarker、Redis（缓存服务器）、Solr（搜索引擎）、Dubbo（调用系统服务）、Nginx（web服务器）、FastDFS（文件服务器）、Shiro（权限框架）、Zookeeper（分布式应用程序协调服务) 爬虫 YayCrawler分布式爬虫系统，简单使用，高级配置。可扩展，减轻开发量，能docker化，适应各种急切需求核心框架：WebMagic, Spring Boot ，MongoDB, ActiveMQ ,Spring + Quartz，Spring Jpa ， Druid，Redis， Ehcache ，SLF4J、Log4j2， Bootstrap + Jquery 等，不详细列举 未分类 jiraJIRA是集项目计划、任务分配、需求管理、错误跟踪于一体的商业软件。JIRA创建的问题类型包括New Feature、Bug、Task和Improvement四种，还可以自己定义，所以它也一是过程管理系统。Jira融合了项目管理、任务管理和缺陷管理，许多著名的开源项目都采用了JIRA。 confluenceAtlassian Confluence（简称Confluence）是一个专业的wiki程序。它是一个知识管理的工具，通过它可以实现团队成员之间的协作和知识共享 zheng基于Spring+SpringMVC+Mybatis分布式敏捷开发系统架构，提供整套公共微服务服务模块：集中权限管理（单点登录）、内容管理、支付中心、用户管理（支持第三方登录）、微信平台、存储系统、配置中心、日志分析、任务和通知等，支持服务治理、监控和追踪，努力为中小型企业打造全方位J2EE企业级开发解决方案。 AG-AdminAG-Admin是基于spring cloud统一管理授权、认证后台管理系统，具备用户管理、资源权限管理、网关API管理等多个模块，支持多业务系统并行开发，可以作为后台管理系统的脚手架。代码简洁，架构清晰，适合学习和直接项目中使用。核心技术采用Eureka、Fegin、Ribbon、Zuul、Hystrix、Security、JWT Token、Mybatis等主要框架和中间件，前端采用Layui组件 webbase业务系统框架,包括员工管理,组织机构,请假出差申请,流程审批,信息汇总,excel导出等基础功能"},{"title":"所有分类","date":"2019-09-13T07:43:29.000Z","updated":"2019-09-13T07:43:29.000Z","comments":true,"path":"blog/categories/index.html","permalink":"https://liyong.ac.cn/blog/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-09-13T07:43:29.000Z","updated":"2019-09-13T07:43:29.000Z","comments":true,"path":"blog/tags/index.html","permalink":"https://liyong.ac.cn/blog/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"aa_category/db/mysql/数据库常用架构方案","date":"2019-09-10T11:57:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/09/10/aa_category/db/mysql/数据库常用架构方案/","link":"","permalink":"https://liyong.ac.cn/2019/09/10/aa_category/db/mysql/数据库常用架构方案/","excerpt":"","text":"数据库架构原则 高可用 高性能 一致性 扩展性 常见的架构方案主备架构只有主库提供读写服务，备库冗余作故障转移用 分析 1、高可用分析： 高可用，主库挂了，keepalive（只是一种工具）会自动切换到备库。这个过程对业务层是透明的，无需修改代码或配置。 2、高性能分析： 读写都操作主库，很容易产生瓶颈。大部分互联网应用读多写少，读会先成为瓶颈，进而影响写性能。另外，备库只是单纯的备份，资源利用率50%，这点方案二可解决。 3、一致性分析： 读写都操作主库，不存在数据一致性问题。 4、扩展性分析： 无法通过加从库来扩展读性能，进而提高整体性能。 5、可落地分析：两点影响落地使用。第一，性能一般，这点可以通过建立高效的索引和引入缓存来增加读性能，进而提高性能。这也是通用的方案。第二，扩展性差，这点可以通过分库分表来扩展。 双主架构两个主库同时提供服务，负载均衡 分析1、高可用分析： 高可用，一个主库挂了，不影响另一台主库提供服务。这个过程对业务层是透明的，无需修改代码或配置。 2、高性能分析： 读写性能相比于方案一都得到提升，提升一倍。 3、一致性分析： 存在数据一致性问题。请看下面的一致性解决方案。 4、扩展性分析： 当然可以扩展成三主循环，但笔者不建议（会多一层数据同步，这样同步的时间会更长）。如果非得在数据库架构层面扩展的话，扩展为方案四。 5、可落地分析： 两点影响落地使用。第一，数据一致性问题，一致性解决方案可解决问题。第二，主键冲突问题，ID统一地由分布式ID生成服务来生成可解决问题。 主从架构一主多从，读写分离 分析高可用分析： 主库单点，从库高可用。一旦主库挂了，写服务也就无法提供。 2、高性能分析：大 部分互联网应用读多写少，读会先成为瓶颈，进而影响整体性能。读的性能提高了，整体性能也提高了。另外，主库可以不用索引，线上从库和线下从库也可以建立不同的索引（线上从库如果有多个还是要建立相同的索引，不然得不偿失；线下从库是平时开发人员排查线上问题时查的库，可以建更多的索引）。 3、一致性分析： 存在数据一致性问题。请看下面介绍的一致性解决方案。 4、扩展性分析： 可以通过加从库来扩展读性能，进而提高整体性能。（带来的问题是，从库越多需要从主库拉取binlog日志的端就越多，进而影响主库的性能，并且数据同步完成的时间也会更长） 5、可落地分析： 两点影响落地使用。第一，数据一致性问题，一致性解决方案可解决问题。第二，主库单点问题，笔者暂时没想到很好的解决方案。 双主+主从架构1、高可用分析： 高可用。 2、高性能分析： 高性能。 3、一致性分析： 存在数据一致性问题。请看，一致性解决方案。 4、扩展性分析： 可 以通过加从库来扩展读性能，进而提高整体性能。（带来的问题同方案二） 5、可落地分析： 同方案二，但数据同步又多了一层，数据延迟更严重。 一致性解决方案 直接忽略 强制读主 选择读主，写操作时根据库+表+业务特征生成一个key放到Cache里并设置超时时间（大于等于主从数据同步时间）。读请求时，同样的方式生成key先去查Cache，再判断是否命中。若命中，则读主库，否则读从库。代价是多了一次缓存读写，基本可以忽略","categories":[],"tags":[]},{"title":"","slug":"aa_category/review/project","date":"2019-09-10T11:57:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/09/10/aa_category/review/project/","link":"","permalink":"https://liyong.ac.cn/2019/09/10/aa_category/review/project/","excerpt":"","text":"自我介绍工作 计算机专业，2012年毕业，16年到北京。第一家公司赢时胜主要做金融产品TA.2018年来到了京东，职位为小组长。主要参与的项目有常旅客、网关、慧销、辅营。参与了部门从0到1的过程 特长，研究的比较深入的是jvm,mybatis,spring. 项目常旅客京东的第一个项目。打通机、酒、火等各个业务线之间的数据。使用hash,求模的方式分表。使用责任链加策略的方式校验参数的合法性。设计的失败，使用数据库配置的方式做一些校验，每次上线的担惊受怕。关键信息尽量用配置文件，不要用数据库配置。如果用数据库配置也要有版本的概念 网关主要实现了http转rpc协议。服务端、客户端。服务端功能有，json解析成map对象，降级，白名单、黑名单、mock、ab,泛化调用客户端的方法。客户端入参转换，方法调用，出参转换。 慧销召回活动​ 南京AI团队，把高潜用户发送到数据库。业务人员配置召回活动。到了召回触发时间分布式worker触发慧销系统。比如有两台机器，分布式worker,给A的参数是2个任务，任务号0，B的参数是2个任务，任务好1，考虑到不能重复发，采取取模的方式 给指定的用户发送短信和优惠券 营销活动​ 用户访问页面的时候根据用户活动以及配置的标签展示优惠券。不同的业务线可以设置不同的用户标标签。用户在指定时间访问A页面n次，访问B页面的时候展示优惠券。storm，和redis list 队列。key为用户标签，value为时间戳。判断30分钟访问3次。先看看队列大小是否够3，其次用当前时间 n-1 n-2 n-3 节点的时间是否小于等于30分钟。 缓存，使用二级缓存，接受一个请求，先看guaua是否命中，然后看redis，再看数据库。 幂等一致性 辅营简单的规则引擎 规则，组件类型，值类型，值之间的关系。规则项，编码，标签，权重，比较方式。编码，值，比较方式。规则和规则直接的关系为&amp; 基于自研的轻量级规则引擎实现的辅助营收平台。可选套餐，配置规则元数据，基于规则元数据建立活动，根据业务线， 根据入参和规则匹配活动，如果匹配展示相关券，在业务线下单。辅营监控业务线订单MQ,根据订单状态发放优惠券 指定规则根据业务线，查找活动，根据入参和规则匹配，如果匹配进行优惠券发放 可选套餐根据业务线，查找活动，根据入参和规则匹配，如果匹配展示优惠券，在业务线下单。辅营监控业务线订单MQ,根据订单状态发放优惠券 技术点 man端的某些统计功能使用到读写分离。 学习 思考、坚持、英语、广度、深度、实战、认识一些大牛（占小狼、翟永超、梁桂钊、小马哥、大R 苯神、江南白衣） 看源码，知道功能，猜怎么实现的，查看源码验证自己的想法 数据结构与算法之美、Linux性能优化实战、深入拆解Java虚拟机、MySQL实战 应该明确描述我们在项目中解决的问题，包括技术难题和业务难题 STAR法则，即在什么情况下(Situation)，面临一项什么样的任务(Task)，采取了什么样的行动(Action)，达到了怎样的结果(Result)。 我们应该主动将问题引导向自己擅长的方面 只讲了项目的功能，而不讲过程中遇到的业务难点和技术难点 表述复杂问题时应该从宏观到微观。先从比较高的层次入手，划分大的模块，确定模块间的交互，然后再逐个模块地细化细节 面试 初级应聘者，基础知识扎实 中级应聘者，基础知识扎实，熟练使用框架 高级应聘者，基础知识扎实，熟练使用框架，可以修改框架 项目复盘 明确项目的周期。 明确项目的价值。（为什么做这个项目，它解决了用户什么痛点，它带来什么价值？） 明确项目的功能。（这个项目涉及哪些功能？） 明确项目的技术。（这个项目用到哪些技术？） 明确个人在项目中的位置和作用。（你在这个项目的承担角色？） 明确项目的整体架构。 明确项目的优缺点,如果重新设计你会如何设计。 明确项目的亮点。（这个项目有什么亮点？） 明确技术成长。（你通过这个项目有哪些技术成长？）","categories":[],"tags":[]},{"title":"","slug":"aa_category/review/md","date":"2019-09-10T11:57:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/09/10/aa_category/review/md/","link":"","permalink":"https://liyong.ac.cn/2019/09/10/aa_category/review/md/","excerpt":"","text":"redisredis&amp;Memcached redis 支持复杂的数据结构 redis 原生支持集群模式 Redis支持数据的持久化 Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型 持久化 RDB周期性，适用于做冷备 fork子进程生成快照数据文件，如果文件大，客户端可能有暂停 AOF，每条命令作为日志，append-only写入日志文件中 通常会启用两种持久化，AOF作为恢复的第一选择，RDB作为冷备 如何使用缓存 为什么使用 高性能（执行时间），高并发 不良后果 缓存和数据库不一致 更新缓存代价比较大；懒加载思想，某一时间段，写多读少 先删，再更新，缓存为空（高并发下是旧）；先更新，再删除，缓存是旧的 雪崩，挂掉，大量key失效 前，高可用（哨兵，集群）；中，缓存+限流；后，持久化 击穿（不存在），穿透（热点key） 并发竞争，先获取锁，写之前判断当前value中的时间戳大于缓存里的时间戳 redis效率高 纯内存操作；基于非阻塞的IO多路复用机制；单线程避免了多线程的上下文切换问题 过期策略，内存淘汰机制 定时删除，惰性删除 allkeys-lru、volatile-lru、volatile-ttl LRU LRUCache&lt;K, V&gt; extends LinkedHashMap redis cluster 通信原理 集中式，读取更新，时效性好，存储有压力 gossip，更新有延时，分散存储压力 寻址算法。hash 算法；一致性 hash 算法；hash slot 算法 主备切换原理 sdown-主观宕机,odown客观宕机,如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机 哨兵 集群监控、消息通知、故障转移、配置中心 哨兵至少需要3个实例，哨兵+redis主从架构不保证数据零丢失 数据丢失，异步复制丢失，脑裂的丢失 至少有一个slave数据复制和同步的延迟不能超过10s slave 配置的自动纠正 slave-&gt;master 选举算法 master断开时间,优先级,offset,run id db分库分表 单表500万左右性能就开始差了 单库最高并发2000 迁移方案 晚上停机 双写，注意老数据覆盖新数据，一轮以后如果不一致，校验，针对不一致的再次写 orderId 模 32 = 库，orderId / 32 模 32 = 表 使用雪花算法 读写分离 主库将变更写入binlog,从库将主库的binglog写入自己的relay日志，执行日志中的语句 半同步，主写入binglog后强制将日志同步到从库，从库写入realy log以后返回ack给主库，至少一个从库ack之后才会认为写完成。 并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志 一个主库拆分成多个主库，代码控制，查主库 zkhttps://www.jianshu.com/p/2bceacd60b8a 场景,分布式协调,分布式锁,分布式队列,配置信息管理 协议,zab,Zookeeper Atomic Broadcast （Zookeeper原子广播） watcher机制 Watch是一次性的，每次都需要重新注册，并且客户端在会话异常结束时不会收到任何通知，而快速重连接时仍不影响接收通知 Watch的回调执行都是顺序执行的，并且客户端在没有收到关注数据的变化事件通知之前是不会看到最新的数据 Watch是轻量级的，WatchEvent是最小的通信单元，结构上只包含通知状态、事件类型和节点路径。 部署模式单机模式、伪集群模式、集群模式 集群角色：leader、Follower、observer。 集群最少要几台机器，集群规则是怎样的?集群规则为2N+1台，N&gt;0，即3台 集群如果有3台机器，挂掉一台集群还能工作吗？挂掉两台呢？集群需要一半以上的机器可用，所以，3台挂掉1台还能工作，2台不能 ZooKeeper保证的是CP zab 什么是Zab协议 Zookeeper Atomic Broadcast ,保证分布式事务的最终一致性 Zab 的作用 使用一个单一的主进程（Leader）来接收并处理客户端的事务请求； 保证一个全局的变更序列被顺序引用 当主进程出现异常的时候，整个zk集群依旧能正常工作 Zab协议原理; 发现：要求zookeeper集群必须选举出一个 Leader 进程，同时 Leader 会维护一个 Follower 可用客户端列表 同步：Leader 要负责将本身的数据与 Follower 完成同步，做到多副本存储。 广播：Leader 可以接受客户端新的事务Proposal请求，将新的Proposal请求广播给所有的 Follower。 协议内容 协议过程 崩溃恢复模式、消息广播模式 新加入的机子是崩溃恢复模式，等找到leader,完成数据同步后成为消息广播模式 消息有序；每一个事务请求转换成对应的 proposal （zxid）来进行广播，将每一个proposal按照其zxid的先后顺序进行排序和处理 Zab 崩溃恢复要求满足。确保已经被 Leader 提交的 Proposal 必须最终被所有的 Follower 服务器提交;确保丢弃已经被 Leader 提出的但是没有被提交的 Proposal 消息广播具体 客户端发起一个写操作请求 Leader 服务器将客户端的请求转化为事务 Proposal 提案 Leader 服务器为每个 Follower 服务器分配一个单独的队列，然后将需要广播的 Proposal 依次放到队列中取 Follower接收到 Proposal 后，以事务日志的方式写入本地磁盘，写入成功后向 Leader 反馈一个 Ack 响应消息 Leader 接收到超过半数以上 Follower 的 Ack 响应消息后，即认为消息发送成功，可以发送 commit 消息 Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交。Follower 接收到 commit 消息后，会将上一条事务提交 Zab 协议如何保证数据一致性 确保已经被 Leader 提交的 Proposal 必须最终被所有的 Follower 服务器提交 确保丢弃已经被 Leader 提出的但是没有被提交的 Proposal。 Zab 如何数据同步 完成 Leader 选举后,Leader 服务器会首先确认事务日志中的所有的 Proposal 是否已经被集群中过半的服务器 Commit Leader 服务器需要确保所有的 Follower 服务器能够接收到每一条事务的 Proposal ，并且能将所有已经提交的事务 Proposal 应用到内存数据中 Zab 数据同步过程中，如何处理需要丢弃的 Proposal（提出没有提交）？ leader和follow对比，比对的结果没有一半以上commit所以进行回退 Zab 的四个阶段？选举、发现、同步、广播 同步，follower会主动将自己最大的zxid发送给leader，leader会将follower的zxid与自身zxid间的所有被Commit过的消息同步给follower 广播,leader会向follower发送NEWLEADER命令并等待大多数服务器的ACK,然后向所有服务器广播UPTODATE命令。收到该命令后的服务器即可对外提供服务 未Commit过的消息对客户端不可见 B在成为Leader后，先判断自身未Commit的消息,是否存在于大多数服务器中从而决定是否要将其Commit dubbo工作原理 service,提供服务者和消费者实现 config proxy,生成consumer,和provide的代理层 registry,注册和发现 cluster,将多个实例组成一个服务，服务的路由及负载均衡 monitor protocal,远程调用层，封装rpc调用 exchange,信息交互层，封装请求和响应模式 transprot 层网络传输，抽象netty serialize,数据序列化 工作流程provide向注册中心注册；consumer在注册中心订阅服务； consume 调用provide dubbo 负载均衡策略 random ,roundrobin ,leastactive ,consistanthash 容错策略failover （读）、failfast （写）、failsafe （出现异常忽略，不重要的接口，记录日志）、failback（失败了自动记录请求，定时重发，写消息队列）、forking(并行调用多个provide,只要一个成功就成功)、broadcacst （逐个调用所有的provider） 降级使用hystrix Dubbo 的 RPC 框架 服务提供者去注册中心注册 -zk 消费者拿到服务信息 基于动态代理生成一个本地类，然后找服务对应的地址 找那台机子请求，负载算法 找到机子怎么发送 netty,发送格式 hesssion protobuf,msgpack 服务器那边一样的，需要针对你自己的服务生成一个动态代理，监听某个网络端口了，然后代理你本地的服务代码。接收到请求的时候，就调用对应的服务代码，对吧 hystrix调用fllback断路器打开 资源已满 接口异常 接口超时 资源隔离 线程池 支持超时 支持异步 开销大 网络访问 信号量 不支持超时 不支持异步 开销小 内部访问 超大并发量 经典降级方案 返回一个默认值 内存中维护一个cache,基于LRU自动清理的缓存，如果异常直接从缓存中获取 调用过程创建comand;调用comand;request cache;circuit breaker;线程池是否已满;执行command;断路健康检查（成功、失败、reject、timeout）; es架构 index -&gt; type -&gt; mapping(表的定义) -&gt; document（一行数据） -&gt; field（一个字段的值） 一个index对应多个shard.每个shard存储部分数据。shard数据有多个备份。primary_shard（写入）、replica_shard. master管理工作。维护索引元数据，切换primary_shard和replica_shard身份。master挂机了，重新选举一个master节点 非master宕机，宕机的primary_shard身份切换到其他replica_shard上。重启以后，修复后的节点也不再是 primary shard，而是 replica shard 写过程 客户端给 coordinating node发送请求 coordinating node 对 document 进行路由，将请求转发给对应primary shard primary shard 处理请求，然后将数据同步到 replica node coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端 读过程 客户端给 coordinating node发送请求 coordinate node 对 doc id 进行哈希路由，得到所在的shard，然后使负载算法确定node 接收请求的 node 返回 document 给 coordinate node,oordinate node 返回 document 给客户端 es 搜索过程 客户端发送请求到一个 coordinate node 协调节点将搜索请求转发到所有的 shard query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果 fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端 提高查询效率 filesystem cache es+hbase 先用es按照非主键在es中搜索，然后根据主键在hbas中查找 数据预热 冷热分离，冷数据写入一个索引中，然后热数据写入另外一个索引中 document 模型设计。不要做复杂的查询；关联查询，将关联好的塑胶写入es中 分页，越到后面越慢;不允许深度分页;scroll api/search_after mq 为什么使用消息队列?解耦、异步、削峰 消息队列有什么优点和缺点?可用性降级，系统复杂度提高，一致性问题 Kafka、ActiveMQ、RabbitMQ、RocketMQ都有什么区别，以及适合哪些场景。 ActiveMQ用的不多了，没经过大规模吞吐量场景的验证 RabbitMQ， erlang 语言阻止 Java 工程师去深入研究，对公司而言，几乎处于不可控的状态 RocketMQ，毕竟是阿里出品，但社区可能有突然黄掉的风险对自己公司技术实力有绝对自信的，推荐用 如何保证消息队列的高可用？ RabbitMQ 的高可用性。单机模式；普通集群模式，元数据同步到多个实例上；镜像集群模式（高可用性），元数据和queue存在多个实例上 Kafka 的高可用性。天然的分布式消息队列；写数据的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者；消费的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到 如何保证消息不被重复消费。消息有唯一id;先根据主键查一下，如果这数据都有了，你就别插入了 如何处理消息丢失的问题. 生产者弄丢了数据.RabbitMQ .confirm异步，事务机制同步；一般使用confirm RabbitMQ 弄丢了数据。开启持久化+confirm.持久化以后发送ack确认数据。 消费端弄丢了数据。RabbitMQ关闭自动ack,手动调用api; 如何保证消息的顺序性. 拆分多个 queue，每个 queue 一个 consumer 一个 大queue是对应一个 consumer，然后这个 consumer 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；N 个线程，每个线程分别消费一个内存 queue 即可 大量消息在 mq 里积压了几个小时了还没解决 申请空间是之前10倍的queue；写个临时couseme程序，消费积压消息，写入临时queue;当程序修复后用10倍的机器消费临时queue. mq 中的消息过期失效了.如果丢失的数据能找出来，晚上重新灌入mq mq 都快写满了。临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后，到了晚上再补数据吧 如果让你写一个消息队列，该如何进行架构设计？ 伸缩性.broker -&gt; topic -&gt; partition，每个 partition 放一个机器，就存一部分数据. 数据落地磁盘.顺序写,这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的 可用性.多副本 -&gt; leader &amp; follower -&gt; broker 挂了重新选举 leader 即可对外服务 使用场景、缺点、重复、丢失、有序、高可用 积压、写满、过期 设计 杂分布式锁 数据库乐观锁 redis SET my:lock 随机值 NX PX 30000 或者回字型lua脚本删除，需要自己不断去尝试获取锁，比较消耗性能 redis RedLock .如果一台机子断电重启，启动以后不要对外提供RedLock的功能 zk 分析 总结高可用 冗余 es，备份shard redis哨兵+主从 RabbitMQ镜像集群模式通过全量备份，kafka通过备份partition zk,通过leader,follow 异步 zk,通过队列 高并发 hash es doc id hash kafka通过hash决定路由哪个partition redis 一致性hash 少数服从多数 redis 客观宕机 redis的redlock zk选举","categories":[],"tags":[]},{"title":"","slug":"aa_category/front/vue/vue入门","date":"2019-09-10T11:57:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/09/10/aa_category/front/vue/vue入门/","link":"","permalink":"https://liyong.ac.cn/2019/09/10/aa_category/front/vue/vue入门/","excerpt":"","text":"Vue入门axios处理http请求 https://www.npmjs.com/package/axios VueRouter路由处理 官网https://router.vuejs.org/en/ webpack处理模块化 Vuex解决组件之间数据共享的问题","categories":[],"tags":[]},{"title":"","slug":"aa_category/se/深入拆解 Java 虚拟机/14Java虚拟机是如何实现Synchronzide","date":"2019-09-10T11:57:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/09/10/aa_category/se/深入拆解 Java 虚拟机/14Java虚拟机是如何实现Synchronzide/","link":"","permalink":"https://liyong.ac.cn/2019/09/10/aa_category/se/深入拆解 Java 虚拟机/14Java虚拟机是如何实现Synchronzide/","excerpt":"title: 14Java虚拟机是如何实现Synchronzidedate: 2019-03-11 10:08:22tags: [se,jvm]categories: setoc: true—​ monitorenter 和 monitorexit 的作用，我们可以抽象地理解为每个锁对象拥有一个锁计数器和一个指向持有该锁的线程的指针。 当执行 monitorenter 时，如果目标锁对象的计数器为 0，那么说明它没有被其他线程所持有。在这个情况下，Java 虚拟机会将该锁对象的持有线程设置为当前线程，并且将其计数器加 1。 在目标锁对象的计数器不为 0 的情况下，如果锁对象的持有线程是当前线程，那么 Java 虚拟机可以将其计数器加 1，否则需要等待，直至持有线程释放该锁。 当执行 monitorexit 时，Java 虚拟机则需将锁对象的计数器减 1。当计数器减为 0 时，那便代表该锁已经被释放掉了。 之所以采用这种计数器的方式，是为了允许同一个线程重复获取同一把锁。举个例子，如果一个 Java 类中拥有多个 synchronized 方法，那么这些方法之间的相互调用，不管是直接的还是间接的，都会涉及对同一把锁的重复加锁操作。因此，我们需要设计这么一个可重入的特性，来避免编程里的隐式约束。","text":"title: 14Java虚拟机是如何实现Synchronzidedate: 2019-03-11 10:08:22tags: [se,jvm]categories: setoc: true—​ monitorenter 和 monitorexit 的作用，我们可以抽象地理解为每个锁对象拥有一个锁计数器和一个指向持有该锁的线程的指针。 当执行 monitorenter 时，如果目标锁对象的计数器为 0，那么说明它没有被其他线程所持有。在这个情况下，Java 虚拟机会将该锁对象的持有线程设置为当前线程，并且将其计数器加 1。 在目标锁对象的计数器不为 0 的情况下，如果锁对象的持有线程是当前线程，那么 Java 虚拟机可以将其计数器加 1，否则需要等待，直至持有线程释放该锁。 当执行 monitorexit 时，Java 虚拟机则需将锁对象的计数器减 1。当计数器减为 0 时，那便代表该锁已经被释放掉了。 之所以采用这种计数器的方式，是为了允许同一个线程重复获取同一把锁。举个例子，如果一个 Java 类中拥有多个 synchronized 方法，那么这些方法之间的相互调用，不管是直接的还是间接的，都会涉及对同一把锁的重复加锁操作。因此，我们需要设计这么一个可重入的特性，来避免编程里的隐式约束。 重量级锁重量级锁是 Java 虚拟机中最为基础的锁实现。在这种状态下，Java 虚拟机会阻塞加锁失败的线程，并且在目标锁被释放的时候，唤醒这些线程。 Java 线程的阻塞以及唤醒，都是依靠操作系统来完成的。举例来说，对于符合 posix 接口的操作系统（如 macOS 和绝大部分的 Linux），上述操作是通过 pthread 的互斥锁（mutex）来实现的。此外，这些操作将涉及系统调用，需要从操作系统的用户态切换至内核态，其开销非常之大。 轻量级锁获取轻量锁的过程与偏向锁不同，竞争锁的线程首先需要拷贝对象头中的Mark Word到帧栈的锁记录中。拷贝成功后使用CAS操作尝试将对象的Mark Word更新为指向当前线程的指针。如果这个更新动作成功了，那么这个线程就拥有了该对象的锁。如果更新失败，那么意味着有多个线程在竞争。 当竞争线程尝试占用轻量级锁失败多次之后（使用自旋）轻量级锁就会膨胀为重量级锁，重量级线程指针指向竞争线程，竞争线程也会阻塞，等待轻量级线程释放锁后唤醒他。 偏向锁引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令。当只有一个线程去竞争锁的时候，我们不需要阻塞，也不需要自旋，因为只有一个线程在竞争，我们只要去判断该偏向锁中的ThreadID是否为当前线程即可。如果是就执行同步代码，不是就尝试使用CAS修改ThreadID，修改成功执行同步代码，不成功就将偏向锁升级成轻量锁。 参考 https://wiki.openjdk.java.net/display/HotSpot/Synchronization","categories":[],"tags":[]},{"title":"","slug":"aa_category/review/汇总","date":"2019-09-10T11:57:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/09/10/aa_category/review/汇总/","link":"","permalink":"https://liyong.ac.cn/2019/09/10/aa_category/review/汇总/","excerpt":"","text":"基础篇 JDBC 流程 加载驱动、获取连接、创建statement、设置sql语句传入参数、执行sql语句、处理返回结果、释放链接 语法糖 switch 支持 String 与枚举、自动装箱与拆箱、变长参数、枚举类 集合 Arraylist （get/set）与 LinkedList (add/remove) ConcurrentHashMap 的工作原理及代码实现。 对于put操作，如果Key对应的数组元素为null，则通过CAS操作将其设置为当前值。如果Key对应的数组元素（也即链表表头或者树的根元素）不为null，则对该元素使用synchronized关键字申请锁，然后进行操作。如果该put操作使得当前链表长度超过一定阈值，则将该链表转换为树，从而提高寻址效率。 线程 sleep() 、join（）、yield（）有什么区别 说说 CountDownLatch 原理,内部维护了一个整数n,await() 方法阻塞当前线,countDown执行一次n减1 说说 CyclicBarrier 原理，每当线程执行await，内部变量count减1，如果count！= 0，说明有线程还未到屏障处，则在锁条件变量trip上等待。当count == 0时，说明所有线程都已经到屏障处，执行条件变量的signalAll方法唤醒等待的线程 说说 Semaphore 原理 线程的生命周期;新建,就绪;运行;阻塞;死亡 锁机制 volatile 实现原理 ,内存屏障，缓存一致性协议 synchronized 与 lock 的区别.隐式锁,显式锁;悲观,乐观 CAS 乐观锁,内存位置（V）、预期原值（A）和新值(B) 核心篇数据存储 MySQL 索引使用的注意事项 列上函数和运算;否定操作符;or;最左前缀；覆盖索引； 分库与分表带来的分布式困境与应对之策 分表。垂直，不常用的字段在一张表，大文本一张表，关联查询的字段一张表 MySQL 遇到的死锁问题 大事务拆小；添加合理索引；尽可能一次锁定资源； 业务日志，是否有回滚；找dba查看最近死锁的记录 为什么要用 B+tree 索引大存储到磁盘上，查找的时候尽量减少磁盘io次数。预读，一次只读一页。 选择合适的数据存储方案 es全文搜索代替like;mongodb高伸缩性，日志系统 倒排索引，搜索引擎用到，根据歌词搜索歌名 框架篇Spring BeanFactory 和 ApplicationContext 有什么区别 Spring 事务底层原理，connection,datasource,transaction Spring MVC 执行流程 DispatcherServlet收到请求调用HandlerMapping HandlerMapping根据请求url找到具体的处理器，生成HandlerExecutionChain(包括处理器对象和处理器拦截器)一并返回给DispatcherServlet DispatcherServlet根据处理器Handler获取处理器适配器HandlerAdapter执行HandlerAdapter处理一系列的操作，如：参数封装，数据格式转换，数据验证等操作 执行处理器Handler，前置处理，执行，后置处理 Handler执行完成返回ModelAndView HandlerAdapter将Handler执行结果ModelAndView返回到DispatcherServlet DispatcherServlet将ModelAndView传给ViewReslover视图解析器 ViewReslover解析后返回具体View DispatcherServlet对View进行渲染视图（即将模型数据model填充至视图中 DispatcherServlet响应用户。 Spring 框架中用到了哪些设计模式。工厂，单例，责任链，代理-aop，观察者-listener,策略 Netty 为什么选择 Netty。相比原生nio,简洁易用，相比mina,文档清晰，更新周期短，使用起来简单 说说业务中，Netty 的使用场景。jsf 原生的 NIO 在 JDK 1.7 版本存在 epoll bug. Selector 空轮询，最终导致 CPU 100% 什么是TCP 粘包/拆包.缓存区的大小是一定的如果小于缓存去就行粘包，大于进行拆包 TCP粘包/拆包的解决办法，消息头的协议，定长消息，消息边界 Netty 线程模型。https://www.jianshu.com/p/38b56531565d 网络处理accept、read、decode、process、encode、send 事件驱动，Task对应特定网络事件 角色。Reactor：负责响应事件，将事件分发给绑定了该事件的Handler处理；Handler：事件处理器；Acceptor：Handler的一种，绑定了connect事件 单线程Reactor；多线程Reactor；主从多线程；netty线程模型多线程Reactor模型类似。 说说 Netty 的零拷贝。传统的零拷贝数据在用户空间与内核中间之间的拷贝；使用传统的堆内存分配，当我们需要将数据通过socket发送的时候，就需要从堆内存拷贝到直接内存，然后再由直接内存拷贝到网卡接口层；Composite Buffers，只是保存两个buffer的引用 linux 上下文切换？ 上下文，寄存器、程序计数器 线程是调度单位，进程是资源拥有单位 分片执行、资源不足、优先级高的抢占、硬件中断 vmstat mysql 优化 索引，不要再列上进行运算，不用not in sql优化，不用*，用连接替换子查询，join小表驱动大表，limit的offset不要过大 表，字段尽可能 not null,分表 微服务篇微服务 说说如何设计一个良好的 API，版本号，正常返回，异常返回 说说 CAP 定理、 BASE 理论;基本可用，软状态，最终一致 微服务与 SOA 的区别，服务总线，服务治理；单体架构，功能独立 安全问题 防范常见的 Web 攻击，XSS攻击（输入恶意的脚本），sql注入，CSRF攻击（） HTTPS 原理剖析 客户端输入SSL 3.0 加密协议； 服务端返回证书 客户端解析证书，生成随机数，加密发送 服务端用私钥进行解密随机数 服务端用私钥进行加密随机数发送给客户端 客户端解密信息 HTTPS 降级攻击 利用SSL 3.0 漏洞，获取加密后的明文内容；因为兼容性问题，当浏览器进行 HTTPS 连接失败的时候会降级。禁用SSL 3.0 加密协议 工程篇需求分析 你如何对需求原型进行理解和拆分 说说你对功能性需求的理解。业务需求转化成er图 说说你对非功能性需求的理解。安全，可靠性，可移植性、可重用性，可扩展性 你针对产品提出哪些交互和改进意见。过度追求精准，死板遵循设计法则，过度假设用户行为，过多展示内在业务逻辑，过多展示内在业务逻辑，过于把用户当「小白」 设计能力 说说你在项目中使用过的 UML 图，活动图，序列图，部署图 说说概要设计 设计模式 你项目中有使用哪些设计模式。构建，责任链，代理，策略，单例， 说说常用开源框架中设计模式使用分析。缓存使用装饰模式，mapperbuiler构建模式，SqlSessionFactory工厂，日志适配模式，plugin代理模式，拦截器责任链 说说你对设计原则的理解。单一，开闭，里氏代换，依赖倒转，接口隔离，合成复用，迪米特 业务工程 你和团队是如何沟通的 说说你对技术与业务的理解 说说你在项目中经常遇到的 Exception。ClassCastException、ClassNotFoundException、IllegalAccessException、NullPointerException、IndexOutOfBoundsException 说说你在项目中遇到印象最深困难，怎么解决的 你觉得你们项目还有哪些不足的地方。 你是否遇到过 CPU 100% ，如何排查与解决 进程，线程，查看线程堆栈信息，定位问题代码 你是否遇到过 内存 OOM ，如何排查与解决，HeapDumpOnOutOfMemoryError 介绍下工作中的一个对自己最有价值的项目，以及在这个过程中的角色 软实力 说说你的亮点。专业技能，发展潜力，软实力 说说你觉得最有意义的技术书籍。深入理解java虚拟机 说说个人发展方向方面的思考。架构师，技术专家。 说说你认为的服务端开发工程师应该具备哪些能力。初级可以写简单的业务代码，中级可以写复杂的业务代码，高级写公用性的底层代码 架构师主要做什么？架构师需要参与项目开发的全部过程，包括需求分析、架构设计、系统实现、集成、测试和部署各个阶段，负责在整个项目中对技术活动和技术说明进行指导和协调 调优 Native OOM最好的方法就是用google perftools java 内存mat +btrace HR 篇 谈一谈你的一次失败经历。分表hash 你觉得你最大的缺点是什么。不是名校 你觉得自己那方面能力最急需提高 你来我们公司最希望得到什么，你希望从这份工作中获得什么 自己学习的东西能落地，认识一些大牛， 您还有什么想问的，你对现在应聘的职位有什么了解 有哪些需要继续提升的 工作内容，参与什么项目，直接领导 主要用哪些技术 我如果能来，是什么角色 咱们部门的规划是？ 未来如果我要加入这个团队，你对我的期望是什么 团队现在面临的最大挑战是什么 你怎么看待自己的职涯 你有什么业余爱好。运动类 方案篇秒杀 配置静态页面 静态资源(JS,CSS,图片)刷新到CDN 服务器上进行熔断，限流，降级， 数据库使用乐观锁，比悲观锁性能好 reids，单个key的流量不能超过单台缓存服务器的流量，单个key并发访问量*对应value的大小&lt;单台缓存服务器的流量限制 分布式事务两阶段提交（2PC）、补偿事务（TCC） 分布式ID UUID;优点，性能高；缺点，太长不易于存储，信息不安全（造成MAC地址泄露） 数据库生成；优点，简单。缺点，性能限制在单台mysql的性能 redis;优点，缺点，需要引入redis zookeeper主要通过其znode数据版本来生成序列号 雪花算法，时间戳，机器码，序列号 杂在浏览器中输入url地址到显示主页的过程,整个过程会使用哪些协议 DNS解析，TCP连接，发送HTTP请求，服务器处理请求并返回HTTP报文，浏览器解析渲染页面，连接结束 架构模式分层、分割、分布式、集群、缓存、异步、冗余、自动化、安全。 缓存，CDN,反向代理，本地缓存，分布式缓存 架构要素性能、可用性、伸缩性、扩展性、安全性 高性能架构性能测试 测试指标；响应时间，吞吐量，错误率，并发数，资源利用率 基准测试，负载测试，压力测试，并发测试，稳定性测试 性能优化 web前端 浏览器，减少请求，浏览器缓存，压缩，减少cookie cdn加速 反向代理 页面静态化 服务器 缓存，本地缓存，分布式缓存 异步操作 集群 代码优化，sql调优 存储优化 机械硬盘vs固态硬盘 高可用架构 高可用的架构 硬件故障是常态，软件高可用主要是保障硬件故障软件依然可用 高可用架构的手段冗余备份，故障转移 高可用的应用，通过负载均衡进行无状态服务端的失效转移 高可用的服务 分级管理，核心服务用好的硬件 异步调用，服务降级、幂等性 高可用的数据 cap原理 数据备份，冷备，热备 失效转移 监控，监控数据收集，监控管理 资料 https://zhuanlan.zhihu.com/p/31250861 https://www.funtl.com/zh/interview/ https://github.com/Snailclimb/JavaGuide/blob/master/docs/essential-content-for-interview/PreparingForInterview/%E7%BE%8E%E5%9B%A2%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.md","categories":[],"tags":[]},{"title":"","slug":"aa_category/middleware/redis/安装","date":"2019-09-10T11:57:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/09/10/aa_category/middleware/redis/安装/","link":"","permalink":"https://liyong.ac.cn/2019/09/10/aa_category/middleware/redis/安装/","excerpt":"","text":"The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128 net.core.somaxconn= 1024 然后执行sysctl -p","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-09-10T11:57:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/09/10/hello-world/","link":"","permalink":"https://liyong.ac.cn/2019/09/10/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"","slug":"aa_category/review/algorithm","date":"2019-09-10T11:57:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/09/10/aa_category/review/algorithm/","link":"","permalink":"https://liyong.ac.cn/2019/09/10/aa_category/review/algorithm/","excerpt":"","text":"连接https://github.com/CyC2018/CS-Notes/blob/master/notes/Leetcode%20%E9%A2%98%E8%A7%A3%20-%20%E7%9B%AE%E5%BD%95.md LeetCodeAnimationhttps://github.com/MisterBooo/LeetCodeAnimation?utm_source=gold_browser_extension 序号 题目&amp;题解 掌握 思 0 十大经典排序算法动画与解析，看我就够了！（配代码完全版） 冒泡、选择、插入、希尔、归并、快速、堆排序、计数、桶、基数。 1 两数之和 通过map来存储，key是数组的值，value是数组的下标 2 两数相加 value = (a + b + carry) % 10，carry= (a + b + carry) / 10 3 无重复字符的最长子串 int freq[256] = {0}; int l = 0, r = -1; //滑动窗口为s[l…r]; int res = 0; 9 回文数 div int left = x / div; int right = x % 10; 15 三数之和 三数之和转变成两数之和 19 删除链表的倒数第 N 个节点 end移动n+1;pre,end移动直至end==null 20 有效的括号 栈 21 合并两个有序链表 哨兵 23 合并 K 个排序链表 链表排序，优先级队列，分治 24 两两交换链表中的节点 放后 26 删除排序数组中的重复项 快慢指针 66 加一 和9做比较 a[i]++ 75 颜色分类 三路快速排序,计数法 86 分割链表 拆分成两个链表，合并 92 反转链表 II 94 二叉树的中序遍历 栈 101 对称二叉树 递归 102 二叉树的层序遍历 队列 103 二叉树的锯齿形层次遍历 放后 107 二叉树的层次遍历 II 类似二叉树的右视图 118 杨辉三角 1，11,121,1331 119 杨辉三角II 放后 110 平衡二叉树 递归 121 买卖股票的最佳时机 122 买卖股票的最佳时机II 忽略 123 买卖股票的最佳时机III 忽略 125 验证回文串 131 分割回文串 放后 136 只出现一次的数字 异或运算 138 复制带随机指针 139 单词拆分 放后 144 二叉树的前序遍历 栈，右，左。插入集合末尾 145 二叉树的后序遍历 栈，左，右。插入集合头 146 LRU缓存机制 LinkedHashMap，initialCapacity，loadFactor，accessOrde 150 逆波兰表达式求值 放后 167 两数之和 II - 输入有序数组 双指针 172 阶乘后的零 n == 0 ? 0 : n / 5 + trailingZeroes(n / 5); 187 重复的 DNA 序列 放后 191 位1的个数 count += n &amp; 1; n = n &gt;&gt; 1; 199 二叉树的右视图 201 数字范围按位与 203 移除链表元素 206 反转链表 209 长度最小的子数组 219 存在重复元素 II 231 2的幂 n&amp;(n-1)==0 237 删除链表中的节点 239 滑动窗口最大值 忽略 268 缺失数字 异或运算 279 完全平方数 放后 283 移动零 a[i]!=0 a[k++]=a[i]; 295 数据流的中位数 忽略 301 删除无效的括号 忽略 326 3 的幂 3 的幂次的质因子只有 3 328 奇偶链表 342 4的幂 344 反转字符串 349 两个数组的交集 350 两个数组的交集 II 445 两数相加 II 双指针 447 回旋镖的数量 放后 454 四数相加 II 忽略 642 设计一个搜索自动完成系统 忽略 690 员工的重要性 HashMap 队列 广度优先 877 石子游戏 忽略","categories":[],"tags":[]},{"title":"","slug":"aa_category/review/se","date":"2019-09-10T11:57:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/09/10/aa_category/review/se/","link":"","permalink":"https://liyong.ac.cn/2019/09/10/aa_category/review/se/","excerpt":"","text":"w https://blog.csdn.net/zhangsanfeng2009/article/details/81001567 https://www.oracle.com/technetwork/java/javase/tech/memorymanagement-whitepaper-1-150020.pdf https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&amp;mid=2247484877&amp;idx=1&amp;sn=f54d41b68f0cd6cc7c0348a2fddbda9f&amp;chksm=cea24a06f9d5c3102bfef946ba6c7cc5df9a503ccb14b9b141c54e179617e4923c260c0b0a01&amp;token=1082669959&amp;lang=zh_CN&amp;scene=21#wechat_redirect https://www.jianshu.com/p/0df3d5641c1c 线程池相关https://www.jianshu.com/p/6c6f396fc88e 单机上一个线程池正在处理服务，如果忽然断电了怎么办（正在处理和阻塞队列里的请求怎么处理）？ 说说几种常见的线程池及使用场景 newSingleThreadExecutor newFixedThreadPool 堆积的请求处理队列可能会耗费非常大的内存，甚至OOM newCachedThreadPool newScheduledThreadPool 线程数最大数是Integer.MAX_VALUE 线程池都有哪几种工作队列 ArrayBlockingQueue LinkedBlockingQueue SynchronousQueue PriorityBlockingQueue jdk8移除了PermGen，取而代之的是MetaSpace 1、字符串存在永久代中，容易出现性能问题和内存溢出。 2、类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。 3、永久代会为 GC 带来不必要的复杂度，并且回收效率偏低 ThreadLocal使用原理、注意问题、使用场景 java.lang.Thread#threadLocals 脏数据 内存泄漏 读写分离上下文 日志traceid 为什么要把堆和栈区分出来呢？栈中不是也可以存储数据吗？ 栈是运行时的单位 , 而堆是存储的单元 栈解决程序的运行问题，即程序如何执行，或者说如何处理数据 堆解决的是数据存储的问题，即数据怎么放，放在哪儿 一个空Object对象的占多大空间 64位虚拟机还是32位，如果是64位还要看有没有开启CompressedOops，还要看 ObjectAlignmentInBytes Java中，栈的大小通过什么参数来设置 -Xss -XX:ThreadStackSize 对象引用类型分为哪几类 强引用、软引用、弱引用和虚引用 垃圾回收算法 标记-清除 复制 标记-整理 如何解决内存碎片的问题 为什么要分代 不同的对象的生命周期是不一样的 不同声明周期的对象可以采取不同的收集方式，以便提高回收效率 什么情况下触发垃圾回收 young GC：eden区分配满的时候触发 full GC ：调用System.gc时；老年代空间不足；方法区空间不足； 如何选择合适的垃圾收集算法 https://www.cnblogs.com/cxxjohnson/p/8625713.html Serial收，复制，单线程，暂停 ParNew收集器，Serial收集器的多线程版本 Parallel Scavenge，吞吐量优先，复制算法 Serial Old,标记-整理”算法（Mark-Sweep-Compact） Parallel Old,标记-整理（Mark-Sweep-Compact） CMS,标记-清除,并发收集、低停顿 对 CPU 资源敏感，无法处理浮动垃圾，会产生空间碎片 foreground collector，遇到对象分配但空间不够，就会直接触发 background collector，默认时间间隔2s GC cause 是 gclocker 且配置了 GCLockerInvokesConcurrent ;GC cause 是javalangsystemgc（就是 System.gc()调用）and 且配置了 ExplicitGCInvokesConcurrent 根据统计数据动态计算 未配置 UseCMSInitiatingOccupancyOnly 时，会根据统计数据动态判断是否需要进行一次 CMS GC。判断逻辑是，如果预测 CMS GC 完成所需要的时间大于预计的老年代将要填满的时间，则进行 GC，第一次根据Old Gen 的使用占比（50%）来判断是否要进行 GC 根据 Old Gen 情况判断 根据增量 GC 是否可能会失败 根据 meta space 情况判断 GC过程 初始标记、并发标记 预清理，处理并发标记期间的新生代晋升老年代、直接进入老年代、更新老年代的引用关系 可中断的预清理，这个阶段其实跟上一个阶段做的东西一样，也是为了减少下一个STW重新标记阶段的工作量。增加这一阶段是为了让我们可以控制这个阶段的结束时机，比如扫描多长时间（默认5秒）或者Eden区使用占比达到期望比例（默认50%）就结束本阶段 并发清理、并发重置 G1收集器。 G1把内存分成一块块的Region，每块的Region的大小都是一样的。 G1保留了YGC并加上了一种全新的MIXGC用于收集老年代，G1中的Full GC是采用serial old Full GC。在MIXGC中的Cset是选定所有young gen里的region，外加根据global concurrent marking统计得出收集收益高的若干old gen region。在YGC中的Cset是选定所有young gen里的region。通过控制young gen的region个数来控制young GC的开销。YGC与MIXGC都是采用多线程复制清除，整个过程会STW。 G1的低延迟原理在于其回收的区域变得精确并且范围变小了 全局并发标记分的五个阶段 用STAB来维持并发GC的准确性 当一个对象大于Region大小的50%，称为巨型对象 针对于拥有多核处理器和大内存的机器 RSet记录了其他Region中的对象引用本Region中对象的关系 伪共享 缓存是以缓存行(Cache Line)为单位的。一般缓存行的大小是64字节。小于64字节的变量，是有可能存在于同一条缓存行的。例如变量X大小32字节，变量Y大小32字节，那么他们有可能会存在于一条缓存行上。 多个线程同时修改共享在同一个缓存行里的独立变量，无意中影响了性能 Busy spin Busy spin 是一种在不释放 CPU 的基础上等待事件的技术。它经常用于避免丢失 CPU 缓存中的数据 能创建一个包含可变对象的不可变对象吗 是的。不要共享可变对象的引用就可以了，如果返回可变对象，就返回原对象的一个拷贝 JVM 出现 fullGC 很频繁 触发fullgc的条件 jmap mat 单摘出一台机子做测试 jvm中一次完整的GC流程（从ygc到fgc）是怎样的 对象优先在新生代区中分配，若没有足够 空间，Minor GC 大对象（需要大量连续内存空间）直接进入老年态 长期存活的对象进入老年 如果新生代的大小，小于则不进行fullgc 如果新生代的大小，大于则看是否有担保，如果有看老年代连续大小是否大于历次晋升到老年代的平均大小 废弃的类 classloader class instance 内存溢出 java.lang.OutOfMemoryError: Java heap space 增加jvm的内存大小 优化程序，释放垃圾 内存中加载的数据量过于庞大，导出功能；集合类中有对对象的引用，使用完后未清空；或循环产生过多重复的对象实体；第三方软件的bug；启动参数内存值设定的过小 StackOverflowError 栈深度。通过代码限制深度，通常是代码问题 OutOfMemoryError 适当减小栈的深度 类加载机制 加载-&gt;连接（验证、准备、解析）-&gt;初始化-&gt;使用-&gt;卸载 SpringBeanFactory 和 FactoryBean application的生命周期 BeanFactoryPostProcessor BeanPostProcessor finishBeanFactoryInitialization springbean 的生命周期 创建bean 设置属性 aware postProcessBeforeInitialization init postProcessAfterInitialization","categories":[],"tags":[]},{"title":"GC概述","slug":"aa_category/se/gc/概述","date":"2019-06-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/06/11/aa_category/se/gc/概述/","link":"","permalink":"https://liyong.ac.cn/2019/06/11/aa_category/se/gc/概述/","excerpt":"查看垃圾回收器12345java -XX:+PrintCommandLineFlags -version-XX:InitialHeapSize=126504128 -XX:MaxHeapSize=2024066048 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGCjava version \"1.8.0_151\"Java(TM) SE Runtime Environment (build 1.8.0_151-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode) -XX:+UseParallelGC 新生代使用ParallerGC","text":"查看垃圾回收器12345java -XX:+PrintCommandLineFlags -version-XX:InitialHeapSize=126504128 -XX:MaxHeapSize=2024066048 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGCjava version \"1.8.0_151\"Java(TM) SE Runtime Environment (build 1.8.0_151-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode) -XX:+UseParallelGC 新生代使用ParallerGC 强制fullgc1jmap -histo:live &lt;pid&gt; 打印gc日志jinfo -flag UseParallelGC 参数https://blog.codecentric.de/en/2014/01/useful-jvm-flags-part-8-gc-logging/ PrintGCEnables printing of messages at every GC. By default, this option is disabled. 123[GC (Allocation Failure) 31143K-&gt;28042K(119808K), 0.0420462 secs][GC (Allocation Failure) 59255K-&gt;59202K(151040K), 0.0149887 secs][Full GC (Ergonomics) 59202K-&gt;59132K(196608K), 0.0178210 secs] PrintGCDetails123[GC (Allocation Failure) [PSYoungGen: 31232K-&gt;5112K(36352K)] 31232K-&gt;27889K(119808K), 0.0400211 secs] [Times: user=0.00 sys=0.00, real=0.04 secs] [GC (Allocation Failure) [PSYoungGen: 36221K-&gt;5112K(67584K)] 58998K-&gt;58562K(151040K), 0.0147648 secs] [Times: user=0.03 sys=0.02, real=0.02 secs] [Full GC (Ergonomics) [PSYoungGen: 5112K-&gt;0K(67584K)] [ParOldGen: 53449K-&gt;58483K(130560K)] 58562K-&gt;58483K(198144K), [Metaspace: 3654K-&gt;3654K(1056768K)], 0.0138515 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] 123[GC (Allocation Failure) [PSYoungGen: 31143K-&gt;5099K(36352K)] 31143K-&gt;28014K(119808K), 0.0225225 secs] [Times: user=0.02 sys=0.00, real=0.02 secs] [GC (Allocation Failure) [PSYoungGen: 36297K-&gt;5113K(67584K)] 59212K-&gt;59110K(151040K), 0.0137243 secs] [Times: user=0.01 sys=0.02, real=0.01 secs] [Full GC (Ergonomics) [PSYoungGen: 5113K-&gt;0K(67584K)] [ParOldGen: 53997K-&gt;59027K(131584K)] 59110K-&gt;59027K(199168K), [Metaspace: 3655K-&gt;3655K(1056768K)], 0.0159174 secs] [Times: user=0.02 sys=0.00, real=0.02 secs] PrintGC&amp;PrintGCDetails123[GC (Allocation Failure) [PSYoungGen&#123;区域&#125;: 31205K&#123;回收前&#125;-&gt;5093K&#123;回收前&#125;(36352K&#123;该区域总容量&#125;)] 31205K&#123;堆前&#125;-&gt;28126K&#123;堆后&#125;(119808K&#123;堆总容量&#125;), 0.0126378 secs&#123;时间&#125;] [Times: user=0.00&#123;用户&#125; sys=0.03&#123;系统&#125;, real=0.01&#123;实际&#125; secs] [GC (Allocation Failure) [PSYoungGen: 36320K-&gt;5093K(67584K)] 59353K-&gt;59162K(151040K), 0.0128752 secs] [Times: user=0.02 sys=0.03, real=0.01 secs] [Full GC (Ergonomics) [PSYoungGen: 5093K-&gt;0K(67584K)] [ParOldGen: 54069K-&gt;59100K(135680K)] 59162K-&gt;59100K(203264K), [Metaspace: 3654K-&gt;3654K(1056768K)], 0.0226501 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] -XX:+PrintGCTimeStamps10.284&#123;从jvm启动直到垃圾收集发生所经历的时间&#125;: [GC (Allocation Failure) [PSYoungGen: 31232K-&gt;5107K(36352K)] 31232K-&gt;27716K(119808K), 0.0131908 secs] [Times: user=0.00 sys=0.05, real=0.01 secs] With -XX:+PrintGCTimeStamps a timestamp reflecting the real time passed in seconds since JVM -XX:+PrintGCDateStamps12019-03-01T10:56:27.863+0800: [GC (Allocation Failure) 31205K-&gt;28094K(119808K), 0.0149649 secs] -Xloggc1-Xloggc:C:\\\\Producer_gc.log GC-XX:MaxTenuringThreshold=thresholdSets the maximum tenuring threshold for use in adaptive GC sizing. The largest value is 15. The default value is 15 for the parallel (throughput) collector, and 6 for the CMS collector. -XX:+PrintGCDetailsEnables printing of detailed messages at every GC. By default, this option is disabled. -XX:+PrintGCDateStampsEnables printing of a date stamp at every GC. By default, this option is disabled. -XX:+PrintGCTimeStampsEnables printing of time stamps at every GC. By default, this option is disabled. -XX:SurvivorRatio=ratioSets the ratio between eden space size and survivor space size. By default, this option is set to 8. -XX:CMSInitiatingOccupancyFraction=percent算法 熟悉算法 熟悉垃圾回收期 实际使用的垃圾回收是什么","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"gc","slug":"gc","permalink":"https://liyong.ac.cn/tags/gc/"}]},{"title":"异常追踪","slug":"aa_category/se/异常追踪","date":"2019-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/05/12/aa_category/se/异常追踪/","link":"","permalink":"https://liyong.ac.cn/2019/05/12/aa_category/se/异常追踪/","excerpt":"ava 虚拟机工具接口（JVMTI）JVMTI（Java Virtual Machine Tool Interface） 即指 Java 虚拟机工具接口， 它是一套由虚拟机直接提供的 native 接口， 它处于整个 JPDA 体系的最底层， 所有调试功能本质上都需要通过 JVMTI 来提供。 通过这些接口， 开发人员不仅调试在该虚拟机上运行的Java 程序， 还能查看它们运行的状态， 设置回调函数， 控制某些环境变量， 从而优化程序性能。","text":"ava 虚拟机工具接口（JVMTI）JVMTI（Java Virtual Machine Tool Interface） 即指 Java 虚拟机工具接口， 它是一套由虚拟机直接提供的 native 接口， 它处于整个 JPDA 体系的最底层， 所有调试功能本质上都需要通过 JVMTI 来提供。 通过这些接口， 开发人员不仅调试在该虚拟机上运行的Java 程序， 还能查看它们运行的状态， 设置回调函数， 控制某些环境变量， 从而优化程序性能。 Java 调试线协议（JDWP）JDWP（Java Debug Wire Protocol） 是一个为 Java 调试而设计的一个通讯交互协议， 它定义了调试器和被调试程序之间传递的信息的格式。在 JPDA 体系中， 作为前端（front-end） 的调试者（debugger） 进程和后端（back-end） 的被调试程序（debuggee） 进程之间的交互数据的格式就是由 JDWP 来描述的， 它详细完整地定义了请求命令、 回应数据和错误代码， 保证了前端和后端的 JVMTI 和 JDI的通信通畅。 Java 调试接口（JDI）JDI（Java Debug Interface） 是三个模块中最高层的接口， 在多数的 JDK 中， 它是由 Java 语言实现的。 JDI 由针对前端定义的接口组成， 通过它， 调试工具开发人员就能通过前端虚拟机上的调试器来远程操控后端虚拟机上被调试程序的运行。 JVMTIStart LoadAttachHeapThreadBreakpoint Java agentAPM Instrument Spring AOP","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"}]},{"title":"堆外内存排查","slug":"aa_category/se/堆外内存排查","date":"2019-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/05/12/aa_category/se/堆外内存排查/","link":"","permalink":"https://liyong.ac.cn/2019/05/12/aa_category/se/堆外内存排查/","excerpt":"现象有人反应线上服务端有一个进程的占用内存特别高，具体如下。 top命令如下： PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND33801 admin 20 0 41.4g 14g 10m S 5.0 5.8 4015:27 java 可以看到这个java进程占用的资源特别高。","text":"现象有人反应线上服务端有一个进程的占用内存特别高，具体如下。 top命令如下： PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND33801 admin 20 0 41.4g 14g 10m S 5.0 5.8 4015:27 java 可以看到这个java进程占用的资源特别高。 分析内存分布启动命令为：-server -Xms4096m -Xmx4096m -XX:MaxPermSize=256m 1.jmap -heap查看JVM占用内存大小 1`[admin@host-xxxx ~]$ jmap -heap 33801``Attaching to process ID 33801, please wait...``Debugger attached successfully.``Server compiler detected.``JVM version is 20.0-b11``using thread-``local` `object allocation.``Parallel GC with 43 thread(s)``Heap Configuration:`` ``MinHeapFreeRatio = 40`` ``MaxHeapFreeRatio = 70`` ``MaxHeapSize = 4294967296 (4096.0MB)`` ``NewSize = 1310720 (1.25MB)`` ``MaxNewSize = 17592186044415 MB`` ``OldSize = 5439488 (5.1875MB)`` ``NewRatio = 2`` ``SurvivorRatio = 8`` ``PermSize = 21757952 (20.75MB)`` ``MaxPermSize = 268435456 (256.0MB)``Heap Usage:``PS Young Generation``Eden Space:`` ``capacity = 1402273792 (1337.3125MB)`` ``used = 1305741520 (1245.2521514892578MB)`` ``free` `= 96532272 (92.06034851074219MB)`` ``93.11601824474518% used``From Space:`` ``capacity = 14680064 (14.0MB)`` ``used = 0 (0.0MB)`` ``free` `= 14680064 (14.0MB)`` ``0.0% used``To Space:`` ``capacity = 14155776 (13.5MB)`` ``used = 0 (0.0MB)`` ``free` `= 14155776 (13.5MB)`` ``0.0% used``PS Old Generation`` ``capacity = 2863333376 (2730.6875MB)`` ``used = 78102656 (74.4844970703125MB)`` ``free` `= 2785230720 (2656.2030029296875MB)`` ``2.7276829395642124% used``PS Perm Generation`` ``capacity = 63176704 (60.25MB)`` ``used = 62522104 (59.62572479248047MB)`` ``free` `= 654600 (0.6242752075195312MB)`` ``98.96385857673107% used` 可以看到堆内内存正常，没有超过4G，说明大部分内存都是堆外内存。 查看堆外内存情况 网上都推荐使用google-perftools来跟踪，但是需要重启应用。我们先不重启，直接dump内存分析下。 执行 pmap 1`[admin@host-xxx~]$ pmap 33801 &gt; 33801.txt``[admin@host-xxx~]$ ``cat` `33801 | ``more``Address Kbytes RSS Dirty Mode Mapping``0000000040000000 36 36 0 r-x-- java``0000000040108000 8 8 8 rwx-- java``0000000040b6d000 132 8 8 rwx-- [ anon ]``……``00007f2c74000000 131072 131072 131072 rwx-- [ anon ]``00007f2c7c000000 131072 131072 131072 rwx-- [ anon ]``00007f2c84000000 131072 131072 131072 rwx-- [ anon ]``00007f2c8c000000 131072 131072 131072 rwx-- [ anon ]``00007f2c94000000 131072 131072 131072 rwx-- [ anon ]``……``……` 其中看见不少 128M的内存块。 比较可疑 dump内存块 使用 gdb进行内存dump。如果没有，则需要安装 yum install gdb。 1`[admin@host-xxx ~]$ gdb --pid 33801``……``Reading symbols from ``/export/Domains/follow``.soa.jd.com``/server1/temp/libnetty-transport-native-epoll4107761723280840577``.so...(no debugging symbols found)...``done``.``Loaded symbols ``for` `/export/Domains/follow``.soa.jd.com``/server1/temp/libnetty-transport-native-epoll4107761723280840577``.so``0x00007f3206fa522d ``in` `pthread_join () from ``/lib64/libpthread``.so.0``Missing separate debuginfos, use: debuginfo-``install` `glibc-2.12-1.149.el6.x86_64``(gdb) ``看到这个代表可以进行调试``然后外面找刚才的一段地址（前面加上0x）进行dump``(gdb) dump binary memory .``/33801_1``.dat 0x00007f2c74000000 0x00007f2c7c000000` `dump完 按q 退出``(gdb) q``A debugging session is active.`` ``Inferior 1 [process 33801] will be detached.``Quit anyway? (y or n) y` 这样我们就拿到了一个内存dump文件。可以通过16进制大概看看里面的内容。 分析内存块。 使用hexdump或者 下载下来拿UltraEdit EditPlus都可以看16进制文件。 1`[admin@host-xxxx ~]$ hexdump -e ``'16/1 \"%02X \" \" | \"'` `-e ``'16/1 \"%_p\" \"\\n\"'` `.``/33801_1``.dat | ``more``20 00 00 98 2D 7F 00 00 00 00 00 80 2C 7F 00 00 | ...-.......,...``00 00 00 04 00 00 00 00 00 00 00 04 00 00 00 00 | ................``00 00 00 00 00 00 00 00 45 00 00 00 00 00 00 00 | ........E.......``45 72 72 6F 72 20 77 68 69 6C 65 20 72 65 61 64 | Error ``while` `read``28 2E 2E 2E 29 3A 20 43 6F 6E 6E 65 63 74 69 6F | (...): Connectio``6E 20 72 65 73 65 74 20 62 79 20 70 65 65 72 00 | n reset by peer.``00 00 00 00 00 00 00 00 45 00 00 00 00 00 00 00 | ........E.......``45 72 72 6F 72 20 77 68 69 6C 65 20 72 65 61 64 | Error ``while` `read``28 2E 2E 2E 29 3A 20 43 6F 6E 6E 65 63 74 69 6F | (...): Connectio``6E 20 72 65 73 65 74 20 62 79 20 70 65 65 72 00 | n reset by peer.``00 00 00 00 00 00 00 00 45 00 00 00 00 00 00 00 | ........E.......``45 72 72 6F 72 20 77 68 69 6C 65 20 72 65 61 64 | Error ``while` `read``28 2E 2E 2E 29 3A 20 43 6F 6E 6E 65 63 74 69 6F | (...): Connectio``6E 20 72 65 73 65 74 20 62 79 20 70 65 65 72 00 | n reset by peer.``00 00 00 00 00 00 00 00 45 00 00 00 00 00 00 00 | ........E.......` 里面全是 ……..E……. Error while read(…): Connection reset by peer. 好像是一个对象，里面的字符串是 Error while read(…): Connection reset by peer. 分析数据来源程序有Dubbo，mq使用了Netty4，zookeeper使用了Netty3。 这几个都有可能申请堆外内存。 搜索字符串“Error while read” 搜索netty4.0.24的源码 发现了该字符串： transport-native-epoll/src/main/c/io_netty_channel_epoll_Native.c 1`jint read0(JNIEnv * env, jclass clazz, jint fd, ``void` `*buffer, jint pos, jint limit) &#123;`` ``ssize_t res;`` ``int` `err;`` ``do` `&#123;`` ``res = read(fd, buffer + pos, (``size_t``) (limit - pos));`` ``// Keep on reading if we was interrupted`` ``&#125; ``while` `(res == -1 &amp;&amp; ((err = ``errno``) == EINTR));` ` ``if` `(res &lt; 0) &#123;`` ``if` `(err == EAGAIN || err == EWOULDBLOCK) &#123;`` ``// Nothing left to read`` ``return` `0;`` ``&#125;`` ``if` `(err == EBADF) &#123;`` ``throwClosedChannelException(env);`` ``return` `-1;`` ``&#125;`` ``throwIOException(env, exceptionMessage(``\"Error while read(...): \"``, err));`` ``return` `-1;`` ``&#125;` ` ``if` `(res == 0) &#123;`` ``// end-of-stream`` ``return` `-1;`` ``&#125;`` ``return` `(jint) res;``&#125;` 分析源码：如果使用了epoll，客户端连上服务端又断开，就会触发这个异常。 继续查看Netty源码和ReleaseNote，发现在4.0.25版本 已经优化掉了这个东西。 参见：http://netty.io/news/2014/12/31/4-0-25-Final.html https://github.com/netty/netty/pull/3227 https://source.jboss.org/changelog/Netty?cs=c0e889ae543e291f03f003485ee54dd281945ec6&amp;_sscc=t 测试：1.测试关闭epoll是否会重现此问题。——-关闭后正常 2.更新netty版本，看是否解决。———解决 解决：1.单独升级Netty4的版本。","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"}]},{"title":"Java8新特性","slug":"aa_category/se/se8","date":"2019-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/05/12/aa_category/se/se8/","link":"","permalink":"https://liyong.ac.cn/2019/05/12/aa_category/se/se8/","excerpt":"Optionaljava1.8引入的新类，用来判断是否小于1.8","text":"Optionaljava1.8引入的新类，用来判断是否小于1.8 Consumer12//只有入参没有出参void accept(T t); 1234// 相当于返回另外一个实现类default Consumer&lt;T&gt; andThen(Consumer&lt;? super T&gt; after) &#123; Objects.requireNonNull(after); return (T t) -&gt; &#123; accept(t); after.accept(t); &#125;; Supplier12//只有出参没有入参T get(); Function12//有输入和输出R apply(T t);","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"}]},{"title":"jstat","slug":"aa_category/se/jstat命令","date":"2019-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/05/12/aa_category/se/jstat命令/","link":"","permalink":"https://liyong.ac.cn/2019/05/12/aa_category/se/jstat命令/","excerpt":"JDK Tools and Utilitieshttps://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html","text":"JDK Tools and Utilitieshttps://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html 类加载统计123jstat -class 172334Loaded Bytes Unloaded Bytes Time 11658 23107.3 1 1.0 16.03 Loaded:加载class的数量 Bytes：所占用空间大小 Unloaded：未加载数量 Bytes:未加载占用空间 Time：时间 编译统计123jstat -compiler 172334Compiled Failed Invalid Time FailedType FailedMethod 14198 6 0 272.07 1 com/mysql/jdbc/AbandonedConnectionCleanupThread run Compiled：编译数量。 Failed：失败数量 Invalid：不可用数量 Time：时间 FailedType：失败类型 FailedMethod：失败的方法 垃圾回收统计123jstat -gc 427623 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 8192.0 8704.0 0.0 896.0 331264.0 270063.5 699392.0 120920.8 75136.0 73687.3 8320.0 8042.8 89 4.616 3 4.195 8.810 S0C：第一个幸存区的大小 S1C：第二个幸存区的大小 S0U：第一个幸存区的使用大小 S1U：第二个幸存区的使用大小 EC：伊甸园区的大小 EU：伊甸园区的使用大小 OC：老年代大小 OU：老年代使用大小 MC：方法区大小 MU：方法区使用大小 CCSC:压缩类空间大小 CCSU:压缩类空间使用大小 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 堆内存统计123jstat -gccapacity 427623 NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC 349184.0 349184.0 349184.0 8192.0 7680.0 333312.0 699392.0 699392.0 699392.0 699392.0 0.0 1116160.0 75136.0 0.0 1048576.0 8320.0 90 3 NGCMN：新生代最小容量 NGCMX：新生代最大容量 NGC：当前新生代容量 S0C：第一个幸存区大小 S1C：第二个幸存区的大小 EC：伊甸园区的大小 OGCMN：老年代最小容量 OGCMX：老年代最大容量 OGC：当前老年代大小 OC:当前老年代大小 MCMN:最小元数据容量 MCMX：最大元数据容量 MC：当前元数据空间大小 CCSMN：最小压缩类空间大小 CCSMX：最大压缩类空间大小 CCSC：当前压缩类空间大小 YGC：年轻代gc次数 FGC：老年代GC次数 346112 70057","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"}]},{"title":"反射常用类","slug":"aa_category/se/reflect/常用类","date":"2019-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/05/12/aa_category/se/reflect/常用类/","link":"","permalink":"https://liyong.ac.cn/2019/05/12/aa_category/se/reflect/常用类/","excerpt":"AnnotatedElementgetAnnotation 可以获取父类声明上的注解","text":"AnnotatedElementgetAnnotation 可以获取父类声明上的注解 MethodisBridge一个子类在继承（或实现）一个父类（或接口）的泛型方法时，在子类中明确指定了泛型类型，那么在编译时编译器会自动生成桥接方法（当然还有其他情况会生成桥接方法，这里只是列举了其中一种情况）","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"reflect","slug":"reflect","permalink":"https://liyong.ac.cn/tags/reflect/"}]},{"title":"Java命令","slug":"aa_category/se/java_option","date":"2019-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/05/12/aa_category/se/java_option/","link":"","permalink":"https://liyong.ac.cn/2019/05/12/aa_category/se/java_option/","excerpt":"https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.htmlhttp://openjdk.java.net/jeps/122","text":"https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.htmlhttp://openjdk.java.net/jeps/122 杂-XX:+TraceClassLoadingEnables tracing of classes as they are loaded. By default, this option is disabled and classes are not traced. 常用https://blog.sokolenko.me/2014/11/javavm-options-production.html 1234567891011121314151617181920212223242526272829303132333435-server-Xms&lt;heap size&gt;[g|m|k] -Xmx&lt;heap size&gt;[g|m|k]//Sets the initial size (in bytes) of the heap. Specifies the maximum size (in bytes) of the memory allocation pool in bytes-XX:MaxMetaspaceSize=&lt;metaspace size&gt;[g|m|k]//Sets the maximum amount of native memory that can be allocated for class metadata.-Xmn&lt;young size&gt;[g|m|k]//Sets the initial and maximum size (in bytes) of the heap for the young generation (nursery) eden+ 2 survivor space.-Xss//Sets the thread stack size (in bytes).-XX:SurvivorRatio=&lt;ratio&gt;// Sets the ratio between eden space size and survivor space size.-XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled// Enables the use of the CMS garbage collector for the old generation.-XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=&lt;percent&gt;-XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark//Enables GC of the young generation before each full GC.-XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xloggc:\"&lt;path to log&gt;\"-XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M-Dsun.net.inetaddr.ttl=&lt;TTL in seconds&gt;-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=&lt;path to dump&gt;`date`.hprof//Enables the dumping of the Java heap to a file in the current directory by using the heap profiler (HPROF) when a java.lang.OutOfMemoryError exception is thrown.-Djava.rmi.server.hostname=&lt;external IP&gt;-Dcom.sun.management.jmxremote.port=&lt;port&gt; -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false 内存XX:MinHeapFreeRatioSets the minimum allowed percentage of free heap space (0 to 100) after a GC event. If free heap space falls below this value, then the heap will be expanded. By default, this value is set to 40%. -XX:MaxHeapFreeRatioSets the maximum allowed percentage of free heap space (0 to 100) after a GC event. If free heap space expands above this value, then the heap will be shrunk -XX:NewRatio=ratioSets the ratio between young and old generation sizes. By default, this option is set to 2. -XX:NewSize=size Sets the initial size (in bytes) of the heap for the young generation (nursery) The -XX:NewSize option is equivalent to -Xmn. -XX:MaxPermSize=sizeSets the maximum permanent generation space size (in bytes). This option was deprecated in JDK 8, and superseded by the -XX:MaxMetaspaceSize option. -XX:MaxMetaspaceSize=sizeSets the maximum amount of native memory that can be allocated for class metadata. By default, the size is not limited. 1-XX:MaxMetaspaceSize=256m -XX:MetaspaceSize=sizeSets the size of the allocated class metadata space that will trigger a garbage collection the first time it is exceeded. This threshold for a garbage collection is increased or decreased depending on the amount of metadata used. The default size depends on the platform -XX:CompressedClassSpaceSize=128m江南白衣http://calvin1978.blogcn.com -XX:+PrintFlagsFinal打印参数值 -XX:-UseBiasedLocking-XX:AutoBoxCacheMax=20000","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"}]},{"title":"Java问题排查工具单","slug":"aa_category/se/Java问题排查工具单","date":"2019-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/05/12/aa_category/se/Java问题排查工具单/","link":"","permalink":"https://liyong.ac.cn/2019/05/12/aa_category/se/Java问题排查工具单/","excerpt":"Linux命令类tailtail -300f shopbase.log","text":"Linux命令类tailtail -300f shopbase.log grep12345678910grep forest f.txt #文件查找grep forest f.txt cpf.txt #多文件查找grep 'log' /home/admin -r -n #目录下查找所有符合关键字的文件cat f.txt | grep -i shopbasegrep 'shopbase' /home/admin -r -n --include *.&#123;vm,java&#125; #指定文件后缀grep 'shopbase' /home/admin -r -n --exclude *.&#123;vm,java&#125; #反匹配seq 10 | grep 5 -A 3 #上匹配seq 10 | grep 5 -B 3 #下匹配seq 10 | grep 5 -C 3 #上下匹配，平时用这个就妥了cat f.txt | grep -c 'SHOPBASE' awk1 基础命令jmap 123456awk '&#123;print $4,$6&#125;' f.txtawk '&#123;print NR,$0&#125;' f.txt cpf.txtawk '&#123;print FNR,$0&#125;' f.txt cpf.txtawk '&#123;print FNR,FILENAME,$0&#125;' f.txt cpf.txtawk '&#123;print FILENAME,\"NR=\"NR,\"FNR=\"FNR,\"$\"NF\"=\"$NF&#125;' f.txt cpf.txtecho 1:2:3:4 | awk -F: '&#123;print $1,$2,$3,$4&#125;' 2 匹配 1234awk '/ldb/ &#123;print&#125;' f.txt #匹配ldbawk '!/ldb/ &#123;print&#125;' f.txt #不匹配ldbawk '/ldb/ &amp;&amp; /LISTEN/ &#123;print&#125;' f.txt #匹配ldb和LISTENawk '$5 ~ /ldb/ &#123;print&#125;' f.txt #第五列匹配ldb 3 内建变量 NR:NR表示从awk开始执行后，按照记录分隔符读取的数据次数，默认的记录分隔符为换行符，因此默认的就是读取的数据行数，NR可以理解为Number of Record的缩写。 FNR:在awk处理多个输入文件的时候，在处理完第一个文件后，NR并不会从1开始，而是继续累加，因此就出现了FNR，每当处理一个新文件的时候，FNR就从1开始计数，FNR可以理解为File Number of Record。 NF: NF表示目前的记录被分割的字段的数目，NF可以理解为Number of Field。 find12345678910111213sudo -u admin find /home/admin /tmp /usr -name \\*.log(多个目录去找)find . -iname \\*.txt(大小写都匹配)find . -type d(当前目录下的所有子目录)find /usr -type l(当前目录下所有的符号链接)find /usr -type l -name \"z*\" -ls(符号链接的详细信息 eg:inode,目录)find /home/admin -size +250000k(超过250000k的文件，当然+改成-就是小于了)find /home/admin f -perm 777 -exec ls -l &#123;&#125; \\; (按照权限查询文件)find /home/admin -atime -1 1天内访问过的文件find /home/admin -ctime -1 1天内状态改变过的文件find /home/admin -mtime -1 1天内修改过的文件find /home/admin -amin -1 1分钟内访问过的文件find /home/admin -cmin -1 1分钟内状态改变过的文件find /home/admin -mmin -1 1分钟内修改过的文件 pgm批量查询vm-shopbase满足条件的日志 1pgm -A -f vm-shopbase 'cat /home/admin/shopbase/logs/shopbase.log.2017-01-17|grep 2069861630' tsarTsar (Taobao System Activity Reporter) is a monitoring tool, which can be used to gather and summarize system information, e.g. CPU, load, IO, and application information, e.g. nginx, HAProxy, Squid, etc. The results can be stored at local disk or sent to Nagios. top12ps -ef | grep javatop -H -p pid 获得线程10进制转16进制后jstack去抓看这个线程到底在干啥 netstat12#查看当前连接，注意close_wait偏高的情况netstat -nat|awk '&#123;print $6&#125;'|sort|uniq -c|sort -rn 排查利器btraceA safe, dynamic tracing tool for the Java platform Greyshttps://github.com/oldmanpushcart/greys-anatomy/wiki/greys-pdf javOSizehttp://www.javosize.com/ JProfilerhttps://www.ej-technologies.com/products/jprofiler/overview.html java三板斧jps1jps -mlvV jstack普通用法1jstack 2815 native+java栈1jstack -m 2815 jinfo1jinfo -flags 2815 jmap查看堆的情况1jmap -heap 2815 dump1jmap -dump:live,format=b,file=/tmp/heap2.bin 2815 看看堆都被谁占了1jmap -histo 2815 | head -10 jstat1jstat -gcutil 2815 1000 jar包冲突打出所有依赖1mvn dependency:tree &gt; ~/dependency.txt","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"}]},{"title":"JVM常用命令整理","slug":"aa_category/se/JVM常用命令整理","date":"2019-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/05/12/aa_category/se/JVM常用命令整理/","link":"","permalink":"https://liyong.ac.cn/2019/05/12/aa_category/se/JVM常用命令整理/","excerpt":"看对象数量jmap -histo [pid]&gt;jmaphisto.logjmap -F -histo [pid]&gt;jmaphisto.log 服务已死加 -Fjmap -histo:live [pid]&gt;jmaphisto.log 只看存活（会触发fullgc再导出）","text":"看对象数量jmap -histo [pid]&gt;jmaphisto.logjmap -F -histo [pid]&gt;jmaphisto.log 服务已死加 -Fjmap -histo:live [pid]&gt;jmaphisto.log 只看存活（会触发fullgc再导出） 看内存Dumpjmap dump:format=b,file=jmapdump.hprof [pid]jmap -F -dump:format=b,file=jmapdump.hprof [pid]tar zcvf jmapdump.hprof.tar.gz jmapdump.hprof 看GC情况jstat -gccapacityjstat -gcutil [pid] 1000 100 每1000毫秒看内存情况，持续100次 指标解释：S0 — Heap上的 Survivor space 0 区已使用空间的百分比S1 — Heap上的 Survivor space 1 区已使用空间的百分比E — Heap上的 Eden space 区已使用空间的百分比O — Heap上的 Old space 区已使用空间的百分比P — Perm space 区已使用空间的百分比YGC — 从应用程序启动到采样时发生 Young GC 的次数YGCT– 从应用程序启动到采样时 Young GC 所用的时间(单位秒)FGC — 从应用程序启动到采样时发生 Full GC 的次数FGCT– 从应用程序启动到采样时 Full GC 所用的时间(单位秒)GCT — 从应用程序启动到采样时用于垃圾回收的总时间(单位秒) 看线程堆栈jstack [pid] &gt;jstack.logjstack -F [pid] &gt;jstack.log 服务已死加 -Fjstack -F -m [pid] &gt;jstack.logjstack -F -m -l [pid] &gt;jstack.log 看线程的资源情况pstree [pid] 打印进程的线程使用情况top 看资源使用情况top -Hp [pid] 看进程内线程资源情况转为16进制配合jstack可查询线程cpu情况","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"}]},{"title":"mat","slug":"aa_category/tool/mat","date":"2019-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/05/12/aa_category/tool/mat/","link":"","permalink":"https://liyong.ac.cn/2019/05/12/aa_category/tool/mat/","excerpt":"","text":"https://www.cnblogs.com/trust-freedom/p/6744948.html with outGoing references 当前对象，引用了外部对象 with incoming references 当前查看的对象，被外部应用 Retained size：是该对象自己的 shallow size，加上从该对象能直接或间接访问到对象的 shallow size 之和。换句话说，retained size 是该对象被 GC 之后所能回收到内存的总和。","categories":[{"name":"tool","slug":"tool","permalink":"https://liyong.ac.cn/categories/tool/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"tool","slug":"tool","permalink":"https://liyong.ac.cn/tags/tool/"}]},{"title":"加载顺序","slug":"aa_category/ee/servlet/加载顺序","date":"2019-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/05/12/aa_category/ee/servlet/加载顺序/","link":"","permalink":"https://liyong.ac.cn/2019/05/12/aa_category/ee/servlet/加载顺序/","excerpt":"context-param -&gt; listener -&gt; filter -&gt; servlet ServletContextcontext-param上下文初始化用到的参数","text":"context-param -&gt; listener -&gt; filter -&gt; servlet ServletContextcontext-param上下文初始化用到的参数 ListenerServletContextListener12contextInitializedcontextDestroyed HttpSessionListener12sessionCreatedsessionDestroyed HttpSessionAttributeListener123attributeAddedattributeRemovedattributeReplaced ServletRequestListener12requestInitializedrequestDestroyed","categories":[{"name":"ee","slug":"ee","permalink":"https://liyong.ac.cn/categories/ee/"}],"tags":[{"name":"ee","slug":"ee","permalink":"https://liyong.ac.cn/tags/ee/"},{"name":"web","slug":"web","permalink":"https://liyong.ac.cn/tags/web/"}]},{"title":"05基础篇：某个应用的CPU使用率居然达到100%我该怎么办","slug":"aa_category/linux/Linux性能优化实战/05基础篇：某个应用的CPU使用率居然达到100%，我该怎么办","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/Linux性能优化实战/05基础篇：某个应用的CPU使用率居然达到100%，我该怎么办/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/Linux性能优化实战/05基础篇：某个应用的CPU使用率居然达到100%，我该怎么办/","excerpt":"CPU 使用率12grep 'CONFIG_HZ=' /boot/config-$(uname -r)CONFIG_HZ=250 12345# 只保留各个 CPU 的数据$ cat /proc/stat | grep ^cpucpu 280580 7407 286084 172900810 83602 0 583 0 0 0cpu0 144745 4181 176701 86423902 52076 0 301 0 0 0cpu1 135834 3226 109383 86476907 31525 0 282 0 0 0","text":"CPU 使用率12grep 'CONFIG_HZ=' /boot/config-$(uname -r)CONFIG_HZ=250 12345# 只保留各个 CPU 的数据$ cat /proc/stat | grep ^cpucpu 280580 7407 286084 172900810 83602 0 583 0 0 0cpu0 144745 4181 176701 86423902 52076 0 301 0 0 0cpu1 135834 3226 109383 86476907 31525 0 282 0 0 0 user（通常缩写为 us），代表用户态 CPU 时间。注意，它不包括下面的 nice 时间，但包括了 guest 时间。 nice（通常缩写为 ni），代表低优先级用户态 CPU 时间，也就是进程的 nice 值被调整为 1-19 之间时的 CPU 时间。这里注意，nice 可取值范围是 -20 到 19，数值越大，优先级反而越低。 system（通常缩写为 sys），代表内核态 CPU 时间。 idle（通常缩写为 id），代表空闲时间。注意，它不包括等待 I/O 的时间（iowait）。 iowait（通常缩写为 wa），代表等待 I/O 的 CPU 时间。 irq（通常缩写为 hi），代表处理硬中断的 CPU 时间。 soft（通常缩写为 si），代表处理软中断的 CPU 时间。 steal（通常缩写为 st），代表当系统运行在虚拟机中的时候，被其他虚拟机占用的 CPU 时间。 guest（通常缩写为 guest），代表通过虚拟化运行其他操作系统的时间，也就是运行虚拟机的 CPU 时间。 guest_nice（通常缩写为 gnice），代表以低优先级运行虚拟机的时间。 CPU 使用率，就是除了空闲时间外的其他时间占总 CPU 时间的百分比 怎么查看 CPU 使用率top 显示了系统总体的 CPU 和内存使用情况，以及各个进程的资源使用情况。 ps 则只显示了每个进程的资源使用情况。 CPU 使用率过高怎么办？ GDB（The GNU Project Debugger） perf top perfhttps://perf.wiki.kernel.org/index.php/Main_Page","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"开篇词_别再让Linux性能问题成为你的绊脚石","slug":"aa_category/linux/Linux性能优化实战/开篇词_别再让Linux性能问题成为你的绊脚石","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/Linux性能优化实战/开篇词_别再让Linux性能问题成为你的绊脚石/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/Linux性能优化实战/开篇词_别再让Linux性能问题成为你的绊脚石/","excerpt":"最好的学习方式一定是带着问题学习 具体来看，我会分为 5 个模块。前 4 个模块我会从资源使用的视角出发，带你分析各种 Linux 资源可能会碰到的性能问题，包括 CPU 性能、磁盘 I/O 性能、内存性能、网络性能","text":"最好的学习方式一定是带着问题学习 具体来看，我会分为 5 个模块。前 4 个模块我会从资源使用的视角出发，带你分析各种 Linux 资源可能会碰到的性能问题，包括 CPU 性能、磁盘 I/O 性能、内存性能、网络性能 基础篇，介绍 Linux 必备的基本原理以及对应的性能指标和性能工具。比如怎么理解平均负载，怎么理解上下文切换，Linux 内存的工作原理等等。 案例篇，这里我会通过模拟案例，帮你分析高手在遇到资源瓶颈时，是如何观测、定位、分析并优化这些性能问题的。 套路篇，在理解了基础，亲身体验了模拟案例之后，我会帮你梳理出排查问题的整体思路，也就是检查性能问题的一般步骤，这样，以后你遇到问题，就可以按照这样的路子来。 答疑篇，我相信在学习完每一个模块之后，你都会有很多的问题，在答疑篇里，我会拿出提问频次较高的问题给你系统解答。","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"24-25基础篇：Linux 磁盘IO是怎么工作的","slug":"aa_category/linux/Linux性能优化实战/24-25基础篇：Linux 磁盘IO是怎么工作的","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/Linux性能优化实战/24-25基础篇：Linux 磁盘IO是怎么工作的/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/Linux性能优化实战/24-25基础篇：Linux 磁盘IO是怎么工作的/","excerpt":"磁盘 第一类，机械磁盘，也称为硬盘驱动器（Hard Disk Driver），通常缩写为 HDD。机械磁盘主要由盘片和读写磁头组成，数据就存储在盘片的环状磁道中。在读写数据前，需要移动读写磁头，定位到数据所在的磁道，然后才能访问数据。 第二类，固态磁盘（Solid State Disk），通常缩写为 SSD，由固态电子元器件组成。固态磁盘不需要磁道寻址，所以，不管是连续 I/O，还是随机 I/O 的性能，都比机械磁盘要好得多。","text":"磁盘 第一类，机械磁盘，也称为硬盘驱动器（Hard Disk Driver），通常缩写为 HDD。机械磁盘主要由盘片和读写磁头组成，数据就存储在盘片的环状磁道中。在读写数据前，需要移动读写磁头，定位到数据所在的磁道，然后才能访问数据。 第二类，固态磁盘（Solid State Disk），通常缩写为 SSD，由固态电子元器件组成。固态磁盘不需要磁道寻址，所以，不管是连续 I/O，还是随机 I/O 的性能，都比机械磁盘要好得多。 通用块层 第一个功能跟虚拟文件系统的功能类似。向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序。 第二个功能，通用块层还会给文件系统和应用程序发来的 I/O 请求排队，并通过重新排序、请求合并等方式，提高磁盘读写的效率。 I/O 栈 文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据。 通用块层，包括块设备 I/O 队列和 I/O 调度器。它会对文件系统的 I/O 请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层。 设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I/O 操作。 磁盘性能指标 使用率，是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈。 饱和度，是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。 IOPS（Input/Output Per Second），是指每秒的 I/O 请求数。 吞吐量，是指每秒的 I/O 请求大小。 响应时间，是指 I/O 请求从发出到收到响应的间隔时间。 使用率只考虑有没有 I/O，而不考虑 I/O 的大小。换句话说，当使用率是 100% 的时候，磁盘依然有可能接受新的 I/O 请求。 磁盘 I/O 观测12# -d -x 表示显示所有磁盘 I/O 的指标$ iostat -d -x 1 %util ，就是我们前面提到的磁盘 I/O 使用率； r/s+ w/s ，就是 IOPS； rkB/s+wkB/s ，就是吞吐量； r_await+w_await ，就是响应时间。 进程I/O观测1pidstat -d 1 每秒读取的数据大小（kB_rd/s） ，单位是 KB。 每秒发出的写请求数据大小（kB_wr/s） ，单位是 KB。 每秒取消的写请求数据大小（kB_ccwr/s） ，单位是 KB。 块 I/O 延迟（iodelay），包括等待同步块 I/O 和换入块 I/O 结束的时间，单位是时钟周期。","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"16基础篇：怎么理解内存中的Buffer和Cache","slug":"aa_category/linux/Linux性能优化实战/16基础篇：怎么理解内存中的Buffer和Cache","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/Linux性能优化实战/16基础篇：怎么理解内存中的Buffer和Cache/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/Linux性能优化实战/16基础篇：怎么理解内存中的Buffer和Cache/","excerpt":"Buffers 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值。 Cache 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与 SReclaimable 之和。","text":"Buffers 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值。 Cache 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与 SReclaimable 之和。 1234567891011Buffers %lu Relatively temporary storage for raw disk blocks that shouldn't get tremendously large (20MB or so).Cached %lu In-memory cache for files read from the disk (the page cache). Doesn't include SwapCached....SReclaimable %lu (since Linux 2.6.19) Part of Slab, that might be reclaimed, such as caches. SUnreclaim %lu (since Linux 2.6.19) Part of Slab, that cannot be reclaimed on memory pressure. Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，通常不会特别大（20MB 左右）。这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等等。 Cached 是从磁盘读取文件的页缓存，也就是用来缓存从文件读取的数据。这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。 SReclaimable 是 Slab 的一部分。Slab 包括两部分，其中的可回收部分，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。 案例12# 清理文件页、目录项、Inodes 等各种缓存echo 3 &gt; /proc/sys/vm/drop_caches 1234# 首先清理缓存echo 3 &gt; /proc/sys/vm/drop_caches# 然后运行 dd 命令向磁盘分区 /dev/sdb1 写入 2G 数据dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048 总结 Buffer 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”。 Cache 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”。 Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"15 基础篇：Linux内存是怎么工作的","slug":"aa_category/linux/Linux性能优化实战/15 基础篇：Linux内存是怎么工作的","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/Linux性能优化实战/15 基础篇：Linux内存是怎么工作的/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/Linux性能优化实战/15 基础篇：Linux内存是怎么工作的/","excerpt":"内存映射Linux 内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存。","text":"内存映射Linux 内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存。 虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同字长（也就是单个 CPU 指令可以处理数据的最大长度）的处理器，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示： ​ 进程在用户态时，只能访问用户空间内存；只有进入内核态后，才可以访问内核空间内存。虽然每个进程的地址空间都包含了内核空间，但这些内核空间，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。 既然每个进程都有一个这么大的地址空间，那么所有进程的虚拟内存加起来，自然要比实际的物理内存大得多。所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才分配物理内存，并且分配后的物理内存，是通过内存映射来管理的。 内存映射，其实就是将虚拟内存地址映射到物理内存地址。为了完成内存映射，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系，如下图所示： 虚拟内存空间分布 只读段，包括代码和常量等。 数据段，包括全局变量等。 堆，包括动态分配的内存，从低地址开始向上增长。 文件映射段，包括动态库、共享内存等，从高地址开始向下增长。 栈，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。 内存分配与回收 回收缓存，比如使用 LRU（Least Recently Used）算法，回收最近使用最少的内存页面； 回收不常访问的内存，把不常用的内存通过交换分区直接写到磁盘中； 杀死进程，内存紧张时系统还会通过 OOM（Out of Memory），直接杀掉占用大量内存的进程。 回收不常访问的内存会用到交换分区（以下简称 Swap）。Swap 其实就是把一块磁盘空间当成内存来用。它可以把进程暂时不用的数据存储到磁盘中（这个过程称为换出），当进程访问这些内存时，再从磁盘读取这些数据到内存中（这个过程称为换入）。 如何查看内存使用情况1234$ free total used free shared buff/cache availableMem: 8169348 263524 6875352 668 1030472 7611064Swap: 0 0 0 available 不仅包含未使用内存，还包括了可回收的缓存，所以一般会比未使用内存更大。不过，并不是所有缓存都可以回收，因为有些缓存可能正在使用中。 123456789# 按下 M 切换到内存排序$ top...KiB Mem : 8169348 total, 6871440 free, 267096 used, 1030812 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 7607492 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 430 root 19 -1 122360 35588 23748 S 0.0 0.4 0:32.17 systemd-journal 1075 root 20 0 771860 22744 11368 S 0.0 0.3 0:38.89 snapd VIRT 是进程虚拟内存的大小，只要是进程申请过的内存，即便还没有真正分配物理内存，也会计算在内。 RES 是常驻内存的大小，也就是进程实际使用的物理内存大小，但不包括 Swap 和共享内存。 SHR 是共享内存的大小，比如与其他进程共同使用的共享内存、加载的动态链接库以及程序的代码段等。","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"23基础篇：Linux 文件系统是怎么工作的","slug":"aa_category/linux/Linux性能优化实战/23基础篇：Linux 文件系统是怎么工作的","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/Linux性能优化实战/23基础篇：Linux 文件系统是怎么工作的/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/Linux性能优化实战/23基础篇：Linux 文件系统是怎么工作的/","excerpt":"索引节点和目录项在 Linux 中一切皆文件。不仅普通的文件和目录，就连块设备、套接字、管道等，也都要通过统一的文件系统来管理。","text":"索引节点和目录项在 Linux 中一切皆文件。不仅普通的文件和目录，就连块设备、套接字、管道等，也都要通过统一的文件系统来管理。 为了方便管理，Linux 文件系统为每个文件都分配两个数据结构，索引节点（index node）和目录项（directory entry）。它们主要用来记录文件的元信息和目录结构。 索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间。 目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。 索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构。目录项和索引节点的关系是多对一，你可以简单理解为，一个文件可以有多个别名。 举个例子，通过硬链接为文件创建的别名，就会对应不同的目录项，不过这些目录项本质上还是链接同一个文件，所以，它们的索引节点相同。 第一，目录项本身就是一个内存缓存，而索引节点则是存储在磁盘中的数据。在前面的 Buffer 和 Cache 原理中，我曾经提到过，为了协调慢速磁盘与快速 CPU 的性能差异，文件内容会缓存到页缓存 Cache 中。 第二，磁盘在执行文件系统格式化时，会被分成三个存储区域，超级块、索引节点区和数据块区。其中， 超级块，存储整个文件系统的状态。 索引节点区，用来存储索引节点。 数据块区，则用来存储文件数据。 虚拟文件系统 第一类是基于磁盘的文件系统，也就是把数据直接存储在计算机本地挂载的磁盘中。常见的 Ext4、XFS、OverlayFS 等，都是这类文件系统。 第二类是基于内存的文件系统，也就是我们常说的虚拟文件系统。这类文件系统，不需要任何磁盘分配存储空间，但会占用内存。我们经常用到的 /proc 文件系统，其实就是一种最常见的虚拟文件系统。此外，/sys 文件系统也属于这一类，主要向用户空间导出层次化的内核对象。 第三类是网络文件系统，也就是用来访问其他计算机数据的文件系统，比如 NFS、SMB、iSCSI 等。 这些文件系统，要先挂载到 VFS 目录树中的某个子目录（称为挂载点），然后才能访问其中的文件。拿第一类，也就是基于磁盘的文件系统为例，在安装系统时，要先挂载一个根目录（/），在根目录下再把其他文件系统（比如其他的磁盘分区、/proc 文件系统、/sys 文件系统、NFS 等）挂载进来。 文件系统 I/O 第一种，根据是否利用标准库缓存，可以把文件 I/O 分为缓冲 I/O 与非缓冲 I/O。 缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件。 非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存。 第二，根据是否利用操作系统的页缓存，可以把文件 I/O 分为直接 I/O 与非直接 I/O。 直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。 非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘。 第三，根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O： 所谓阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务。 所谓非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果。 第四，根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O： 所谓同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得 I/O 响应。 所谓异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序。 性能观测容量123 df /dev/sda1 Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda1 30308240 3167020 27124836 11% / 123df -i /dev/sda1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 3870720 157460 3713260 5% / 缓存1234cat /proc/meminfo | grep -E \"SReclaimable|Cached\" Cached: 748316 kB SwapCached: 0 kB SReclaimable: 179508 kB 1234567891011$ cat /proc/slabinfo | grep -E '^#|dentry|inode' # name &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt; xfs_inode 0 0 960 17 4 : tunables 0 0 0 : slabdata 0 0 0 ... ext4_inode_cache 32104 34590 1088 15 4 : tunables 0 0 0 : slabdata 2306 2306 0hugetlbfs_inode_cache 13 13 624 13 2 : tunables 0 0 0 : slabdata 1 1 0 sock_inode_cache 1190 1242 704 23 4 : tunables 0 0 0 : slabdata 54 54 0 shmem_inode_cache 1622 2139 712 23 4 : tunables 0 0 0 : slabdata 93 93 0 proc_inode_cache 3560 4080 680 12 2 : tunables 0 0 0 : slabdata 340 340 0 inode_cache 25172 25818 608 13 2 : tunables 0 0 0 : slabdata 1986 1986 0 dentry 76050 121296 192 21 1 : tunables 0 0 0 : slabdata 5776 5776 0 1234567891011121314# 按下 c 按照缓存大小排序，按下 a 按照活跃对象数排序 $ slabtop Active / Total Objects (% used) : 277970 / 358914 (77.4%) Active / Total Slabs (% used) : 12414 / 12414 (100.0%) Active / Total Caches (% used) : 83 / 135 (61.5%) Active / Total Size (% used) : 57816.88K / 73307.70K (78.9%) Minimum / Average / Maximum Object : 0.01K / 0.20K / 22.88K OBJS ACTIVE USE OBJ SIZE SLABS OBJ/SLAB CACHE SIZE NAME 69804 23094 0% 0.19K 3324 21 13296K dentry 16380 15854 0% 0.59K 1260 13 10080K inode_cache 58260 55397 0% 0.13K 1942 30 7768K kernfs_node_cache 485 413 0% 5.69K 97 5 3104K task_struct 1472 1397 0% 2.00K 92 16 2944K kmalloc-2048","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"02基础篇：到底应该怎么理解“平均负载”","slug":"aa_category/linux/Linux性能优化实战/02基础篇：到底应该怎么理解“平均负载”","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/Linux性能优化实战/02基础篇：到底应该怎么理解“平均负载”/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/Linux性能优化实战/02基础篇：到底应该怎么理解“平均负载”/","excerpt":"uptime平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数，它和 CPU 使用率并没有直接关系。 可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。 不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。 不可中断状态实际上是系统对进程和硬件设备的一种保护机制 平均负载其实就是平均活跃进程数。平均活跃进程数，直观上的理解就是单位时间内的活跃进程数，但它实际上是活跃进程数的指数衰减平均值。这个“指数衰减平均”的详细含义你不用计较，这只是系统的一种更快速的计算方式，你把它直接当成活跃进程数的平均值也没问题。","text":"uptime平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数，它和 CPU 使用率并没有直接关系。 可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。 不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。 不可中断状态实际上是系统对进程和硬件设备的一种保护机制 平均负载其实就是平均活跃进程数。平均活跃进程数，直观上的理解就是单位时间内的活跃进程数，但它实际上是活跃进程数的指数衰减平均值。这个“指数衰减平均”的详细含义你不用计较，这只是系统的一种更快速的计算方式，你把它直接当成活跃进程数的平均值也没问题。 既然平均的是活跃进程数，那么最理想的，就是每个 CPU 上都刚好运行着一个进程，这样每个 CPU 都得到了充分利用。比如当平均负载为 2 时，意味着什么呢？ 在只有 2 个 CPU 的系统上，意味着所有的 CPU 都刚好被完全占用。 在 4 个 CPU 的系统上，意味着 CPU 有 50% 的空闲。 而在只有 1 个 CPU 的系统中，则意味着有一半的进程竞争不到 CPU。 12# 关于 grep 和 wc 的用法请查询它们的手册或者网络搜索$ grep 'model name' /proc/cpuinfo | wc -l 当平均负载高于 CPU 数量 70% 的时候 平均负载与 CPU 使用率CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的； I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高； 大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。 实战 more /etc/issue 查看系统版本 安装 stress 和 sysstat。 stresshttps://wiki.archlinux.org/index.php/Stress_testing 123456# cpu$ stress --cpu 1 --timeout 600# io$ stress -i 1 --timeout 600# 并发$ stress -c 8 --timeout 600 sysstathttps://github.com/sysstat/sysstat 1234 # 显示所有 CPU 的指标，并在间隔 5 秒输出一组数据 mpstat -P ALL 5 1 # 间隔 5 秒后输出一组数据，-u 表示 CPU 指标pidstat -u 5","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"01如何学习Linux性能优化","slug":"aa_category/linux/Linux性能优化实战/01如何学习Linux性能优化","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/Linux性能优化实战/01如何学习Linux性能优化/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/Linux性能优化实战/01如何学习Linux性能优化/","excerpt":"学习方法论只要理解了应用程序和系统的少数几个基本原理，再进行大量的实战练习，建立起整体性能的全局观，大多数性能问题的优化就会水到渠成。","text":"学习方法论只要理解了应用程序和系统的少数几个基本原理，再进行大量的实战练习，建立起整体性能的全局观，大多数性能问题的优化就会水到渠成。 性能指标是什么？ 随着应用负载的增加，系统资源的使用也会升高，甚至达到极限。而性能问题的本质，就是系统资源已经达到瓶颈，但请求的处理却还不够快，无法支撑更多的请求。 性能分析，其实就是找出应用或系统的瓶颈，并设法去避免或者缓解它们，从而更高效地利用系统资源处理更多的请求。这包含了一系列的步骤，比如下面这六个步骤。 选择指标评估应用程序和系统的性能； 为应用程序和系统设置性能目标； 进行性能基准测试； 性能分析定位瓶颈； 优化系统和应用程序； 性能监控和告警。 学习的重点是什么？想要学习好性能分析和优化，建立整体系统性能的全局观是最核心的话题。因而， 理解最基本的几个系统知识原理；掌握必要的性能工具；通过实际的场景演练，贯穿不同的组件。 另外，我还要特别强调一点，就是性能工具的选用。有句话是这么说的，一个正确的选择胜过千百次的努力。虽然夸张了些，但是选用合适的性能工具，确实可以大大简化整个性能优化过程。在什么场景选用什么样的工具、以及怎么学会选择合适工具，都是我想教给你的东西。 怎么学更高效？ 技巧一：虽然系统的原理很重要，但在刚开始一定不要试图抓住所有的实现细节。 有哪些指标可以衡量性能？ 使用什么样的性能工具来观察指标？ 导致这些指标变化的因素等。 技巧二：边学边实践，通过大量的案例演习掌握 Linux 性能的分析和优化。 技巧三：勤思考，多反思，善总结，多问为什么。","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"如何用十条命令在一分钟内检查Linux服务器性能","slug":"aa_category/linux/如何用十条命令在一分钟内检查Linux服务器性能","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/如何用十条命令在一分钟内检查Linux服务器性能/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/如何用十条命令在一分钟内检查Linux服务器性能/","excerpt":"概述通过执行以下命令，可以在1分钟内对系统资源使用情况有个大致的了解。 uptime dmesg | tail vmstat 1 mpstat -P ALL 1 pidstat 1 iostat -xz 1 free -m sar -n DEV 1 sar -n TCP,ETCP 1 top","text":"概述通过执行以下命令，可以在1分钟内对系统资源使用情况有个大致的了解。 uptime dmesg | tail vmstat 1 mpstat -P ALL 1 pidstat 1 iostat -xz 1 free -m sar -n DEV 1 sar -n TCP,ETCP 1 top 其中一些命令需要安装sysstat包，有一些由procps包提供。这些命令的输出，有助于快速定位性能瓶颈，检查出所有资源（CPU、内存、磁盘IO等）的利用率（utilization）、饱和度（saturation）和错误（error）度量，也就是所谓的USE方法。 下面我们来逐一介绍下这些命令，有关这些命令更多的参数和说明，请参照命令的手册。 uptime$ uptime 23:51:26 up 21:31, 1 user, load average: 30.02, 26.43, 19.02 这个命令可以快速查看机器的负载情况。在Linux系统中，这些数据表示等待CPU资源的进程和阻塞在不可中断IO进程（进程状态为D）的数量。这些数据可以让我们对系统资源使用有一个宏观的了解。 命令的输出分别表示1分钟、5分钟、15分钟的平均负载情况。通过这三个数据，可以了解服务器负载是在趋于紧张还是趋于缓解。如果1分钟平均负载很高，而15分钟平均负载很低，说明服务器正在命令高负载情况，需要进一步排查CPU资源都消耗在了哪里。反之，如果15分钟平均负载很高，1分钟平均负载较低，则有可能是CPU资源紧张时刻已经过去。 上面例子中的输出，可以看见最近1分钟的平均负载非常高，且远高于最近15分钟负载，因此我们需要继续排查当前系统中有什么进程消耗了大量的资源。可以通过下文将会介绍的vmstat、mpstat等命令进一步排查。 dmesg丨tail$ dmesg | tail [1880957.563150] perl invoked oom-killer: gfp_mask=0x280da, order=0, oom_score_adj=0 […] [1880957.563400] Out of memory: Kill process 18694 (perl) score 246 or sacrifice child [1880957.563408] Killed process 18694 (perl) total-vm:1972392kB, anon-rss:1953348kB, file-rss:0kB [2320864.954447] TCP: Possible SYN flooding on port 7001. Dropping request. Check SNMP counters. 该命令会输出系统日志的最后10行。示例中的输出，可以看见一次内核的oom kill和一次TCP丢包。这些日志可以帮助排查性能问题。千万不要忘了这一步。 vmstat 1$ vmstat 1 procs ———memory———- —swap– —–io—- -system– ——cpu—– r b swpd free buff cache si so bi bo in cs us sy id wa st 34 0 0 200889792 73708 591828 0 0 0 5 6 10 96 1 3 0 0 32 0 0 200889920 73708 591860 0 0 0 592 13284 4282 98 1 1 0 0 32 0 0 200890112 73708 591860 0 0 0 0 9501 2154 99 1 0 0 0 32 0 0 200889568 73712 591856 0 0 0 48 11900 2459 99 0 0 0 0 32 0 0 200890208 73712 591860 0 0 0 0 15898 4840 98 1 1 0 0 ^C vmstat(8) 命令，每行会输出一些系统核心指标，这些指标可以让我们更详细的了解系统状态。后面跟的参数1，表示每秒输出一次统计信息，表头提示了每一列的含义，这几介绍一些和性能调优相关的列： r：等待在CPU资源的进程数。这个数据比平均负载更加能够体现CPU负载情况，数据中不包含等待IO的进程。如果这个数值大于机器CPU核数，那么机器的CPU资源已经饱和。 free：系统可用内存数（以千字节为单位），如果剩余内存不足，也会导致系统性能问题。下文介绍到的free命令，可以更详细的了解系统内存的使用情况。 si，so：交换区写入和读取的数量。如果这个数据不为0，说明系统已经在使用交换区（swap），机器物理内存已经不足。 us, sy, id, wa, st：这些都代表了CPU时间的消耗，它们分别表示用户时间（user）、系统（内核）时间（sys）、空闲时间（idle）、IO等待时间（wait）和被偷走的时间（stolen，一般被其他虚拟机消耗）。 上述这些CPU时间，可以让我们很快了解CPU是否出于繁忙状态。一般情况下，如果用户时间和系统时间相加非常大，CPU出于忙于执行指令。如果IO等待时间很长，那么系统的瓶颈可能在磁盘IO。 示例命令的输出可以看见，大量CPU时间消耗在用户态，也就是用户应用程序消耗了CPU时间。这不一定是性能问题，需要结合r队列，一起分析。 mpstat -P ALL 1$ mpstat -P ALL 1 Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 x86_64 (32 CPU) 07:38:49 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 07:38:50 PM all 98.47 0.00 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.78 07:38:50 PM 0 96.04 0.00 2.97 0.00 0.00 0.00 0.00 0.00 0.00 0.99 07:38:50 PM 1 97.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 2.00 07:38:50 PM 2 98.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 07:38:50 PM 3 96.97 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.03 […] 该命令可以显示每个CPU的占用情况，如果有一个CPU占用率特别高，那么有可能是一个单线程应用程序引起的。 pidstat 1$ pidstat 1 Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 x86_64 (32 CPU) 07:41:02 PM UID PID %usr %system %guest %CPU CPU Command 07:41:03 PM 0 9 0.00 0.94 0.00 0.94 1 rcuos/0 07:41:03 PM 0 4214 5.66 5.66 0.00 11.32 15 mesos-slave 07:41:03 PM 0 4354 0.94 0.94 0.00 1.89 8 java 07:41:03 PM 0 6521 1596.23 1.89 0.00 1598.11 27 java 07:41:03 PM 0 6564 1571.70 7.55 0.00 1579.25 28 java 07:41:03 PM 60004 60154 0.94 4.72 0.00 5.66 9 pidstat 07:41:03 PM UID PID %usr %system %guest %CPU CPU Command 07:41:04 PM 0 4214 6.00 2.00 0.00 8.00 15 mesos-slave 07:41:04 PM 0 6521 1590.00 1.00 0.00 1591.00 27 java07:41:04 PM 0 6564 1573.00 10.00 0.00 1583.00 28 java 07:41:04 PM 108 6718 1.00 0.00 0.00 1.00 0 snmp-pass 07:41:04 PM 60004 60154 1.00 4.00 0.00 5.00 9 pidstat ^C pidstat命令输出进程的CPU占用率，该命令会持续输出，并且不会覆盖之前的数据，可以方便观察系统动态。如上的输出，可以看见两个JAVA进程占用了将近1600%的CPU时间，既消耗了大约16个CPU核心的运算资源。 iostat -xz 1$ iostat -xz 1 Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 x86_64 (32 CPU) avg-cpu: %user %nice %system %iowait %steal %idle ​ 73.96 0.00 3.73 0.03 0.06 22.21 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util xvda 0.00 0.23 0.21 0.18 4.52 2.08 34.37 0.00 9.98 13.80 5.42 2.44 0.09 xvdb 0.01 0.00 1.02 8.94 127.97 598.53 145.79 0.00 0.43 1.78 0.28 0.25 0.25 xvdc 0.01 0.00 1.02 8.86 127.79 595.94 146.50 0.00 0.45 1.82 0.30 0.27 0.26 dm-0 0.00 0.00 0.69 2.32 10.47 31.69 28.01 0.01 3.23 0.71 3.98 0.13 0.04 dm-1 0.00 0.00 0.00 0.94 0.01 3.78 8.00 0.33 345.84 0.04 346.81 0.01 0.00 dm-2 0.00 0.00 0.09 0.07 1.35 0.36 22.50 0.00 2.55 0.23 5.62 1.78 0.03 […] ^C iostat命令主要用于查看机器磁盘IO情况。该命令输出的列，主要含义是： r/s, w/s, rkB/s, wkB/s：分别表示每秒读写次数和每秒读写数据量（千字节）。读写量过大，可能会引起性能问题。 await：IO操作的平均等待时间，单位是毫秒。这是应用程序在和磁盘交互时，需要消耗的时间，包括IO等待和实际操作的耗时。如果这个数值过大，可能是硬件设备遇到了瓶颈或者出现故障。 avgqu-sz：向设备发出的请求平均数量。如果这个数值大于1，可能是硬件设备已经饱和（部分前端硬件设备支持并行写入）。 %util：设备利用率。这个数值表示设备的繁忙程度，经验值是如果超过60，可能会影响IO性能（可以参照IO操作平均等待时间）。如果到达100%，说明硬件设备已经饱和。 如果显示的是逻辑设备的数据，那么设备利用率不代表后端实际的硬件设备已经饱和。值得注意的是，即使IO性能不理想，也不一定意味这应用程序性能会不好，可以利用诸如预读取、写缓存等策略提升应用性能。 free -m$ free -m ​ total used free shared buffers cached Mem: 245998 24545 221453 83 59 541 -/+ buffers/cache: 23944 222053 Swap: 0 0 0 free命令可以查看系统内存的使用情况，-m参数表示按照兆字节展示。最后两列分别表示用于IO缓存的内存数，和用于文件系统页缓存的内存数。需要注意的是，第二行-/+ buffers/cache，看上去缓存占用了大量内存空间。 这是Linux系统的内存使用策略，尽可能的利用内存，如果应用程序需要内存，这部分内存会立即被回收并分配给应用程序。因此，这部分内存一般也被当成是可用内存。 如果可用内存非常少，系统可能会动用交换区（如果配置了的话），这样会增加IO开销（可以在iostat命令中提现），降低系统性能。 sar -n DEV 1$ sar -n DEV 1 Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 x86_64 (32 CPU) 12:16:48 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil 12:16:49 AM eth0 18763.00 5032.00 20686.42 478.30 0.00 0.00 0.00 0.00 12:16:49 AM lo 14.00 14.00 1.36 1.36 0.00 0.00 0.00 0.00 12:16:49 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12:16:49 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil 12:16:50 AM eth0 19763.00 5101.00 21999.10 482.56 0.00 0.00 0.00 0.00 12:16:50 AM lo 20.00 20.00 3.25 3.25 0.00 0.00 0.00 0.00 12:16:50 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ^C sar命令在这里可以查看网络设备的吞吐率。在排查性能问题时，可以通过网络设备的吞吐量，判断网络设备是否已经饱和。如示例输出中，eth0网卡设备，吞吐率大概在22 Mbytes/s，既176 Mbits/sec，没有达到1Gbit/sec的硬件上限。 sar -n TCP,ETCP 1$ sar -n TCP,ETCP 1 Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 x86_64 (32 CPU) 12:17:19 AM active/s passive/s iseg/s oseg/s 12:17:20 AM 1.00 0.00 10233.00 18846.00 12:17:19 AM atmptf/s estres/s retrans/s isegerr/s orsts/s 12:17:20 AM 0.00 0.00 0.00 0.00 0.00 12:17:20 AM active/s passive/s iseg/s oseg/s 12:17:21 AM 1.00 0.00 8359.00 6039.00 12:17:20 AM atmptf/s estres/s retrans/s isegerr/s orsts/s 12:17:21 AM 0.00 0.00 0.00 0.00 0.00 ^C sar命令在这里用于查看TCP连接状态，其中包括： active/s：每秒本地发起的TCP连接数，既通过connect调用创建的TCP连接； passive/s：每秒远程发起的TCP连接数，即通过accept调用创建的TCP连接； retrans/s：每秒TCP重传数量； TCP连接数可以用来判断性能问题是否由于建立了过多的连接，进一步可以判断是主动发起的连接，还是被动接受的连接。TCP重传可能是因为网络环境恶劣，或者服务器压力过大导致丢包。 top$ top top - 00:15:40 up 21:56, 1 user, load average: 31.09, 29.87, 29.92 Tasks: 871 total, 1 running, 868 sleeping, 0 stopped, 2 zombie %Cpu(s): 96.8 us, 0.4 sy, 0.0 ni, 2.7 id, 0.1 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem: 25190241+total, 24921688 used, 22698073+free, 60448 buffers KiB Swap: 0 total, 0 used, 0 free. 554208 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 20248 root 20 0 0.227t 0.012t 18748 S 3090 5.2 29812:58 java 4213 root 20 0 2722544 64640 44232 S 23.5 0.0 233:35.37 mesos-slave 66128 titancl+ 20 0 24344 2332 1172 R 1.0 0.0 0:00.07 top 5235 root 20 0 38.227g 547004 49996 S 0.7 0.2 2:02.74 java 4299 root 20 0 20.015g 2.682g 16836 S 0.3 1.1 33:14.42 java 1 root 20 0 33620 2920 1496 S 0.0 0.0 0:03.82 init ​ 2 root 20 0 0 0 0 S 0.0 0.0 0:00.02 kthreadd ​ 3 root 20 0 0 0 0 S 0.0 0.0 0:05.35 ksoftirqd/0 ​ 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H ​ 6 root 20 0 0 0 0 S 0.0 0.0 0:06.94 kworker/u256:0 ​ 8 root 20 0 0 0 0 S 0.0 0.0 2:38.05 rcu_sched top命令包含了前面好几个命令的检查的内容。比如系统负载情况（uptime）、系统内存使用情况（free）、系统CPU使用情况（vmstat）等。因此通过这个命令，可以相对全面的查看系统负载的来源。同时，top命令支持排序，可以按照不同的列排序，方便查找出诸如内存占用最多的进程、CPU占用率最高的进程等。 但是，top命令相对于前面一些命令，输出是一个瞬间值，如果不持续盯着，可能会错过一些线索。这时可能需要暂停top命令刷新，来记录和比对数据。 总结排查Linux服务器性能问题还有很多工具，上面介绍的一些命令，可以帮助我们快速的定位问题。例如前面的示例输出，多个证据证明有JAVA进程占用了大量CPU资源，之后的性能调优就可以针对应用程序进行。","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"33-34关于 Linux 网络，你必须知道这些","slug":"aa_category/linux/Linux性能优化实战/33-34关于 Linux 网络，你必须知道这些","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/Linux性能优化实战/33-34关于 Linux 网络，你必须知道这些/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/Linux性能优化实战/33-34关于 Linux 网络，你必须知道这些/","excerpt":"网络模型","text":"网络模型 开放式系统互联通信参考模型(（Open System Interconnection Reference Model) 应用层，负责为应用程序提供统一的接口。 表示层，负责把数据转换成兼容接收系统的格式。 会话层，负责维护计算机之间的通信连接。 传输层，负责为数据加上传输表头，形成数据包。 网络层，负责数据的路由和转发。 数据链路层，负责 MAC 寻址、错误侦测和改错。 物理层，负责在物理网络中传输数据帧。 TCP/IP 应用层，负责向用户提供一组应用程序，比如 HTTP、FTP、DNS 等。 传输层，负责端到端的通信，比如 TCP、UDP 等。 网络层，负责网络包的封装、寻址和路由，比如 IP、ICMP 等。 网络接口层，负责网络包在物理网络中的传输，比如 MAC 寻址、错误侦测以及通过网卡传输网络帧等。 七层和四层负载均衡，对应的分别是 OSI 模型中的应用层和传输层（而它们对应到 TCP/IP 模型中，实际上是四层和三层）。 Linux 网络栈有了 TCP/IP 模型后，在进行网络传输时，数据包就会按照协议栈，对上一层发来的数据进行逐层处理；然后封装上该层的协议头，再发送给下一层。 而封装做的事情就很简单了，只是在原来的负载前后，增加固定格式的元数据，原始的负载数据并不会被修改。 物理链路中并不能传输任意大小的数据包。网络接口配置的最大传输单元（MTU），就规定了最大的 IP 包大小。在我们最常用的以太网中，MTU 默认值是 1500（这也是 Linux 的默认值）。 一旦网络包超过 MTU 的大小，就会在网络层分片，以保证分片后的 IP 包不大于 MTU 值。显然，MTU 越大，需要的分包也就越少，自然，网络吞吐能力就越好。 Linux 网络收发流程 当一个网络帧到达网卡后，网卡会通过 DMA 方式，把这个网络包放到收包队列中；然后通过硬中断，告诉中断处理程序已经收到了网络包。 接着，网卡中断处理程序会为网络帧分配内核数据结构（sk_buff），并将其拷贝到 sk_buff 缓冲区中；然后再通过软中断，通知内核收到了新的网络帧。 接下来，内核协议栈从缓冲区中取出网络帧，并通过网络协议栈，从下到上逐层处理这个网络帧。比如， 在链路层检查报文的合法性，找出上层协议的类型（比如 IPv4 还是 IPv6），再去掉帧头、帧尾，然后交给网络层。 网络层取出 IP 头，判断网络包下一步的走向，比如是交给上层处理还是转发。当网络层确认这个包是要发送到本机后，就会取出上层协议的类型（比如 TCP 还是 UDP），去掉 IP 头，再交给传输层处理。 传输层取出 TCP 头或者 UDP 头后，根据 &lt; 源 IP、源端口、目的 IP、目的端口 &gt; 四元组作为标识，找出对应的 Socket，并把数据拷贝到 Socket 的接收缓存中。 最后，应用程序就可以使用 Socket 接口，读取到新接收到的数据了。 网络包的发送流程首先，应用程序调用 Socket API（比如 sendmsg）发送网络包。 由于这是一个系统调用，所以会陷入到内核态的套接字层中。套接字层会把数据包放到 Socket 发送缓冲区中。 接下来，网络协议栈从 Socket 发送缓冲区中，取出数据包；再按照 TCP/IP 栈，从上到下逐层处理。比如，传输层和网络层，分别为其增加 TCP 头和 IP 头，执行路由查找确认下一跳的 IP，并按照 MTU 大小进行分片。 分片后的网络包，再送到网络接口层，进行物理地址寻址，以找到下一跳的 MAC 地址。然后添加帧头和帧尾，放到发包队列中。这一切完成后，会有软中断通知驱动程序：发包队列中有新的网络帧需要发送。 最后，驱动程序通过 DMA ，从发包队列中读出网络帧，并通过物理网卡把它发送出去。 性能指标 带宽，表示链路的最大传输速率，单位通常为 b/s （比特 / 秒）。 吞吐量，表示单位时间内成功传输的数据量，单位通常为 b/s（比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽限制，而吞吐量 / 带宽，也就是该网络的使用率。 延时，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。在不同场景中，这一指标可能会有不同含义。比如，它可以表示，建立连接需要的时间（比如 TCP 握手延时），或一个数据包往返所需的时间（比如 RTT）。 延时，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。在不同场景中，这一指标可能会有不同含义。比如，它可以表示，建立连接需要的时间（比如 TCP 握手延时），或一个数据包往返所需的时间（比如 RTT）。 网络可用性 并发连接数 丢包率 重传率 网络配置12ifconfig enp0s3ip -s addr show dev enp0s3 第一，网络接口的状态标志。ifconfig 输出中的 RUNNING ，或 ip 输出中的 LOWER_UP ，都表示物理网络是连通的，即网卡已经连接到了交换机或者路由器中。如果你看不到它们，通常表示网线被拔掉了。 第二，MTU 的大小。MTU 默认大小是 1500，根据网络架构的不同（比如是否使用了 VXLAN 等叠加网络），你可能需要调大或者调小 MTU 的数值。 第三，网络接口的 IP 地址、子网以及 MAC 地址。这些都是保障网络功能正常工作所必需的，你需要确保配置正确。 第四，网络收发的字节数、包数、错误数以及丢包情况，特别是 TX 和 RX 部分的 errors、dropped、overruns、carrier 以及 collisions 等指标不为 0 时，通常表示出现了网络 I/O 问题。其中： errors 表示发生错误的数据包数，比如校验错误、帧同步错误等； dropped 表示丢弃的数据包数，即数据包已经收到了 Ring Buffer，但因为内存不足等原因丢包； overruns 表示超限数据包数，即网络 I/O 速度过快，导致 Ring Buffer 中的数据包来不及处理（队列满）而导致的丢包； carrier 表示发生 carrirer 错误的数据包数，比如双工模式不匹配、物理电缆出现问题等； collisions 表示碰撞数据包数。 套接字信息1234567891011# head -n 3 表示只显示前面 3 行# -l 表示只显示监听套接字# -n 表示显示数字地址和端口 (而不是名字)# -p 表示显示进程信息netstat -nlp | head -n 3# -l 表示只显示监听套接字# -t 表示只显示 TCP 套接字# -n 表示显示数字地址和端口 (而不是名字)# -p 表示显示进程信息 ss -ltnp | head -n 3 123# 协议栈统计信息netstat -s$ ss -s 网络吞吐和 PPS12# 数字 1 表示每隔 1 秒输出一组数据sar -n DEV 1 rxpck/s 和 txpck/s 分别是接收和发送的 PPS，单位为包 / 秒。 rxkB/s 和 txkB/s 分别是接收和发送的吞吐量，单位是 KB/ 秒。 rxcmp/s 和 txcmp/s 分别是接收和发送的压缩数据包数，单位是包 / 秒。 %ifutil 是网络接口的使用率，即半双工模式下为 (rxkB/s+txkB/s)/Bandwidth，而全双工模式下为 max(rxkB/s, txkB/s)/Bandwidth。 1ethtool eth0 | grep Speed 连通性和延时12# -c3 表示发送三次 ICMP 包后停止ping -c3 114.114.114.114 第一部分，是每个 ICMP 请求的信息，包括 ICMP 序列号（icmp_seq）、TTL（生存时间，或者跳数）以及往返延时。 第二部分，则是三次 ICMP 请求的汇总。","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"03-04基础篇：经常说的 CPU 上下文切换是什么意思","slug":"aa_category/linux/Linux性能优化实战/03-04基础篇：经常说的 CPU 上下文切换是什么意思","date":"2019-04-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/04/12/aa_category/linux/Linux性能优化实战/03-04基础篇：经常说的 CPU 上下文切换是什么意思/","link":"","permalink":"https://liyong.ac.cn/2019/04/12/aa_category/linux/Linux性能优化实战/03-04基础篇：经常说的 CPU 上下文切换是什么意思/","excerpt":"CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 cpu上下文","text":"CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 cpu上下文 知道了什么是 CPU 上下文，我想你也很容易理解 CPU 上下文切换。CPU 上下文切换，就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。 而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。 根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景，也就是进程上下文切换、线程上下文切换以及中断上下文切换。 进程上下文切换因此，进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的虚拟内存、栈等保存下来；而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。 其一，为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。 其二，进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。 其三，当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。 其四，当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行 最后一个，发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。 线程上下文切换线程与进程最大的区别在于，线程是调度的基本单位，而进程则是资源拥有的基本单位。说白了，所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源。所以，对于线程和进程，我们可以这么理解： 当进程只有一个线程时，可以认为进程就等于线程。 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。 第一种， 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。 第二种，前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。 中断上下文切换 为了快速响应硬件的事件，中断处理会打断进程的正常调度和执行，转而调用中断处理程序，响应设备事件。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行。 跟进程上下文不同，中断上下文切换并不涉及到进程的用户态。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等。 对同一个 CPU 来说，中断处理比进程拥有更高的优先级 怎么查看系统的上下文切换情况vmstat12345# 每隔 5 秒输出 1 组数据$ vmstat 5procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 7005360 91564 818900 0 0 0 0 25 33 0 0 100 0 0 cs（context switch）是每秒上下文切换的次数,The number of context switches per second。 in（interrupt）则是每秒中断的次数,The number of interrupts per second, including the clock。 r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。 b（Blocked）则是处于不可中断睡眠状态的进程数。 1234567# 每隔 5 秒输出 1 组数据$ pidstat -w 5Linux 4.15.0 (ubuntu) 09/23/18 _x86_64_ (2 CPU)08:18:26 UID PID cswch/s nvcswch/s Command08:18:31 0 1 0.20 0.00 systemd08:18:31 0 8 5.40 0.00 rcu_sched 这个结果中有两列内容是我们的重点关注对象。一个是 cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数，另一个则是 nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数。 所谓自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。 而非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。 实战sysbenchhttps://github.com/akopytov/sysbench 具体操作12# 以 10 个线程运行 5 分钟的基准测试，模拟多线程切换的问题sysbench --threads=10 --max-time=300 threads run 12# 每隔 1 秒输出 1 组数据（需要 Ctrl+C 才结束）$ vmstat 3 123# 每隔 1 秒输出 1 组数据（需要 Ctrl+C 才结束）# -w 参数表示输出进程切换指标，而 -u 参数则表示输出 CPU 使用指标$ pidstat -w -u 3 123# 每隔 1 秒输出一组数据（需要 Ctrl+C 才结束）# -wt 参数表示输出线程的上下文切换指标$ pidstat -wt 1 123456# -d 参数表示高亮显示变化的区域$ watch -d cat /proc/interrupts CPU0 CPU1...RES: 2450431 5279697 Rescheduling interrupts... 观察一段时间，你可以发现，变化速度最快的是重调度中断（RES），这个中断类型表示，唤醒空闲状态的 CPU 来调度新的任务运行。这是多处理器系统（SMP）中，调度器用来分散任务到不同 CPU 的机制，通常也被称为处理器间中断（Inter-Processor Interrupts，IPI） 自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I/O 等其他问题； 非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU 的确成了瓶颈； 中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件来分析具体的中断类型。","categories":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://liyong.ac.cn/tags/linux/"}]},{"title":"4深入浅出索引","slug":"aa_category/db/mysql/mysql实战/4深入浅出索引","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/4深入浅出索引/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/4深入浅出索引/","excerpt":"哈希表 和hashmap的思路一样 区间查询比较慢 适用于等值查询的场景","text":"哈希表 和hashmap的思路一样 区间查询比较慢 适用于等值查询的场景 有序数组 等值查询和范围查询性能非常优秀 但是更新成本太高 搜索树InnoDB 的索引模型12345create table T(id int primary key, k int not null, name varchar(16),index (k))engine=InnoDB; 表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。 基于主键索引和普通索引的查询有什么区别？ 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树； 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。 索引维护显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的? 只有一个索引； 该索引必须是唯一索引 如何避免长事务对业务的影响？ 确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例） 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill； Percona 的 pt-kill 这个工具不错，推荐使用； 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题； 把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。 General_log开启 general log 将所有到达MySQL Server的SQL语句记录下来。 一般不会开启开功能，因为log的量会非常庞大。但个别情况下可能会临时的开一会儿general log以供排障使用。相关参数一共有3：general_log、log_output、general_log_file 12345678910111213show variables like 'general_log'; -- 查看日志是否开启set global general_log=on; -- 开启日志功能show variables like 'general_log_file'; -- 看看日志文件保存位置set global general_log_file='tmp/general.lg'; -- 设置日志文件保存位置show variables like 'log_output'; -- 看看日志输出类型 table或fileset global log_output='table'; -- 设置输出类型为 tableset global log_output='file'; -- 设置输出类型为file log_output=’FILE’ 表示将日志存入文件,默认值是FILE log_output=’TABLE’表示将日志存入数据库,这样日志信息就会被写入到mysql.slow_log表中.","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"13Java内存模型","slug":"aa_category/se/深入拆解 Java 虚拟机/13Java内存模型","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/13Java内存模型/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/13Java内存模型/","excerpt":"1234567891011int a=0, b=0;public void method1() &#123; int r2 = a; b = 1;&#125;public void method2() &#123; int r1 = b; a = 2;&#125;","text":"1234567891011int a=0, b=0;public void method1() &#123; int r2 = a; b = 1;&#125;public void method2() &#123; int r1 = b; a = 2;&#125; 多线程乱序 及时编译器的重排序 处理器的乱序执行 内存系统的重排序 Java 内存模型和 happens-before关系 单线程内，编写顺序和执行顺序一致 解锁操作在加锁操作之后 volatile字段操读作先于写操作 线程的start方法先行与此线程的每一个动作 线程的所有操作先行于线程的终止检测 对线程的interrupt()方法调用优先于被中断线程的代码检测到中断事件 对象的构造方法先行于finalize()方法 传递性，A先行于B,B先行于C,那么A先行于C Java 内存模型的底层实现 Java 内存模型是通过内存屏障（memory barrier）来禁止重排序的。 对于即时编译器来说，它会针对前面提到的每一个 happens-before 关系，向正在编译的目标方法中插入相应的读读、读写、写读以及写写内存屏障。 这些内存屏障会限制即时编译器的重排序操作。以 volatile 字段访问为例，所插入的内存屏障将不允许 volatile 字段写操作之前的内存访问被重排序至其之后；也将不允许 volatile 字段读操作之后的内存访问被重排序至其之前。 在 X86_64 平台上，只有 volatile 字段的写操作会强制刷新缓存。因此，理想情况下对 volatile 字段的使用应当多读少写，并且应当只有一个线程进行写操作。 volatile 字段的另一个特性是即时编译器无法将其分配到寄存器里。换句话说，volatile 字段的每次访问均需要直接从内存中读写。 final 实例字段则涉及新建对象的发布问题。当一个对象包含 final 实例字段时，我们希望其他线程只能看到已初始化的 final 实例字段。 因此，即时编译器会在 final 字段的写操作后插入一个写写屏障，以防某些优化将新建对象的发布（即将实例对象写入一个共享引用中）重排序至 final 字段的写操作之前。在 X86_64 平台上，写写屏障是空操作。 reference https://docs.oracle.com/javase/specs/jls/se10/html/jls-17.html#jls-17.4 http://gee.cs.oswego.edu/dl/jmm/cookbook.html https://blogs.oracle.com/dave/instruction-selection-for-volatile-fences-:-mfence-vs-lock:add https://vlkan.com/blog/post/2014/02/14/java-safe-publication/ https://wiki.openjdk.java.net/display/CodeTools/jcstress http://hg.openjdk.java.net/code-tools/jcstress/file/64f2cf32fa0a/tests-custom/src/main/java/org/openjdk/jcstress/tests/unsafe/UnsafePublication.java 总结有点深，以后反过来看，暂时放置","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"11-12垃圾回收","slug":"aa_category/se/深入拆解 Java 虚拟机/11-12垃圾回收","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/11-12垃圾回收/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/11-12垃圾回收/","excerpt":"引用计数法和可达性分析引用计数循环引用的问题 可达性分析 方法栈桢中的局部变量 已加载类的静态变量 JNI handles 已启动未停止的java线程","text":"引用计数法和可达性分析引用计数循环引用的问题 可达性分析 方法栈桢中的局部变量 已加载类的静态变量 JNI handles 已启动未停止的java线程 stop-the-world 以及安全点 stop-the-world ，停止其他非垃圾回收线程，直到完成垃圾回收。 安全点并不是让线程停下来，而是到达一个稳定的状态（堆栈不发生变化） 对于解释执行，字节码和字节码都是安全点。java虚拟机的做法是，当有安全点的请求是，执行一条字节码进行一次安全点检查 对于直接运行的机器码，在及时编译时插入安全安全检测点。通常在方法的出口，以及非技术循环的循环回边插入安全检测点 垃圾回收的三种方式清除 内存碎片 分配效率低 压缩 性能开销较大 copy 使用率低下 demo123456789101112131415161718192021222324252627282930313233public class SafePointTest &#123;// time java SafepointTestp/// 你还可以使用如下几个选项 // -XX:+PrintGC// -XX:+PrintGCApplicationStoppedTime// -XX:+PrintSafepointStatistics// -XX:+UseCountedLoopSafepoints// public class SafepointTest &#123; static double sum = 0; public static void foo() &#123; for (int i = 0; i &lt; 0x77777777; i++) &#123; sum += Math.sqrt(i); &#125; &#125; public static void bar() &#123; for (int i = 0; i &lt; 50_000_000; i++) &#123; new Object().hashCode(); &#125; &#125; public static void main(String[] args) &#123; Runnable foo = SafePointTest::foo; new Thread(foo).start(); new Thread(SafePointTest::bar).start(); &#125; &#125;//&#125; 虚拟机推划分 TLAB(Thread Local Allocation Buffer) 卡表 推分为512Byte的卡，通过卡表定位卡 截获字节码的写操作容易 截获机器码的写操作不容器，采取宁可错杀一万不肯放过一个的策略 GC日志122019-01-05T22:21:02.445+0800: [GC (Allocation Failure) [PSYoungGen: 76800K-&gt;12792K(89600K)] 76800K-&gt;35744K(113664K), 0.0438584 secs][Times: user=0.03 sys=0.05, real=0.04 secs] 2019-01-05T22:21:02.489+0800: [Full GC (Ergonomics) [PSYoungGen: 12792K-&gt;12767K(89600K)] [ParOldGen: 22952K-&gt;22745K(86016K)] 35744K-&gt;35512K(175616K), [Metaspace: 3457K-&gt;3457K(1056768K)], 0.2233472 secs][Times: user=0.19 sys=0.00, real=0.22 secs] 123456789Heap PSYoungGen total 69120K, used 67955K [0x00000000f9c00000, 0x0000000100000000, 0x0000000100000000) eden space 35840K, 97% used [0x00000000f9c00000,0x00000000fbe3cd10,0x00000000fbf00000) from space 33280K, 98% used [0x00000000fbf00000,0x00000000fdf20000,0x00000000fdf80000) to space 33280K, 0% used [0x00000000fdf80000,0x00000000fdf80000,0x0000000100000000) ParOldGen total 123904K, used 99900K [0x0000000087400000, 0x000000008ed00000, 0x00000000f9c00000) object space 123904K, 80% used [0x0000000087400000,0x000000008d58f1d8,0x000000008ed00000) Metaspace used 9344K, capacity 9658K, committed 9984K, reserved 1058816K class space used 1086K, capacity 1160K, committed 1280K, reserved 1048576K VM Options -XX:+PrintGC 打印GC日志 -XX:+PrintGCApplicationStoppedTime -XX:+PrintSafepointStatistics -XX:+UseCountedLoopSafepoints XX:+UsePSAdaptiveSurvivorSizePolicy -XX:SurvivorRatio -XX:+UseTLAB 引用 http://psy-lob-saw.blogspot.com/2015/12/safepoints.html http://openjdk.java.net/jeps/122 http://psy-lob-saw.blogspot.com/2014/10/the-jvm-write-barrier-card-marking.html https://blogs.oracle.com/dave/false-sharing-induced-by-card-table-marking http://openjdk.java.net/jeps/291","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"10Java对象的内存布局","slug":"aa_category/se/深入拆解 Java 虚拟机/10Java对象的内存布局","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/10Java对象的内存布局/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/10Java对象的内存布局/","excerpt":"对象的内存布局 对象头 存储对象自身的运行时数据，Mark Word 类型指针，对象指向类元数据的指针（32bit–&gt;32bit，64bit–&gt;64bit(未开启压缩指针)，32bit(开启压缩指针)），JVM通过这个指针来确定这个对象是哪个类的实例（根据对象确定其Class的指针） 实例数据，对象真正存储的有效信息 对齐填充，JVM要求对象的大小必须是8的整数倍，若不是，需要补位对齐","text":"对象的内存布局 对象头 存储对象自身的运行时数据，Mark Word 类型指针，对象指向类元数据的指针（32bit–&gt;32bit，64bit–&gt;64bit(未开启压缩指针)，32bit(开启压缩指针)），JVM通过这个指针来确定这个对象是哪个类的实例（根据对象确定其Class的指针） 实例数据，对象真正存储的有效信息 对齐填充，JVM要求对象的大小必须是8的整数倍，若不是，需要补位对齐 初始化方式 new，新建对象覆盖了所有父类的实例字段，虽然无法直接访问父类的私有字段，但是可以覆盖父类的私有字段，也可以为父类分配内存 反射机制 Oject.clone 反序列化 压缩指针 -XX:+UseCompressedOops 将堆中原本 64 位的 Java 对象指针压缩成 32 位的，对象头中的类型指针也会被压缩成 32 位,作用于引用类型的字段,引用类型数组。 对象头包含类型指针和标记字段 字段填充让字段只出现在同一CPU缓存行 64位虚拟中，对象头又16位字节转变成12位，只转变类型指针 默认情况下，Java 虚拟机堆中对象的起始地址需要对齐至 8 的倍数。如果一个对象用不到 8N 个字节，那么空白的那部分空间就浪费掉了。这些浪费掉的空间我们称之为对象间的填充（padding）。 字段重排列 如果一个字段占据C字节，那么该字段的偏移量需要对齐至NC 子类继承的偏移量需要和父类对应字段的偏移量保持一致 Contended 解决虚共享问题 JOL12345&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jol&lt;/groupId&gt; &lt;artifactId&gt;jol-core&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt; 引用 https://wiki.openjdk.java.net/display/HotSpot/CompressedOops http://openjdk.java.net/jeps/142","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"07JVM是如何实现反射的","slug":"aa_category/se/深入拆解 Java 虚拟机/07JVM是如何实现反射的","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/07JVM是如何实现反射的/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/07JVM是如何实现反射的/","excerpt":"反射调用的实现 -Dsun.reflect.inflationThreshold 设置本地调用的阀值 -verbose:class -XX:+PrintGC 打印GC 在实践中需要保存反射得到的结果","text":"反射调用的实现 -Dsun.reflect.inflationThreshold 设置本地调用的阀值 -verbose:class -XX:+PrintGC 打印GC 在实践中需要保存反射得到的结果 demo112345678910public class Test &#123; public static void target(int i) &#123; new Exception(\"#\" + i).printStackTrace(); &#125; public static void main(String[] args) throws Exception &#123; Class&lt;?&gt; klass = Class.forName(\"reflect.Test\"); Method method = klass.getMethod(\"target\", int.class); method.invoke(null, 0); &#125;&#125; 123456target:22, Test (reflect)invoke0(Method, Object, Object[]):-1, NativeMethodAccessorImpl (sun.reflect)invoke(Object, Object[]):62, NativeMethodAccessorImpl (sun.reflect)invoke(Object, Object[]):43, DelegatingMethodAccessorImpl (sun.reflect)invoke(Object, Object[]):498, Method (java.lang.reflect)main(String[]):28, Test (reflect) 这里你可能会疑问，为什么反射调用还要采取委派实现作为中间层？直接交给本地实现不可以么？ 其实，Java 的反射调用机制还设立了另一种动态生成字节码的实现（下称动态实现），直接使用 invoke 指令来调用目标方法。之所以采用委派实现，便是为了能够在本地实现以及动态实现中切换。 12345678910// 动态实现的伪代码，这里只列举了关键的调用逻辑，其实它还包括调用者检测、参数检测的字节码。package jdk.internal.reflect;public class GeneratedMethodAccessor1 extends ... &#123; @Overrides public Object invoke(Object obj, Object[] args) throws ... &#123; Test.target((int) args[0]); return null; &#125;&#125; 反射调用的开销 变长参数Object数组 自动装箱 getMethod Class.getMethod 则会遍历该类的公有方法。如果没有匹配到，它还将遍历父类的公有方法。可想而知，这两个操作都非常费时。 以 getMethod 为代表的查找方法操作，会返回查找得到结果的一份拷贝。因此，我们应当避免在热点代码中使用返回 Method 数组的 getMethods 或者 getDeclaredMethods 方法，以减少不必要的堆空间消耗。 demo2123456789101112131415161718192021public class Test2 &#123; public static void target(int i) &#123; // 空方法 &#125; public static void main(String[] args) throws Exception &#123; Class&lt;?&gt; klass = Class.forName(\"reflect.Test2\"); Method method = klass.getMethod(\"target\", int.class); long current = System.currentTimeMillis(); for (int i = 1; i &lt;= 2_000_000_000; i++) &#123; if (i % 100_000_000 == 0) &#123; long temp = System.currentTimeMillis(); System.out.println(temp - current); current = temp; &#125; method.invoke(null, 128); &#125; &#125;&#125; 1234567891059: aload_2 // 加载 Method 对象 60: aconst_null // 反射调用的第一个参数 null 61: iconst_1 62: anewarray Object // 生成一个长度为 1 的 Object 数组 65: dup 66: iconst_0 67: sipush 128 70: invokestatic Integer.valueOf // 将 128 自动装箱成 Integer 73: aastore // 存入 Object 数组中 74: invokevirtual Method.invoke // 反射调用 由于 Method.invoke 是一个变长参数方法，在字节码层面它的最后一个参数会是 Object 数组（感兴趣的同学私下可以用 javap 查看）。Java 编译器会在方法调用处生成一个长度为传入参数数量的 Object 数组，并将传入参数一一存储进该数组中。 由于 Object 数组不能存储基本类型，Java 编译器会对传入的基本类型参数进行自动装箱。 参考 https://docs.oracle.com/javase/tutorial/reflect/ http://hg.openjdk.java.net/jdk10/jdk10/jdk/file/777356696811/src/java.base/share/classes/jdk/internal/reflect/ReflectionFactory.java#l80 http://hg.openjdk.java.net/jdk10/jdk10/jdk/file/777356696811/src/java.base/share/classes/jdk/internal/reflect/ReflectionFactory.java#l78 https://docs.oracle.com/javase/tutorial/reflect/class/classMembers.html","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"08-09JVM是怎么实现invokedynamic的","slug":"aa_category/se/深入拆解 Java 虚拟机/08-09JVM是怎么实现invokedynamic的","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/08-09JVM是怎么实现invokedynamic的/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/08-09JVM是怎么实现invokedynamic的/","excerpt":"方法句柄的概念 方法句柄的权限取决于Lookup对象的创建位置 方法句柄的类型（MethodType）是由所指向方法的参数类型以及返回类型组成的","text":"方法句柄的概念 方法句柄的权限取决于Lookup对象的创建位置 方法句柄的类型（MethodType）是由所指向方法的参数类型以及返回类型组成的 方法句柄的操作 PolymorphicSignature 方法句柄的实现 -XX:+UnlockDiagnosticVMOptions -XX:+ShowHiddenFrames 这个参数来打印被 Java 虚拟机隐藏了的栈信息 CallSite ConstantCallSite MethodVisitor 总结 和工作相关性比较大，暂时可以不关注 参考 https://en.wikipedia.org/wiki/Duck_typing https://docs.oracle.com/javase/10/docs/api/java/lang/invoke/MethodHandle.html https://en.wikipedia.org/wiki/Currying http://openjdk.java.net/jeps/303","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"06JVM是如何处理异常的","slug":"aa_category/se/深入拆解 Java 虚拟机/06JVM是如何处理异常的","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/06JVM是如何处理异常的/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/06JVM是如何处理异常的/","excerpt":"异常的基本概念","text":"异常的基本概念 Java 虚拟机是如何捕获异常的 当程序触发异常时，Java 虚拟机会从上至下遍历异常表中的所有条目。当触发异常的字节码的索引值在某个异常表条目的监控范围内，Java 虚拟机会判断所抛出的异常和该条目想要捕获的异常是否匹配。如果匹配，Java 虚拟机会将控制流转移至该条目 target 指针指向的字节码。 finally，复制finally代码块的内容，分别放置try-catch 代码块中所有正常以及异常路径的出口处 Java 7 的 Supressed 异常以及语法糖 try-with-resources","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"04-05JVM是如何执行方法调用的","slug":"aa_category/se/深入拆解 Java 虚拟机/04-05JVM是如何执行方法调用的","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/04-05JVM是如何执行方法调用的/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/04-05JVM是如何执行方法调用的/","excerpt":"重载选取过程 不考虑自动拆装箱，不考虑变长参数的情况，选择重载方法 如果在第1阶段没有找到重载方法，运行自动拆装箱，不考虑变长参数的情况，选择重载方法 如果在第2个阶段没有找到重载方法，考虑变长参数","text":"重载选取过程 不考虑自动拆装箱，不考虑变长参数的情况，选择重载方法 如果在第1阶段没有找到重载方法，运行自动拆装箱，不考虑变长参数的情况，选择重载方法 如果在第2个阶段没有找到重载方法，考虑变长参数 选取原则 选择最为贴切的方法，贴切由参数类型的继承关系 私有方法如果子类定义了与父类中非私有方法同名的方法，而且这两个方法的参数类型相同，那么这两个方法之间又是什么关系呢？ 如果是静态方法，子类隐藏父类的方法。如果不是静态子类重写了父类中的方法 JVM静态绑定和动态绑定 静态绑定编译期间可以确定执行的方法 动态绑定执行期间才可以确定执行的方法 重写属于动态绑定 重载属于静态绑定 invokestatic,用于调用静态方法 invokespeccial,调用私有方法、构造器、super关键字调用的父类实例或构造方法，和接口的默认方法 invokevirtual,调用实例非私有方法 invokeinterface,调用接口 invokedynamic,调用动态方法 invokestatic 、invokespecial ，Java 虚拟机能够直接识别具体的目标方法。 invokevirtual 、invokeinterface，虚拟机需要在执行过程中，根据调用者的动态类型，来确定具体的目标方法。 调用指令的符号引用 javap -v 打印某个类的常量池 非接口符号引用 在C中查找符合名字及描述符的方法 如果没有找到在C的父类中寻找，直至Object 如果没有找到，在C所有直接或间接实现的接口中查找 接口符号引用 在I中查找符合名字及描述符的方法 如果没有找到，在Objet类的公有实例方法中搜索 如果没有找到，在I的超接口中搜索 虚方法调用 invokestatic 、invokespecial 方法表 子类方法表包含父类方法表中的所有方法 同一方法在子类和父类的方法表中的下标相同 访问栈上的调用者，读取调用者的动态类型，读取该类型的方法表，读取方法表中某个索引对应的方法 内联缓存 单态 多态 超多态 参考 https://docs.oracle.com/javase/8/docs/technotes/guides/language/varargs.html https://docs.oracle.com/javase/tutorial/java/generics/bridgeMethods.html https://wiki.openjdk.java.net/display/HotSpot/VirtualCalls https://wiki.openjdk.java.net/display/HotSpot/InterfaceCalls","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"03Java虚拟机是如何加载Java类的","slug":"aa_category/se/深入拆解 Java 虚拟机/03Java虚拟机是如何加载Java类的","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/03Java虚拟机是如何加载Java类的/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/03Java虚拟机是如何加载Java类的/","excerpt":"杂 对calss文件加密，使用classloader对其解密 类的唯一性是有类加载器和类的全名一起确定的 -verbose:class 可以查看加载过程","text":"杂 对calss文件加密，使用classloader对其解密 类的唯一性是有类加载器和类的全名一起确定的 -verbose:class 可以查看加载过程 加载 查找字节流，并根据此创建类的过程 双亲委托加载机制 链接验证 验证加载的类是否满足Java虚拟机约束条件 准备 静态字段准备内存 构造跟类层次相关的数据结构（符号引用） 解析 符号引用解析成实际引用 初始化 如果字段被final修饰，且是基本类型或字符串，会标记为常量，初始化由Java虚拟机完成。除此之外的赋值操作，以及静态代码块中的代码，则会被编译器置于方法 执行clinit方法的时候通过加锁确保只被执行一次 类的初始化触发时机 用户指定的主类 引用静态字段,初始化字段所在的类 引用静态方法,初始化方法所在的类 new 指定的目标类 通过反射调用某个类,初始化这个类 子类的初始化会触发父类的初始化 接口定义了default方法,那么直接实现或者间接实现该接口的类的初始化,会初始化接口的初始化 当初次调用MethodHandle实例是,初始化MethodHandle指向的类 反例说明 import 某类,不会触发该类的初始化","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"02Java的基本类型","slug":"aa_category/se/深入拆解 Java 虚拟机/02Java的基本类型","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/02Java的基本类型/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/02Java的基本类型/","excerpt":"ifeq succeeds if and only if value = 0 ifne succeeds if and only if value ≠ 0 ifle succeeds if and only if value ≤ 0 ifgt succeeds if and only if value &gt; 0 ifge succeeds if and only if value ≥ 0","text":"ifeq succeeds if and only if value = 0 ifne succeeds if and only if value ≠ 0 ifle succeeds if and only if value ≤ 0 ifgt succeeds if and only if value &gt; 0 ifge succeeds if and only if value ≥ 0","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"01Java代码是怎么运行的","slug":"aa_category/se/深入拆解 Java 虚拟机/01Java代码是怎么运行的","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/01Java代码是怎么运行的/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/01Java代码是怎么运行的/","excerpt":"虚拟机从虚拟机视角来看，执行 Java 代码首先需要将它编译而成的 class 文件加载到 Java 虚拟机中。加载后的 Java 类会被存放于方法区（Method Area）中。实际运行时，虚拟机会执行方法区内的代码。 在运行过程中，每当调用进入一个 Java 方法，Java 虚拟机会在当前线程的 Java 方法栈中生成一个栈帧，用以存放局部变量以及字节码的操作数。这个栈帧的大小是提前计算好的，而且 Java 虚拟机不要求栈帧在内存空间里连续分布。 硬件从硬件视角来看，Java 字节码无法直接执行。因此，Java 虚拟机需要将字节码翻译成机器码。","text":"虚拟机从虚拟机视角来看，执行 Java 代码首先需要将它编译而成的 class 文件加载到 Java 虚拟机中。加载后的 Java 类会被存放于方法区（Method Area）中。实际运行时，虚拟机会执行方法区内的代码。 在运行过程中，每当调用进入一个 Java 方法，Java 虚拟机会在当前线程的 Java 方法栈中生成一个栈帧，用以存放局部变量以及字节码的操作数。这个栈帧的大小是提前计算好的，而且 Java 虚拟机不要求栈帧在内存空间里连续分布。 硬件从硬件视角来看，Java 字节码无法直接执行。因此，Java 虚拟机需要将字节码翻译成机器码。 HotSpot在 HotSpot 里面，上述翻译过程有两种形式：第一种是解释执行，即逐条将字节码翻译成机器码并执行；第二种是即时编译（Just-In-Time compilation，JIT），即将一个方法中包含的所有字节码编译成机器码后再执行。 前者的优势在于无需等待编译，而后者的优势在于实际运行速度更快。HotSpot 默认采用混合模式，综合了解释执行和即时编译两者的优点。它会先解释执行字节码，而后将其中反复执行的热点代码，以方法为单位进行即时编译。","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"3为什么你改了我还看不见","slug":"aa_category/db/mysql/mysql实战/3为什么你改了我还看不见","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/3为什么你改了我还看不见/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/3为什么你改了我还看不见/","excerpt":"隔离性与隔离级别 Atomicity、Consistency、Isolation、Durability 脏读（dirty read）、不可重复读(non-repeatable read)、幻读(phantom read) 读未提交（read uncommitted）,一个事务还没有提交，就能被其它事务看到 读提交（read committed），一个事务提交才能被其它事务看到 可重复读（repeatable read），一个事务执行过程中看到的数据和启动时看到的数据一致 串行化（serializable ），对于同一行记录，写会加写锁，读会加读锁，当出现读写锁冲突的时候，必须等待前一个事务结束，才能继续 查看隔离级别 show variables like ‘transaction_isolation’; SELECT @@tx_isolation","text":"隔离性与隔离级别 Atomicity、Consistency、Isolation、Durability 脏读（dirty read）、不可重复读(non-repeatable read)、幻读(phantom read) 读未提交（read uncommitted）,一个事务还没有提交，就能被其它事务看到 读提交（read committed），一个事务提交才能被其它事务看到 可重复读（repeatable read），一个事务执行过程中看到的数据和启动时看到的数据一致 串行化（serializable ），对于同一行记录，写会加写锁，读会加读锁，当出现读写锁冲突的时候，必须等待前一个事务结束，才能继续 查看隔离级别 show variables like ‘transaction_isolation’; SELECT @@tx_isolation 12create table T(c int) engine=InnoDB;insert into T(c) values(1); 事务隔离的实现假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面有类似的记录 不要使用长事务 不要使用长事务 长事务意味系统里会存在很多老的事务视图 事务的启动方式 显式写begin 或 start transaction，提交commit，回滚写rollback set autocommit=0 将自动提交关闭 select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60 SELECT @@tx_isolation select version() from dual; set session transaction isolation level repeatable read question","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"2一条SQL更新语句是如何执行的","slug":"aa_category/db/mysql/mysql实战/2一条SQL更新语句是如何执行的","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/2一条SQL更新语句是如何执行的/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/2一条SQL更新语句是如何执行的/","excerpt":"summarizeredo log Write-Ahead Logging，先写日志，再写磁盘 先写日志，更新内存，当系统比较清闲的时候，写入磁盘 write pos 是当前记录的位置 chekpoint 是当前要擦出的位置 write pos 到chekpoint 之间的位置是可以记录的空间 crash-safe","text":"summarizeredo log Write-Ahead Logging，先写日志，再写磁盘 先写日志，更新内存，当系统比较清闲的时候，写入磁盘 write pos 是当前记录的位置 chekpoint 是当前要擦出的位置 write pos 到chekpoint 之间的位置是可以记录的空间 crash-safe binlog和redo log区别 redo log 是物理日志，binlog是逻辑日志 redo log是循环写的，binglog是追加的 执行过程 两阶段提交","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"zookeeper入门","slug":"aa_category/middleware/zookeeper/zookeeper入门","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/middleware/zookeeper/zookeeper入门/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/middleware/zookeeper/zookeeper入门/","excerpt":"","text":"EventType12345None (-1),NodeCreated (1),NodeDeleted (2),NodeDataChanged (3),NodeChildrenChanged (4); KeeperState1234567891011121314151617// 失去连接Disconnected (0),// 同步连接SyncConnected (3),// 校验失败AuthFailed (4),// 只读连接ConnectedReadOnly (5), SaslAuthenticated(6),// 失效Expired (-112);","categories":[{"name":"middleware","slug":"middleware","permalink":"https://liyong.ac.cn/categories/middleware/"}],"tags":[{"name":"middleware","slug":"middleware","permalink":"https://liyong.ac.cn/tags/middleware/"},{"name":"zk","slug":"zk","permalink":"https://liyong.ac.cn/tags/zk/"}]},{"title":"zk使用场景","slug":"aa_category/middleware/zookeeper/zk使用场景","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/middleware/zookeeper/zk使用场景/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/middleware/zookeeper/zk使用场景/","excerpt":"分布式协调 分布式锁 配置信息管理 HA高可用","text":"分布式协调 分布式锁 配置信息管理 HA高可用 分布式协调","categories":[{"name":"middleware","slug":"middleware","permalink":"https://liyong.ac.cn/categories/middleware/"}],"tags":[{"name":"middleware","slug":"middleware","permalink":"https://liyong.ac.cn/tags/middleware/"},{"name":"zk","slug":"zk","permalink":"https://liyong.ac.cn/tags/zk/"}]},{"title":"20幻读是什么，幻读有什么问题","slug":"aa_category/db/mysql/mysql实战/20幻读是什么，幻读有什么问题","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/20幻读是什么，幻读有什么问题/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/20幻读是什么，幻读有什么问题/","excerpt":"12345678910CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);","text":"12345678910CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25); 幻读是什么 幻读是指新插入的行 幻读有什么问题 1234567891011121314151617181920212223242526272829303132333435START TRANSACTION ;SELECT * from t where d=5 for update;//会把扫描中遇到的行都加锁update t set d =100 where d=5;SELECT * from t where d=5 for update;SELECT * from t where d=5 for update;ROLLBACK;COMMIT;start TRANSACTION; update t set d=5 where id=0; update t set c=5 where id=0; ROLLBACK;COMMIT;start TRANSACTION; insert into t values (1,1,5)update t set c=5 where id=1; ROLLBACK;COMMIT; 如何解决幻读 跟行锁有冲突关系的是“另外一个行锁 跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"1一条SQL查询语句是如何执行的","slug":"aa_category/db/mysql/mysql实战/1一条SQL查询语句是如何执行的","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/1一条SQL查询语句是如何执行的/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/1一条SQL查询语句是如何执行的/","excerpt":"summarize server层包括连接器、查询缓存、分析器、优化器、执行器以及所有的内置函数（如日期、时间、数学函数）","text":"summarize server层包括连接器、查询缓存、分析器、优化器、执行器以及所有的内置函数（如日期、时间、数学函数） 连接器 修改用户权限，不影响已经建立的连接 show processlist 查看连接的客户端 wait_timeout 默认值 8 建立连接比较麻烦，建议使用长连接 查询缓存 query_cache_type 设置成DEMAND select SQL_CACHE * from T where ID=10 使用SQL_CACHE 显示的使用缓存 分析器 词法分析 语法分析 优化器 决定索引 决定各个表的连接顺序 执行器1select * from T where ID=10; 取这个表的第一行判断ID值是否等于10，如果不是则跳过，如果是则将结果保存到结果中 取下一行，重复相同的逻辑判断，直至最后一行 将满足条件的记录返回给客户端","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"13为什么表数据删掉一半,表文件大小不变","slug":"aa_category/db/mysql/mysql实战/13为什么表数据删掉一半,表文件大小不变","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/13为什么表数据删掉一半,表文件大小不变/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/13为什么表数据删掉一半,表文件大小不变/","excerpt":"innodb_file_per_table 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起； 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。","text":"innodb_file_per_table 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起； 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。 数据删除流程 不止是删除数据会造成空洞，插入数据也会 另外，更新索引上的值，可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造成空洞的。","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"12为什么我的MySQL会“抖”一下","slug":"aa_category/db/mysql/mysql实战/12为什么我的MySQL会“抖”一下","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/12为什么我的MySQL会“抖”一下/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/12为什么我的MySQL会“抖”一下/","excerpt":"当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”","text":"当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页” 那么，什么情况会引发数据库的 flush 过程呢？ 粉板（redo log）满了，记不下了。 对应的就是系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。 MySQL 认为系统“空闲”的时候 MySQL 正常关闭的情况 内存不够用了内存不够用了要先将脏页写到磁盘”，这种情况其实是常态。InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态： 还没有使用的； 使用了并且是干净页； 使用了并且是脏页。 刷脏页虽然是常态，但是出现以情况，都是会明显影响性能的：一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长； InnoDB 刷脏页的控制策略这就要用到 innodb_io_capacity 这个参数了，它会告诉 InnoDB 你的磁盘能力。这个值我建议你设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令： 1fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest 一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。 在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"11怎么给字符串字段加索引","slug":"aa_category/db/mysql/mysql实战/11怎么给字符串字段加索引","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/11怎么给字符串字段加索引/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/11怎么给字符串字段加索引/","excerpt":"12345678 create table SUser(ID bigint unsigned primary key,email varchar(64) ) ;alter table SUser add index index1(email);alter table SUser add index index2(email(6));","text":"12345678 create table SUser(ID bigint unsigned primary key,email varchar(64) ) ;alter table SUser add index index1(email);alter table SUser add index index2(email(6)); 1select id,name,email from SUser where email='zhangssxyz@xxx.com' 使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本 select count(distinct email) as L from SUser; 使用前缀索引就用不上覆盖索引对查询性能的优化了","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"10MySQL为什么有时候会选错索引","slug":"aa_category/db/mysql/mysql实战/10MySQL为什么有时候会选错索引","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/10MySQL为什么有时候会选错索引/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/10MySQL为什么有时候会选错索引/","excerpt":"","text":"12345set long_query_time=0;select * from t where a between 10000 and 20000; /*Q1*/select * from t force index(a) where a between 10000 and 20000;/*Q2*/show index from t; 对于由于索引统计信息不准确导致的问题用 analyze table 来解决","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"21为什么我只改一行的语句，锁这么多","slug":"aa_category/db/mysql/mysql实战/21为什么我只改一行的语句，锁这么多","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/21为什么我只改一行的语句，锁这么多/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/21为什么我只改一行的语句，锁这么多/","excerpt":"两个“原则”、两个“优化”和一个“bug 原则 1：加锁的基本单位是 next-key lock。 原则 2：查找过程中访问到的对象才会加锁。 优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止","text":"两个“原则”、两个“优化”和一个“bug 原则 1：加锁的基本单位是 next-key lock。 原则 2：查找过程中访问到的对象才会加锁。 优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止 12345678910CREATE TABLE `t21` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`)) ENGINE=InnoDB;insert into t21 values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25); 案例一：等值查询间隙锁 session A session B session C start TRANSACTION;update t21 set d=d+1 where id=7; start TRANSACTION;insert into t21 values(8,8,8) start TRANSACTION;update t21 set d=d+1 where id=10; 由于表 t 中没有 id=7 的记录，所以用我们上面提到的加锁规则判断一下的话： 根据原则 1，加锁单位是 next-key lock，session A 加锁范围就是 (5,10]； 同时根据优化 2，这是一个等值查询 (id=7)，而 id=10 不满足查询条件，next-key lock 退化成间隙锁，因此最终加锁的范围是 (5,10)。 所以，session B 要往这个间隙里面插入 id=8 的记录会被锁住，但是 session C 修改 id=10 这行是可以的。 案例二：非唯一索引等值锁 session A session B session C start TRANSACTION;SELECT id from t21 where c=5 lock in SHARE mode; start TRANSACTION;update t21 set d=d+1 where id=5; start TRANSACTION;insert into t21 values(8,8,8) 这里 session A 要给索引 c 上 c=5 的这一行加上读锁。 根据原则 1，加锁单位是 next-key lock，因此会给 (0,5] 加上 next-key lock。 要注意 c 是普通索引，因此仅访问 c=5 这一条记录是不能马上停下来的，需要向右遍历，查到 c=10 才放弃。根据原则 2，访问到的都要加锁，因此要给 (5,10] 加 next-key lock。 但是同时这个符合优化 2：等值判断，向右遍历，最后一个值不满足 c=5 这个等值条件，因此退化成间隙锁 (5,10)。 根据原则 2 ，只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么 session B 的 update 语句可以执行完成。 案例三：主键索引范围锁12select * from t where id=10 for update;select * from t where id&gt;=10 and id&lt;11 for update; 在逻辑上，这两条查语句肯定是等价的，但是它们的加锁规则不太一样 session A session B session C start TRANSACTION;SELECT * from t21 where id&gt;=10 and id&lt;11 for update; start TRANSACTION; insert into t21 values(8,8,8);insert into t21 values(13,13,13); start TRANSACTION;update t21 set d=d+1 where id=15 开始执行的时候，要找到第一个 id=10 的行，因此本该是 next-key lock(5,10]。 根据优化 1， 主键 id 上的等值条件，退化成行锁，只加了 id=10 这一行的行锁。 范围查找就往后继续找，找到 id=15 这一行停下来，因此需要加 next-key lock(10,15]。 所以，session A 这时候锁的范围就是主键索引上，行锁 id=10 和 next-key lock(10,15]。这样，session B 和 session C 的结果你就能理解了。 这里你需要注意一点，首次 session A 定位查找 id=10 的行的时候，是当做等值查询来判断的，而向右扫描到 id=15 的时候，用的是范围查询判断。 案例四：非唯一索引范围锁 session A session B session C start TRANSACTION;SELECT * from t21 where c&gt;=10 and id&lt;11 for update; start TRANSACTION; insert into t21 values(8,8,8);insert into t21 values(13,13,13); start TRANSACTION;update t21 set d=d+1 where c=15 这次 session A 用字段 c 来判断，加锁规则跟案例三唯一的不同是：在第一次用 c=10 定位记录的时候，索引 c 上加了 (5,10] 这个 next-key lock 后，由于索引 c 是非唯一索引，没有优化规则，也就是说不会蜕变为行锁，因此最终 sesion A 加的锁是，索引 c 上的 (5,10] 和 (10,15] 这两个 next-key lock。 所以从结果上来看，sesson B 要插入（8,8,8) 的这个 insert 语句时就被堵住了。 这里需要扫描到 c=15 才停止扫描，是合理的，因为 InnoDB 要扫到 c=15，才知道不需要继续往后找了。 案例五：唯一索引范围锁 bug session A session B session C start TRANSACTION;SELECT * from t21 where id&gt;10 and id&lt;=15 for update; start TRANSACTION;SELECT * from t21 where id&gt;10 and id&lt;=15 for update; start TRANSACTION;SELECT * from t21 where id&gt;10 and id&lt;=15 for update; session A 是一个范围查询，按照原则 1 的话，应该是索引 id 上只加 (10,15] 这个 next-key lock，并且因为 id 是唯一键，所以循环判断到 id=15 这一行就应该停止了。 但是实现上，InnoDB 会往前扫描到第一个不满足条件的行为止，也就是 id=20。而且由于这是个范围扫描，因此索引 id 上的 (15,20] 这个 next-key lock 也会被锁上。 所以你看到了，session B 要更新 id=20 这一行，是会被锁住的。同样地，session C 要插入 id=16 的一行，也会被锁住。 照理说，这里锁住 id=20 这一行的行为，其实是没有必要的。因为扫描到 id=15，就可以确定不用往后再找了。但实现上还是这么做了，因此我认为这是个 bug。 案例六：非唯一索引上存在”等值”的例子1insert into t values(30,10,30); 可以看到，虽然有两个 c=10，但是它们的主键值 id 是不同的（分别是 10 和 30），因此这两个 c=10 的记录之间，也是有间隙的。 图中我画出了索引 c 上的主键 id。为了跟间隙锁的开区间形式进行区别，我用 (c=10,id=30) 这样的形式，来表示索引上的一行。 现在，我们来看一下案例六。 这次我们用 delete 语句来验证。注意，delete 语句加锁的逻辑，其实跟 select … for update 是类似的，也就是我在文章开始总结的两个“原则”、两个“优化”和一个“bug” session A session B session C start TRANSACTION;DELETE from t21 where c=10; start TRANSACTION; insert into t21 values(12,12,12); start TRANSACTION;update t21 set d=d+1 where c=15 session A 是一个范围查询 这时，session A 在遍历的时候，先访问第一个 c=10 的记录。同样地，根据原则 1，这里加的是 (c=5,id=5) 到 (c=10,id=10) 这个 next-key lock。 然后，session A 向右查找，直到碰到 (c=15,id=15) 这一行，循环才结束。根据优化 2，这是一个等值查询，向右查找到了不满足条件的行，所以会退化成 (c=10,id=10) 到 (c=15,id=15) 的间隙锁。 案例七：limit 语句加锁我在文章开始总结的两个“原则”、两个“优化”和一个“bug” session A session B start TRANSACTION;DELETE from t21 where c=10 LIMIT 2; start TRANSACTION; insert into t21 values(12,12,12); session A 的 delete 语句加了 limit 2。你知道表 t 里 c=10 的记录其实只有两条，因此加不加 limit 2，删除的效果都是一样的，但是加锁的效果却不同。可以看到，session B 的 insert 语句执行通过了，跟案例六的结果不同。 这是因为，案例七里的 delete 语句明确加了 limit 2 的限制，因此在遍历到 (c=10, id=30) 这一行之后，满足条件的语句已经有两条，循环就结束了。 因此，索引 c 上的加锁范围就变成了从（c=5,id=5) 到（c=10,id=30) 这个前开后闭区间 案例八：一个死锁的例子我在文章开始总结的两个“原则”、两个“优化”和一个“bug” session A session B start TRANSACTION;SELECT id from t21 where c=10 lock in share MODE; start TRANSACTION;update t21 set d=d+1 where c=10 insert into t21 values(8,8,8); [Err] 1213 - Deadlock found when trying to get lock; try restarting transaction 现在，我们按时间顺序来分析一下为什么是这样的结果。 session A 启动事务后执行查询语句加 lock in share mode，在索引 c 上加了 next-key lock(5,10] 和间隙锁 (10,15)； session B 的 update 语句也要在索引 c 上加 next-key lock(5,10] ，进入锁等待； 然后 session A 要再插入 (8,8,8) 这一行，被 session B 的间隙锁锁住。由于出现了死锁，InnoDB 让 session B 回滚。 你可能会问，session B 的 next-key lock 不是还没申请成功吗？ 其实是这样的，session B 的“加 next-key lock(5,10] ”操作，实际上分成了两步，先是加 (5,10) 的间隙锁，加锁成功；然后加 c=10 的行锁，这时候才被锁住的。 也就是说，我们在分析加锁规则的时候可以用 next-key lock 来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的。 小结这里我再次说明一下，我们上面的所有案例都是在可重复读隔离级别 (repeatable-read) 下验证的。同时，可重复读隔离级别遵守两阶段锁协议，所有加锁的资源，都是在事务提交或者回滚的时候才释放的 在最后的案例中，你可以清楚地知道 next-key lock 实际上是由间隙锁加行锁实现的。如果切换到读提交隔离级别 (read-committed) 的话，就好理解了，过程中去掉间隙锁的部分，也就是只剩下行锁的部分","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"8事务到底是隔离的还是不隔离的","slug":"aa_category/db/mysql/mysql实战/8事务到底是隔离的还是不隔离的","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/8事务到底是隔离的还是不隔离的/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/8事务到底是隔离的还是不隔离的/","excerpt":"begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句（第一个快照读语句），事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。","text":"begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句（第一个快照读语句），事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。 快照”在 MVCC 里是怎么工作的 InnoDB为每个事物构造了一个数组，用来保存启动瞬间，当前正在活跃的事务ID,活跃是指启动但是未提交 当前系统里面已经创建的事务ID最大值加1记为高水位 对于当前事务启动瞬间来说，一个数据版本row trx_id有集中可能 如果小于低水位，落在绿色区间，可见 如果大于高水位，落在红色区间 ，不可见 如果落在黄色区间不可见 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。 当前读 更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读“ 这里我们提到了一个概念，叫作当前读。其实，除了 update 语句外，select 语句如果加锁，也是当前读。 update 属于排它锁，一定要等待其他线程写完 可重复读&amp;读提交 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"7行锁功过：怎么减少行锁对性能的影响","slug":"aa_category/db/mysql/mysql实战/7行锁功过：怎么减少行锁对性能的影响","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/7行锁功过：怎么减少行锁对性能的影响/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/7行锁功过：怎么减少行锁对性能的影响/","excerpt":"在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。","text":"在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。 死锁和死锁检测 解决死锁的策略 直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"6全局锁和表锁 ：给表加个字段怎么有这么多阻碍","slug":"aa_category/db/mysql/mysql实战/6全局锁和表锁 ：给表加个字段怎么有这么多阻碍","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/6全局锁和表锁 ：给表加个字段怎么有这么多阻碍/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/6全局锁和表锁 ：给表加个字段怎么有这么多阻碍/","excerpt":"全局锁 加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份 single-transaction 方法只适用于所有的表使用事务引擎的库","text":"全局锁 加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份 single-transaction 方法只适用于所有的表使用事务引擎的库 表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL) 表锁的语法是 lock tables … read/write 另一类表级的锁是 MDL（metadata lock) MDL 不需要显式使用，在访问一个表的时候会被自动加上 MDL 的作用是，保证读写的正确性 当对一个表做增删改查操作的时候，加 MDL 读锁 当要对表做结构变更操作的时候，加 MDL 写锁。 上期问题12345678910CREATE TABLE `geek` ( `a` int(11) NOT NULL, `b` int(11) NOT NULL, `c` int(11) NOT NULL, `d` int(11) NOT NULL, PRIMARY KEY (`a`,`b`), KEY `c` (`c`), KEY `ca` (`c`,`a`), KEY `cb` (`c`,`b`)) ENGINE=InnoDB;","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"5深入浅出索引","slug":"aa_category/db/mysql/mysql实战/5深入浅出索引","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/5深入浅出索引/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/5深入浅出索引/","excerpt":"12345678create table T (ID int primary key,k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '',index k(k))engine=InnoDB;insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');","text":"12345678create table T (ID int primary key,k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '',index k(k))engine=InnoDB;insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg'); 如果我执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？ 在 k 索引树上找到 k=3 的记录，取得 ID = 300； 再到 ID 索引树查到 ID=300 对应的 R3； 在 k 索引树取下一个值 k=5，取得 ID=500； 再回到 ID 索引树查到 ID=500 对应的 R4； 在 k 索引树取下一个值 k=6，不满足条件，循环结束。 覆盖索引如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。 最左前缀原则12345678910CREATE TABLE `tuser` ( `id` int(11) NOT NULL, `id_card` varchar(32) DEFAULT NULL, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `ismale` tinyint(1) DEFAULT NULL, PRIMARY KEY (`id`), KEY `id_card` (`id_card`), KEY `name_age` (`name`,`age`)) ENGINE=InnoDB B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。 只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 索引下推索引没有下推 索引有下推","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"【工具篇】常用工具","slug":"aa_category/se/深入拆解 Java 虚拟机/【工具篇】常用工具","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/【工具篇】常用工具/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/【工具篇】常用工具/","excerpt":"javaphttps://docs.oracle.com/javase/8/docs/technotes/tools/unix/javap.html -v 选项 基本信息，涵盖了class文件的相关信息。 常量池，存放各种产量以及符号引用 字段区域，用来列举该类中的各个字段。 方法区域","text":"javaphttps://docs.oracle.com/javase/8/docs/technotes/tools/unix/javap.html -v 选项 基本信息，涵盖了class文件的相关信息。 常量池，存放各种产量以及符号引用 字段区域，用来列举该类中的各个字段。 方法区域 方法区域 异常表 LineNumberTable，源码到字节码偏移量的映射 LocalVariableTable（javac -g）每个局部变量的名字类型，以及作用域 字节码操作数映射表（StackMapTable） -g 编译 LocalVariableTable -parameters 编译 MethodParameters OpenJDK 项目 Code Tools：实用小工具集参考 https://docs.oracle.com/javase/specs/jvms/se10/html/jvms-4.html#jvms-4.1 http://openjdk.java.net/projects/code-tools/ https://wiki.openjdk.java.net/display/CodeTools/asmtools https://cs.au.dk/~mis/dOvs/jvmspec/ref--21.html http://openjdk.java.net/projects/code-tools/jol/ https://asm.ow2.io/ http://web.cs.ucla.edu/~msb/cs239-tutorial/","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"jvm资源","slug":"aa_category/se/深入拆解 Java 虚拟机/jvm资源","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/jvm资源/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/jvm资源/","excerpt":"虚拟机牛人 RednaxelaFX https://www.zhihu.com/people/rednaxelafx/activities 你假苯 http://xxfox.perfma.com/ 江南白衣 占小狼 杨晓峰 https://medium.com/graalvm http://cliffc.org/blog/ https://shipilev.net/jvm/anatomy-quarks/ https://shipilev.net/ http://psy-lob-saw.blogspot.com/ http://openjdk.java.net/projects/mlvm/jvmlangsummit/ https://www.oracle.com/code-one/ https://github.com/deephacks/awesome-jvm","text":"虚拟机牛人 RednaxelaFX https://www.zhihu.com/people/rednaxelafx/activities 你假苯 http://xxfox.perfma.com/ 江南白衣 占小狼 杨晓峰 https://medium.com/graalvm http://cliffc.org/blog/ https://shipilev.net/jvm/anatomy-quarks/ https://shipilev.net/ http://psy-lob-saw.blogspot.com/ http://openjdk.java.net/projects/mlvm/jvmlangsummit/ https://www.oracle.com/code-one/ https://github.com/deephacks/awesome-jvm 参考 https://docs.oracle.com/javase/specs/jvms/se10/html/jvms-4.html#jvms-4.1 http://openjdk.java.net/projects/code-tools/ https://wiki.openjdk.java.net/display/CodeTools/asmtools https://cs.au.dk/~mis/dOvs/jvmspec/ref--21.html http://openjdk.java.net/projects/code-tools/jol/ https://asm.ow2.io/ http://web.cs.ucla.edu/~msb/cs239-tutorial/ http://openjdk.java.net/groups/compiler/doc/compilation-overview/index.html http://hannesdorfmann.com/annotation-processing/annotationprocessing101 https://docs.oracle.com/javase/10/docs/api/javax/lang/model/element/package-summary.html https://projectlombok.org/ http://notatube.blogspot.com/2010/11/project-lombok-trick-explained.html http://jnb.ociweb.com/jnb/jnbJan2010.html#controversy https://docs.oracle.com/javase/specs/jvms/se10/html/jvms-6.html#jvms-6.5 https://en.wikipedia.org/wiki/Static_single_assignment_form https://en.wikipedia.org/wiki/Reaching_definition https://wiki.openjdk.java.net/display/HotSpot/Synchronization https://docs.oracle.com/javase/specs/jls/se10/html/jls-17.html#jls-17.4 http://gee.cs.oswego.edu/dl/jmm/cookbook.html https://blogs.oracle.com/dave/instruction-selection-for-volatile-fences-:-mfence-vs-lock:add https://vlkan.com/blog/post/2014/02/14/java-safe-publication/ https://wiki.openjdk.java.net/display/CodeTools/jcstress http://hg.openjdk.java.net/code-tools/jcstress/file/64f2cf32fa0a/tests-custom/src/main/java/org/openjdk/jcstress/tests/unsafe/UnsafePublication.java http://psy-lob-saw.blogspot.com/2015/12/safepoints.html http://openjdk.java.net/jeps/122 http://psy-lob-saw.blogspot.com/2014/10/the-jvm-write-barrier-card-marking.html https://blogs.oracle.com/dave/false-sharing-induced-by-card-table-marking http://openjdk.java.net/jeps/291 https://wiki.openjdk.java.net/display/HotSpot/CompressedOops http://openjdk.java.net/jeps/142 https://en.wikipedia.org/wiki/Duck_typing https://docs.oracle.com/javase/10/docs/api/java/lang/invoke/MethodHandle.html https://en.wikipedia.org/wiki/Currying http://openjdk.java.net/jeps/303 https://docs.oracle.com/javase/tutorial/reflect/ http://hg.openjdk.java.net/jdk10/jdk10/jdk/file/777356696811/src/java.base/share/classes/jdk/internal/reflect/ReflectionFactory.java#l80 http://hg.openjdk.java.net/jdk10/jdk10/jdk/file/777356696811/src/java.base/share/classes/jdk/internal/reflect/ReflectionFactory.java#l78 https://docs.oracle.com/javase/tutorial/reflect/class/classMembers.html https://docs.oracle.com/javase/8/docs/technotes/guides/language/varargs.html https://docs.oracle.com/javase/tutorial/java/generics/bridgeMethods.html https://wiki.openjdk.java.net/display/HotSpot/VirtualCalls https://wiki.openjdk.java.net/display/HotSpot/InterfaceCalls https://docs.oracle.com/javase/specs/jvms/se10/html/jvms-4.html#jvms-4.1 http://openjdk.java.net/projects/code-tools/ https://wiki.openjdk.java.net/display/CodeTools/asmtools https://cs.au.dk/~mis/dOvs/jvmspec/ref--21.html http://openjdk.java.net/projects/code-tools/jol/ https://asm.ow2.io/ http://web.cs.ucla.edu/~msb/cs239-tutorial/","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"35SubstrateVM：AOT编译框架","slug":"aa_category/se/深入拆解 Java 虚拟机/35SubstrateVM：AOT编译框架","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/35SubstrateVM：AOT编译框架/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/35SubstrateVM：AOT编译框架/","excerpt":"Substrate VM is a framework that allows ahead-of-time (AOT) compilation of Java applications under closed-world assumption into executable images or shared objects (ELF-64 or 64-bit Mach-O). 所谓 AOT 编译，是与即时编译相对立的一个概念。我们知道，即时编译指的是在程序的运行过程中，将字节码转换为可在硬件上直接运行的机器码，并部署至托管环境中的过程。","text":"Substrate VM is a framework that allows ahead-of-time (AOT) compilation of Java applications under closed-world assumption into executable images or shared objects (ELF-64 or 64-bit Mach-O). 所谓 AOT 编译，是与即时编译相对立的一个概念。我们知道，即时编译指的是在程序的运行过程中，将字节码转换为可在硬件上直接运行的机器码，并部署至托管环境中的过程。","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"28-29基准测试框架JMH","slug":"aa_category/se/深入拆解 Java 虚拟机/28-29基准测试框架JMH","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/28-29基准测试框架JMH/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/28-29基准测试框架JMH/","excerpt":"","text":"1mvn archetype:generate -DinteractiveMode=false -DarchetypeGroupId=org.openjdk.jmh -DarchetypeArtifactId=jmh-java-benchmark-archetype -DgroupId=com.jenkov -DartifactId=first-benchmark -Dversion=1.0","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"30Java虚拟机的监控及诊断工具","slug":"aa_category/se/深入拆解 Java 虚拟机/30-31Java虚拟机的监控及诊断工具","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/30-31Java虚拟机的监控及诊断工具/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/30-31Java虚拟机的监控及诊断工具/","excerpt":"jpshttps://docs.oracle.com/javase/8/docs/technotes/tools/unix/jps.html#CHDCGECD","text":"jpshttps://docs.oracle.com/javase/8/docs/technotes/tools/unix/jps.html#CHDCGECD 123$ jps -mlv18331 org.example.Foo Hello World18332 jdk.jcmd/sun.tools.jps.Jps -mlv -Dapplication.home=/Library/Java/JavaVirtualMachines/jdk-11.jdk/Contents/Home -Xms8m -Djdk.module.main=jdk.jcmd jstat12345678910111213 $ jstat -options-class-compiler-gc-gccapacity-gccause-gcmetacapacity-gcnew-gcnewcapacity-gcold-gcoldcapacity-gcutil-printcompilation 使用 G1 GC 时，Java 虚拟机不再设置 Eden 区、Survivor 区，老年代区的内存边界，而是将堆划分为若干个等长内存区域。 每个内存区域都可以作为 Eden 区、Survivor 区以及老年代区中的任一种，并且可以在不同区域类型之间来回切换。(https://www.oracle.com/technetwork/tutorials/tutorials-1876574.html) jmapjinfo1jinfo -flag +HeapDumpAfterFullGC &lt;pid&gt; jstackjcmd需要重点关注 eclipse MATJava Mission ControlJITWatch","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"27注解处理器","slug":"aa_category/se/深入拆解 Java 虚拟机/27注解处理器","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/27注解处理器/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/27注解处理器/","excerpt":"注解处理器的原理Java 编译器的工作流程 将源文件解析为抽象语法树； 调用已注册的注解处理器； 生成字节码。","text":"注解处理器的原理Java 编译器的工作流程 将源文件解析为抽象语法树； 调用已注册的注解处理器； 生成字节码。 Processor 所有的注解都要实现该接口 @SupportedAnnotationTypes @SupportedSourceVersion 1234567891011public interface Processor &#123; void init(ProcessingEnvironment processingEnv); Set&lt;String&gt; getSupportedAnnotationTypes(); SourceVersion getSupportedSourceVersion(); boolean process(Set&lt;? extends TypeElement&gt; annotations, RoundEnvironment roundEnv);&#125; 123$ javac -cp /CLASSPATH/TO/CheckGetterProcessor -processor bar.CheckGetterProcessor Foo.javaerror: Class 'Foo' is annotated as @CheckGetter, but field 'a' is without getter1 error Element引用 http://openjdk.java.net/groups/compiler/doc/compilation-overview/index.html http://hannesdorfmann.com/annotation-processing/annotationprocessing101 https://docs.oracle.com/javase/10/docs/api/javax/lang/model/element/package-summary.html https://projectlombok.org/ http://notatube.blogspot.com/2010/11/project-lombok-trick-explained.html http://jnb.ociweb.com/jnb/jnbJan2010.html#controversy","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"22HotSpot虚拟机的intrinsic","slug":"aa_category/se/深入拆解 Java 虚拟机/22HotSpot虚拟机的intrinsic","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/22HotSpot虚拟机的intrinsic/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/22HotSpot虚拟机的intrinsic/","excerpt":"换句话说，HotSpot 虚拟机将为标注了@HotSpotIntrinsicCandidate注解的方法额外维护一套高效实现。如果 Java 核心类库的开发者更改了原本的实现，那么虚拟机中的高效实现也需要进行相应的修改，以保证程序语义一致。 引用 http://openjdk.java.net/jeps/254 http://hg.openjdk.java.net/jdk/hs/file/46dc568d6804/src/hotspot/share/classfile/vmSymbols.hpp#l727 http://hg.openjdk.java.net/jdk8u/jdk8u/hotspot/file/2af8917ffbee/src/share/vm/classfile/vmSymbols.hpp#l647","text":"换句话说，HotSpot 虚拟机将为标注了@HotSpotIntrinsicCandidate注解的方法额外维护一套高效实现。如果 Java 核心类库的开发者更改了原本的实现，那么虚拟机中的高效实现也需要进行相应的修改，以保证程序语义一致。 引用 http://openjdk.java.net/jeps/254 http://hg.openjdk.java.net/jdk/hs/file/46dc568d6804/src/hotspot/share/classfile/vmSymbols.hpp#l727 http://hg.openjdk.java.net/jdk8u/jdk8u/hotspot/file/2af8917ffbee/src/share/vm/classfile/vmSymbols.hpp#l647","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"9普通索引和唯一索引，应该怎么选择","slug":"aa_category/db/mysql/mysql实战/9普通索引和唯一索引，应该怎么选择","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql实战/9普通索引和唯一索引，应该怎么选择/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql实战/9普通索引和唯一索引，应该怎么选择/","excerpt":"查询过程 InnoDB 的数据是按数据页为单位来读写的 每个数据页的大小默认是 16KB 更新过程 如果数据页在内存中就直接更新 如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作 唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。","text":"查询过程 InnoDB 的数据是按数据页为单位来读写的 每个数据页的大小默认是 16KB 更新过程 如果数据页在内存中就直接更新 如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作 唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。 插入一个新记录 (4,400)更新的目标页在内存中 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。 更新的目标页不在内存中 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。 将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。 change buffer 的使用场景 对于写多读少.例如，账单类、日志类的系统。 change buffer 和 redo log1insert into t(id,k) values(id1,k1),(id2,k2); k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中 Page 1 在内存中，直接更新内存； Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息 将上述两个动作记入 redo log 中（图中 3 和 4）。 图中的两个虚线箭头，是后台操作，不影响更新的响应时间。 那在这之后的读请求，要怎么处理呢？ 要执行 select * from t where k in (k1, k2) 如果读语句发生在更新语句后不久，内存中的数据都还在 读 Page 1 的时候，直接从内存返回 读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。 redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"mysql-explain","slug":"aa_category/db/mysql/mysql-explain","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/mysql-explain/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/mysql-explain/","excerpt":"select_typeSIMPLE 类型 说明 SIMPLE 简单表，不使用表连接或子查询 PRIMARY 主查询，即外层的查询 UNION SUBQUERY","text":"select_typeSIMPLE 类型 说明 SIMPLE 简单表，不使用表连接或子查询 PRIMARY 主查询，即外层的查询 UNION SUBQUERY tabletype ALL 全表扫描 index 索引全扫描 range 索引范围扫描 ref 非唯一索引扫描 eq_ref 唯一索引扫描 const,system 单表最多有一个匹配行 NULL 不用扫描表或索引","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"常用sql","slug":"aa_category/db/mysql/常用sql","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/常用sql/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/常用sql/","excerpt":"查看搜索引擎1show ENGINES","text":"查看搜索引擎1show ENGINES 查看搜索引擎的状态1show engine INNODB STATUS 查看搜索引擎版本 查看搜索引擎版本1show VARIABLES like 'innodb_version' 123456//show master status// 设置binlog格式SET GLOBAL binlog_format = 'STATEMENT';// 查看警告信息show warnings 查看表的信息12345678910select t.COLUMN_NAME, t.COLUMN_type, t.IS_NULLABLE, t.COLUMN_DEFAULT, t.COLUMN_key, t.EXTRA, t.COLUMN_COMMENTfrom information_schema.columns twhere table_schema = 'table_schema' and TABLE_NAME = 'TABLE_NAME'","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"20-21方法内联","slug":"aa_category/se/深入拆解 Java 虚拟机/20-21方法内联","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/20-21方法内联/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/20-21方法内联/","excerpt":"在编译过程中遇到方法调用时，将目标方法的方法体纳入编译范围之中，并取代原方法调用的优化手段。 以 getter/setter 为例，如果没有方法内联，在调用 getter/setter 时，程序需要保存当前方法的执行位置，创建并压入用于 getter/setter 的栈帧、访问字段、弹出栈帧，最后再恢复当前方法的执行。而当内联了对 getter/setter 的方法调用后，上述操作仅剩字段访问。","text":"在编译过程中遇到方法调用时，将目标方法的方法体纳入编译范围之中，并取代原方法调用的优化手段。 以 getter/setter 为例，如果没有方法内联，在调用 getter/setter 时，程序需要保存当前方法的执行位置，创建并压入用于 getter/setter 的栈帧、访问字段、弹出栈帧，最后再恢复当前方法的执行。而当内联了对 getter/setter 的方法调用后，上述操作仅剩字段访问。","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"18即时编译器的中间表达形式","slug":"aa_category/se/深入拆解 Java 虚拟机/18即时编译器的中间表达形式","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/18即时编译器的中间表达形式/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/18即时编译器的中间表达形式/","excerpt":"中间表达形式（IR）如果不考虑解释执行的话，从 Java 源代码到最终的机器码实际上经过了两轮编译：Java 编译器将 Java 源代码编译成 Java 字节码，而即时编译器则将 Java 字节码编译成机器码。 Java 字节码本身并不适合直接作为可供优化的 IR。这是因为现代编译器一般采用静态单赋值（Static Single Assignment，SSA）IR。这种 IR 的特点是每个变量只能被赋值一次，而且只有当变量被赋值之后才能使用。 12345示例：x1=4*1024 经过常量折叠后变为 x1=4096x1=4; y1=x1 经过常量传播后变为 x1=4; y1=4y1=x1*3 经过强度削减后变为 y1=(x1&lt;&lt;1)+x1if(2&gt;1)&#123;y1=1;&#125;else&#123;y2=1;&#125;经过死代码删除后变为 y1=1","text":"中间表达形式（IR）如果不考虑解释执行的话，从 Java 源代码到最终的机器码实际上经过了两轮编译：Java 编译器将 Java 源代码编译成 Java 字节码，而即时编译器则将 Java 字节码编译成机器码。 Java 字节码本身并不适合直接作为可供优化的 IR。这是因为现代编译器一般采用静态单赋值（Static Single Assignment，SSA）IR。这种 IR 的特点是每个变量只能被赋值一次，而且只有当变量被赋值之后才能使用。 12345示例：x1=4*1024 经过常量折叠后变为 x1=4096x1=4; y1=x1 经过常量传播后变为 x1=4; y1=4y1=x1*3 经过强度削减后变为 y1=(x1&lt;&lt;1)+x1if(2&gt;1)&#123;y1=1;&#125;else&#123;y2=1;&#125;经过死代码删除后变为 y1=1 Sea-of-nodesGloval Value Numbering总结哥，讲的太深了，暂时根本用不到 引用 https://en.wikipedia.org/wiki/Static_single_assignment_form https://en.wikipedia.org/wiki/Reaching_definition","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"19Java字节码","slug":"aa_category/se/深入拆解 Java 虚拟机/19Java字节码","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/19Java字节码/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/19Java字节码/","excerpt":"常用指令dup通常用于复制 new 指令生成的未初始化的引用","text":"常用指令dup通常用于复制 new 指令生成的未初始化的引用 pop通常用来舍弃调用指令的返回结果 swap用来交换栈顶的两个元素 new 生成该类的未初始化的对象 Create a new class instance instanceof 判断栈顶元素是否为目标类 / 接口的实例。是则压入 1，否则压入 0 Check properties of class instances or arrays checkcast判断栈顶元素是否为目标类 / 接口的实例。如果不是便抛出异常 athrow将栈顶异常抛出 monitorenter为栈顶对象加锁 monitorexit为栈顶对象解锁 操作数栈 存放计算的操作数以及返回的结果 java每执行一条指令前要求该指令的操作数已经压入操作数栈 在执行指令的时候会将指令弹出栈，并将计算结果压入操作数栈 常量加载到操作数栈 局部变量加载存储指令 局部变量 是一个数组 this指针(非静态方法) 方法参数 字节码内部变量 说明 aload 0 指加载第0 局部变量元素到栈 数组访问指令 返回指令表 相关资料https://docs.oracle.com/javase/specs/jvms/se10/html/jvms-6.html#jvms-6.5","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"16-17即时编译","slug":"aa_category/se/深入拆解 Java 虚拟机/16-17即时编译","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/16-17即时编译/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/16-17即时编译/","excerpt":"分层编译将 Java 虚拟机的执行状态分为了五个层次 解释执行； 执行不带 profiling 的 C1 代码； 执行仅带方法调用次数以及循环回边执行次数 profiling 的 C1 代码； 执行带所有 profiling 的 C1 代码； 执行 C2 代码。","text":"分层编译将 Java 虚拟机的执行状态分为了五个层次 解释执行； 执行不带 profiling 的 C1 代码； 执行仅带方法调用次数以及循环回边执行次数 profiling 的 C1 代码； 执行带所有 profiling 的 C1 代码； 执行 C2 代码。 通常情况下，C2 代码的执行效率要比 C1 代码的高出 30% 以上。然而，对于 C1 代码的三种状态，按执行效率从高至低则是 1 层 &gt; 2 层 &gt; 3 层。 其中 1 层的性能比 2 层的稍微高一些，而 2 层的性能又比 3 层高出 30%。这是因为 profiling 越多，其额外的性能开销越大。 profiling 是指在程序执行过程中，收集能够反映程序执行状态的数据。这里所收集的数据我们称之为程序的 profile。 OSR （On-Stack-Replacement）编译它指的是在程序执行过程中，动态地替换掉 Java 方法栈桢，从而使得程序能够在非方法入口处进行解释执行和编译后的代码之间的切换。事实上，去优化（deoptimization）采用的技术也可以称之为 OSR。 在不启用分层编译的情况下，触发 OSR 编译的阈值是由参数 -XX:CompileThreshold 指定的阈值的倍数。 123(OnStackReplacePercentage - InterpreterProfilePercentage)/100其中 -XX:InterpreterProfilePercentage 的默认值为 33，当使用 C1 时 -XX:OnStackReplacePercentage 为 933，当使用 C2 时为 140。 在启用分层编译的情况下，触发 OSR 编译的阈值则是由参数 -XX:TierXBackEdgeThreshold 指定的阈值乘以系数。 基于分支 profile 的优化 根据条件跳转指令的分支 profile，即时编译器可以将从未执行过的分支剪掉，以避免编译这些很有可能不会用到的代码，从而节省编译时间以及部署代码所要消耗的内存空间。此外，“剪枝”将精简程序的数据流，从而触发更多的优化。 基于类型 profile 的优化在 Java 虚拟机中，instanceof 测试并不简单。如果 instanceof 的目标类型是 final 类型，那么 Java 虚拟机仅需比较测试对象的动态类型是否为该 final 类型。 如果目标类型不是 final 类型，比如说我们例子中的 Exception，那么 Java 虚拟机需要从测试对象的动态类型开始，依次测试该类，该类的父类、祖先类，该类所直接实现或者间接实现的接口是否与目标类型一致。 去优化Java 虚拟机给出的解决方案便是去优化，即从执行即时编译生成的机器码切换回解释执行。 实践今天的实践环节，你可以使用参数 -XX:+PrintCompilation 来打印你项目中的即时编译情况。","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"锁","slug":"aa_category/db/mysql/锁","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/db/mysql/锁/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/db/mysql/锁/","excerpt":"","text":"事务的隔离级别 读未提交（read uncommitted）,一个事务还没有提交，就能被其它事务看到 读提交（read committed），一个事务提交才能被其它事务看到 可重复读（repeatable read），一个事务执行过程中看到的数据和启动时看到的数据一致 串行化（serializable ），对于同一行记录，写会加写锁，读会加读锁，当出现读写锁冲突的时候，必须等待前一个事务结束，才能继续 常用命令 锁表LOCK TABLES WRITE 释放锁 UNLOCK TABLES select * from sys.innodb_lock_waits 查看等待 共享锁 用于读 SELECT k from t where id=1 可以进行写入操作 SELECT k from t where id=1 lock in share mode;则不可以进行写 排它锁 又称写锁，简称X锁，排它锁不能与其他锁并存，如果一个事物获取了一个数据的排它锁，其他事务就不能再获取该行的锁，只有获取了排它锁的事务可以对数据进行读取和修改 加锁释锁方式 自动 delete/update/insert 默认加X锁 手动 select * from t where id=1 for update commit/rollback 12345678910CREATE TABLE `t1` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into t1 VALUES(1,1);insert into t1 VALUES(4,4);insert into t1 VALUES(7,7);insert into t1 VALUES(10,10); 场景1-锁住整张表123456# 事务1start TRANSACTION;SELECT * from t1 where id=7 for update ; # 事务2start TRANSACTION;insert into t1 VALUES(11,11); 没有索引，或者没有命中索引就会锁住整张表 场景2-123456# 事务1start TRANSACTION;SELECT * from t1 where id=7 for update ; # 事务2start TRANSACTION;update t1 set k=22 where id=22 会 意向共享锁 桥梁作用，行锁和表锁的桥梁 事务获取共享锁之前必须先获取意向共享锁 意向排它锁 X IX S IS X Conflict Conflict Conflict Conflict IX Conflict Compatible Conflict Compatible S Conflict Conflict Compatible Compatible IS Conflict Compatible Compatible Compatible 记录锁 锁住的是索引 对于非聚集索引，先锁住普通索引，再锁住聚集索引 对于没有聚集索引的表，mysql会生成默认的聚集索引，所以会锁住所有的表 间隙锁&amp;临键锁12345678910CREATE TABLE `t2` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into t2 VALUES(1,1);insert into t2 VALUES(4,4);insert into t2 VALUES(7,7);insert into t2 VALUES(10,10); 间隙锁 (-无穷，1) ,(1,4),(4,7),(7,10),(10,+无穷) 临键锁 (-negative infinity，1] ,(1,4],(4,7],(7,10],(10,+positive infinity) 使用于范围查询 间隔锁只会阻止其他事务的插入操作，就是只有 insert 操作会产生 GAP 锁，update 操作不会参数 GAP 锁 目的是为了防止其他事务插入数据，所以读写不会阻塞 在可重复读级别启用 场景1-临键锁123456# 事务1start TRANSACTION; select * from t2 where id&gt;5 and id&lt;9 for update ; # 事务2start TRANSACTION; select * from t2 where id&gt;5 and id&lt;9 for update ; 由于 5,9之间有有记录7，所以是临键锁，锁住 (4,7],(7,10] 临键锁之间互斥的 场景2-间隙锁123456# 事务1start TRANSACTION; select * from t2 where id&gt;20 for update ; # 事务2start TRANSACTION; insert into t2 VALUES(12,12); 事务1是间隙锁，锁住 (10,+无穷)，防止区间的数据变动 事务2 插入阻塞 场景3-间隙锁123456# 事务1start TRANSACTION; select * from t2 where id&gt;20 for update ; # 事务2start TRANSACTION; select * from t2 where id&gt;20 for update ; 由于 没有大于20的记录，所以是间隙锁，锁住 (10,+无穷) 间隙锁之间不互斥 场景4-排它锁&amp;间隙锁123456# 事务1start TRANSACTION; select * from t2 where id=7 for update ;# 事务2start TRANSACTION; select * from t2 where id=7 for update ; 有主键等于7的记录，所以是排它锁，会互斥 123456# 事务1start TRANSACTION; select * from t2 where id=8 for update ;# 事务2start TRANSACTION; select * from t2 where id=8 for update ; 没有主键等于8的记录，所以是间隙锁，不会互斥 123456# 事务1start TRANSACTION; select * from t2 where id=8 for update ;# 事务2start TRANSACTION; insert into t2 VALUES(8,8); 没有主键等于8的记录，间隙(7,10)，会互斥 123456# 事务1start TRANSACTION; select * from t2 where id=8 for update ;# 事务2start TRANSACTION; update t2 set k=8 where id=8 没有主键等于8的记录，间隙(7,10)，不会互斥，具体原因不清楚 refenerencehttps://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"15Java语法糖与Java编译器","slug":"aa_category/se/深入拆解 Java 虚拟机/15Java语法糖与Java编译器","date":"2019-03-11T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/03/11/aa_category/se/深入拆解 Java 虚拟机/15Java语法糖与Java编译器/","link":"","permalink":"https://liyong.ac.cn/2019/03/11/aa_category/se/深入拆解 Java 虚拟机/15Java语法糖与Java编译器/","excerpt":"自动装箱与自动拆箱也就是说，我们可以通过配置该参数，扩大 Integer 缓存的范围。Java 虚拟机参数 -XX:+AggressiveOpts 也会将 IntegerCache.high 调整至 20000。","text":"自动装箱与自动拆箱也就是说，我们可以通过配置该参数，扩大 Integer 缓存的范围。Java 虚拟机参数 -XX:+AggressiveOpts 也会将 IntegerCache.high 调整至 20000。 泛型与类型擦除桥接方法123456789101112131415public class NumberF&lt;T extends Number&gt; &#123; public void add(T number) &#123; &#125;&#125;public class NumberS extends NumberF&lt;Integer&gt; &#123; @Override public void add(Integer number) &#123; &#125; &#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576javap -v NumberS.classClassfile /D:/workspace/demo/se-demo/src/main/java/brige/NumberS.class Last modified 2019-3-26; size 550 bytes MD5 checksum 5434bc6cbc8277ec249d44c992fe26bc Compiled from \"NumberS.java\"public class brige.NumberS extends brige.NumberF&lt;java.lang.Integer&gt; minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #5.#21 // brige/NumberF.\"&lt;init&gt;\":()V #2 = Class #22 // java/lang/Integer #3 = Methodref #4.#23 // brige/NumberS.add:(Ljava/lang/Integer;)V #4 = Class #24 // brige/NumberS #5 = Class #25 // brige/NumberF #6 = Utf8 &lt;init&gt; #7 = Utf8 ()V #8 = Utf8 Code #9 = Utf8 LineNumberTable #10 = Utf8 add #11 = Utf8 (Ljava/lang/Integer;)V #12 = Utf8 foo #13 = Utf8 ([I)V #14 = Utf8 StackMapTable #15 = Class #26 // \"[I\" #16 = Utf8 (Ljava/lang/Number;)V #17 = Utf8 Signature #18 = Utf8 Lbrige/NumberF&lt;Ljava/lang/Integer;&gt;; #19 = Utf8 SourceFile #20 = Utf8 NumberS.java #21 = NameAndType #6:#7 // \"&lt;init&gt;\":()V #22 = Utf8 java/lang/Integer #23 = NameAndType #10:#11 // add:(Ljava/lang/Integer;)V #24 = Utf8 brige/NumberS #25 = Utf8 brige/NumberF #26 = Utf8 [I&#123; public brige.NumberS(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method brige/NumberF.\"&lt;init&gt;\":()V 4: return LineNumberTable: line 16: 0 public void add(java.lang.Integer); descriptor: (Ljava/lang/Integer;)V flags: ACC_PUBLIC Code: stack=0, locals=2, args_size=2 0: return LineNumberTable: line 21: 0 // 桥接方法 public void add(java.lang.Number); descriptor: (Ljava/lang/Number;)V flags: ACC_PUBLIC, ACC_BRIDGE, ACC_SYNTHETIC Code: stack=2, locals=2, args_size=2 0: aload_0 1: aload_1 2: checkcast #2 // class java/lang/Integer 5: invokevirtual #3 // Method add:(Ljava/lang/Integer;)V 8: return LineNumberTable: line 16: 0&#125;Signature: #18 // Lbrige/NumberF&lt;Ljava/lang/Integer;&gt;;SourceFile: \"NumberS.java\"","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"jvm","slug":"jvm","permalink":"https://liyong.ac.cn/tags/jvm/"}]},{"title":"架构演进之路-网关","slug":"aa_category/methodology/公开课/架构演进之路-网关","date":"2019-02-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/02/01/aa_category/methodology/公开课/架构演进之路-网关/","link":"","permalink":"https://liyong.ac.cn/2019/02/01/aa_category/methodology/公开课/架构演进之路-网关/","excerpt":"","text":"网关 请求鉴权 数据完整性检查 协议转换 路由转发 服务治理(限流/熔断) 概念 newsql tidb https://pingcap.com/docs-cn/ https://github.com/CodisLabs/codis 同步转异步架构通过mq 读不能用异步 写可以异步","categories":[{"name":"standard","slug":"standard","permalink":"https://liyong.ac.cn/categories/standard/"}],"tags":[{"name":"standard","slug":"standard","permalink":"https://liyong.ac.cn/tags/standard/"}]},{"title":"15二分查找：如何用最省内存的方式实现快速查找功能","slug":"aa_category/algorithm/数据结构与算法之美/15二分查找如何用最省内存的方式实现快速查找功能","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/15二分查找如何用最省内存的方式实现快速查找功能/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/15二分查找如何用最省内存的方式实现快速查找功能/","excerpt":"无处不在的二分思想二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0","text":"无处不在的二分思想二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0 O(logn) 惊人的查找速度二分查找的递归与非递归实现123456789101112131415public int search(int[] a, int data) &#123; int high = a.length - 1; int low = 0; while (low &lt;= high) &#123; int middle = low + ((high - low) &gt;&gt; 1); if (a[middle] == data) &#123; return middle; &#125; else if (a[middle] &gt; data) &#123; high = middle - 1; &#125; else &#123; low = middle + 1; &#125; &#125; return Integer.MIN_VALUE; &#125; 1. 循环退出条件注意是 low&lt;=high，而不是 low&lt;high 2.mid 的取值mid=(low+high)/2 这种写法是有问题的。因为如果 low 和 high 比较大的话，两者之和就有可能会溢出 3.low 和 high 的更新low=mid+1，high=mid-1。注意这里的 +1 和 -1，如果直接写成 low=mid 或者 high=mid，就可能会发生死循环。比如，当 high=3，low=3 时，如果 a[3] 不等于 value，就会导致一直循环不退出。 1234567891011121314151617public int search(int[] a, int data) &#123; return searchInternally(a, 0, a.length - 1, data);&#125;public int searchInternally(int[] a, int low, int high, int data) &#123; if (low &lt;= high) &#123; int middle = low + ((high - low) &gt;&gt; 1); if (a[middle] == data) &#123; return middle; &#125; else if (a[middle] &gt; data) &#123; return searchInternally(a, low, middle - 1, data); &#125; else &#123; return searchInternally(a, middle + 1, high, data); &#125; &#125; return Integer.MIN_VALUE;&#125; 二分查找应用场景的局限性 二分查找依赖的是顺序表结构，简单点说就是数组 二分查找能否依赖其他数据结构呢？比如链表。答案是不可以的，主要原因是二分查找算法需要按照下标随机访问元 二分查找针对的是有序数据 数据量太小不适合二分查找 数据量太大也不适合二分查找 二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"11排序：为什么插入排序比冒泡排序更受欢迎","slug":"aa_category/algorithm/数据结构与算法之美/11排序：为什么插入排序比冒泡排序更受欢迎","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/11排序：为什么插入排序比冒泡排序更受欢迎/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/11排序：为什么插入排序比冒泡排序更受欢迎/","excerpt":"如何分析一个“排序算法”？排序算法的执行效率 最好情况、最坏情况、平均情况时间复杂度 时间复杂度的系数、常数 、低阶 比较次数和交换（或移动）次数","text":"如何分析一个“排序算法”？排序算法的执行效率 最好情况、最坏情况、平均情况时间复杂度 时间复杂度的系数、常数 、低阶 比较次数和交换（或移动）次数 排序算法的内存消耗原地排序算法，就是特指空间复杂度是 O(1) 的排序算法 排序算法的稳定性稳定性:如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。 冒泡排序（Bubble Sort） 冒泡排序只会操作相邻的两个数据 每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。 一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作 123456789public void bubbleSort(int[] a) &#123; for (int i = 0; i &lt; a.length-1; i++) &#123; for (int j = 1; j &lt; a.length - i; j++) &#123; if (a[j] &lt; a[j - 1]) &#123; SortUtil.swap(a, j, j - 1); &#125; &#125; &#125;&#125; 刚讲的冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。我这里还有另外一个例子，这里面给 6 个元素排序，只需要 4 次冒泡操作就可以了。 冒泡排序的时间复杂度是多少最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是 O(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以最坏情况时间复杂度为 O(n2) 有序度是数组中具有有序关系的元素对的个数 1有序元素对：a[i] &lt;= a[j], 如果 i &lt; j。 对于一个倒序排列的数组，比如 6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是n*(n-1)/2，也就是 15。我们把这种完全有序的数组的有序度叫作满有序度。 逆序度 = 满有序度 - 有序度 冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是n*(n-1)/2–初始有序度。 插入排序（Insertion Sort）一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。 12345678910public void insertSort(int[] a) &#123; for (int i = 1; i &lt; a.length; i++) &#123; int j = i; int insertValue = a[i]; for (; j &gt;= 1 &amp;&amp; (a[j] &lt; a[j - 1]); j--) &#123; a[j] = a[j - 1]; &#125; a[j] = insertValue; &#125;&#125; 首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。 如图所示，要排序的数据是 4，5，6，1，3，2，其中左侧为已排序区间，右侧是未排序区间。 对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度 选择排序（Selection Sort）选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。 12345678910111213public void selectionSort(int[] a) &#123; for (int i = 0; i &lt; a.length-1; i++) &#123; int minIndex= i; for (int j = i; j &lt; a.length; j++) &#123; if (a[minIndex] &gt; a[j]) &#123; minIndex = j; &#125; &#125; if (i != minIndex) &#123; SortUtil.swap(a, i, minIndex); &#125; &#125;&#125; 小结","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"40-41-42动态规划","slug":"aa_category/algorithm/数据结构与算法之美/40-41-42动态规划","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/40-41-42动态规划/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/40-41-42动态规划/","excerpt":"","text":"","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"39回溯算法","slug":"aa_category/algorithm/数据结构与算法之美/39回溯算法","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/39回溯算法/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/39回溯算法/","excerpt":"如何理解“回溯算法“回溯法采用试错的思想，它尝试分步的去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题的答案 用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解 回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧 利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。","text":"如何理解“回溯算法“回溯法采用试错的思想，它尝试分步的去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题的答案 用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解 回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧 利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。 八皇后","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"38分治算法","slug":"aa_category/algorithm/数据结构与算法之美/38分治算法","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/38分治算法/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/38分治算法/","excerpt":"如何理解“贪心算法”分治法，就是把一个复杂的问题分成两个或更多的相同或相似的子问题，直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并 分治算法的递归实现中，每一层递归都会涉及这样三个操作： 分解：将原问题分解成一系列子问题； 解决：递归地求解各个子问题，若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题。","text":"如何理解“贪心算法”分治法，就是把一个复杂的问题分成两个或更多的相同或相似的子问题，直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并 分治算法的递归实现中，每一层递归都会涉及这样三个操作： 分解：将原问题分解成一系列子问题； 解决：递归地求解各个子问题，若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题。 分治算法能解决的问题，一般需要满足下面这几个条件： 原问题与分解成的小问题具有相同的模式； 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法； 具有分解终止条件，也就是说，当问题足够小时，可以直接求解； 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了 求逆序对个数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class DivideConquer &#123; private int num = 0; // 全局变量或者成员变量 public static void main(String[] args) &#123; int[] a = &#123;4, 5, 6, 1, 2, 3&#125;; System.out.println(new DivideConquer().count(a, a.length)); &#125; public int count(int[] a, int n) &#123; num = 0; mergeSortCounting(a, 0, n - 1); return num; &#125; private void mergeSortCounting(int[] a, int p, int r) &#123; if (p &lt; r) &#123; int q = p + ((r - p) &gt;&gt; 1); mergeSortCounting(a, p, q); mergeSortCounting(a, q + 1, r); merge(a, p, q, r); &#125; &#125; private void merge(int[] a, int p, int q, int r) &#123; int[] tmp = new int[r - p + 1]; int i = p; int j = q + 1; int k = 0; while (i &lt;= q &amp;&amp; j &lt;= r) &#123; if (a[i] &lt;= a[j]) &#123; tmp[k++] = a[i++]; &#125; else &#123;// 统计 p-q 之间，比 a[j] 大的元素个数 num += (q - i + 1); tmp[k++] = a[j++]; &#125; &#125; while (i &lt;= q) &#123; tmp[k++] = a[i++]; &#125; while (j &lt;= r) &#123; tmp[k++] = a[j++]; &#125; for (int m = 0; m &lt; tmp.length; m++) &#123; a[p + m] = tmp[m]; &#125; &#125; &#125;","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"学习指导手册","slug":"aa_category/algorithm/数据结构与算法之美/35Trie树","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/35Trie树/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/35Trie树/","excerpt":"什么是“Trie 树 Trie 树，也叫“字典树”。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题 Trie 树比较适合的是查找前缀匹配的字符串","text":"什么是“Trie 树 Trie 树，也叫“字典树”。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题 Trie 树比较适合的是查找前缀匹配的字符串 how，hi，her，hello，so，see 构造 查找查找字符串“her”，那我们将要查找的字符串分割成单个的字符 h，e，r，然后从 Trie 树的根节点开始匹配。如图所示，绿色的路径就是在 Trie 树中匹配的路径。 如何实现一棵 Trie 树 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class TrieTree &#123; public static void main(String[] args) &#123; TrieTree tree = new TrieTree();// hello、her、hi、how、so、see tree.insert(\"hello\".toCharArray()); tree.insert(\"hello\".toCharArray()); tree.insert(\"her\".toCharArray()); tree.insert(\"hi\".toCharArray()); tree.insert(\"how\".toCharArray()); tree.insert(\"so\".toCharArray()); tree.insert(\"see\".toCharArray()); System.out.println(tree.find(\"se\".toCharArray())); System.out.println(tree.find(\"see\".toCharArray())); &#125; private TrieNode root = new TrieNode('/'); // 存储无意义字符 public void insert(char[] text) &#123; TrieNode p= root; for (char c : text) &#123; int index = c - 'a'; if (p.children[index] == null) &#123; TrieNode trieNode = new TrieNode(c); p.children[index]= trieNode; &#125; p = p.children[index]; &#125; p.isEndingChar = true; &#125; // 在 Trie 树中查找一个字符串 public boolean find(char[] pattern) &#123; TrieNode p = root; for (char c : pattern) &#123; int index = c - 'a'; if (p.children[index] == null) &#123; return false; &#125; p = p.children[index]; &#125; if (p.isEndingChar) &#123; return true; &#125; else &#123; return false; &#125; &#125; public class TrieNode &#123; public char data; public TrieNode[] children = new TrieNode[26]; public boolean isEndingChar = false; public TrieNode(char data) &#123; this.data = data; &#125; &#125;&#125; 构建好 Trie 树后，在其中查找字符串的时间复杂度是 O(k)，k 表示要查找的字符串的长度 Trie 树真的很耗内存吗字符串中包含从 a 到 z 这 26 个字符，那每个节点都要存储一个长度为 26 的数组，并且每个数组存储一个 8 字节指针（。而且，即便一个节点只有很少的子节点，远小于 26 个，比如 3、4 个，我们也要维护一个长度为 26 的数组 Trie 树与散列表、红黑树的比较 字符串中包含的字符集不能太大 要求字符串的前缀重合比较多，不然空间消耗会变大很多 通过指针串起来的数据块是不连续的，而 Trie 树中用到了指针","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"14序优化：如何实现一个通用的、高性能的排序函数","slug":"aa_category/algorithm/数据结构与算法之美/14排序优化：如何实现一个通用的、高性能的排序函数","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/14排序优化：如何实现一个通用的、高性能的排序函数/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/14排序优化：如何实现一个通用的、高性能的排序函数/","excerpt":"如何选择合适的排序算法？","text":"如何选择合适的排序算法？ 如何优化快速排序? 这种 O(n2) 时间复杂度出现的主要原因还是因为我们分区点选的不够合理 最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多 三数取中法我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点 随机法举例分析排序函数qsort 数据量小使用归并排序来排序输入数据 要排序的数据量比较大的时候，qsort() 会改为用快速排序算法来排序 元素的个数小于等于 4 时，qsort() 就退化为插入排序 O(n2) 时间复杂度的算法并不一定比 O(nlogn) 的算法执行时间长","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"37贪心算法","slug":"aa_category/algorithm/数据结构与算法之美/37贪心算法","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/37贪心算法/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/37贪心算法/","excerpt":"如何理解“贪心算法”贪心算法（英语：greedy algorithm），又称贪婪算法，是一种在每一步选择中都采取在当前状态下最好或最优（即最有利）的选择，从而希望导致结果是最好或最优的算法 A greedy algorithm is an algorithmic paradigm that follows the problem solving heuristic of making the locally optimal choice at each stage[1] with the intent of finding a global optimum 用贪心算法解决问题的思路，并不总能给出最优解","text":"如何理解“贪心算法”贪心算法（英语：greedy algorithm），又称贪婪算法，是一种在每一步选择中都采取在当前状态下最好或最优（即最有利）的选择，从而希望导致结果是最好或最优的算法 A greedy algorithm is an algorithmic paradigm that follows the problem solving heuristic of making the locally optimal choice at each stage[1] with the intent of finding a global optimum 用贪心算法解决问题的思路，并不总能给出最优解 贪心算法实战分析分糖果我们有 m 个糖果和 n 个孩子。我们现在要把糖果分给这些孩子吃，但是糖果少，孩子多（m&lt;n），所以糖果只能分配给一部分孩子 钱币找零假设我们有 1 元、2 元、5 元、10 元、20 元、50 元、100 元这些面额的纸币，它们的张数分别是 c1、c2、c5、c10、c20、c50、c100。我们现在要用这些钱来支付 K 元，最少要用多少张纸币呢 区间覆盖 n 个区间中最左端点是 lmin，最右端点是 rmax。这个问题就相当于，我们选择几个不相交的区间，从左到右将 [lmin, rmax] 覆盖上 每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"32-33-34字符串匹配","slug":"aa_category/algorithm/数据结构与算法之美/32-33-34字符串匹配","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/32-33-34字符串匹配/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/32-33-34字符串匹配/","excerpt":"","text":"BF 算法BF 是 Brute Force 的缩写，中文叫作暴力匹配算法，也叫朴素匹配算法 在主串中，检查起始位置分别是 0、1、2…n-m 且长度为 m 的 n-m+1 个子串，看有没有跟模式串匹配的 最坏情况时间复杂度是 O(n*m) 常用算法的原因 实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长 朴素字符串匹配算法思想简单，代码实现也非常简单 RK 算法我们通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题） 提高哈希算法的效率比如要处理的字符串只包含 a～z 这 26 个小写字母，那我们就用二十六进制来表示一个字符串。我们把 a～z 这 26 个字符映射到 0～25 这 26 个数字，a 就表示 0，b 就表示 1，以此类推，z 表示 25 BM 算法BM 算法包含两部分，分别是坏字符规则（bad character rule）和好后缀规则（good suffix shift） 坏字符规则BM 算法的匹配顺序比较特别，它是按照模式串下标从大到小的顺序，倒着匹配的 从模式串的末尾往前倒着匹配，当我们发现某个字符没法匹配的时候。我们把这个没有匹配的字符叫作坏字符（主串中的字符） 拿坏字符 c 在模式串中查找，发现模式串中并不存在这个字符，也就是说，字符 c 与模式串中的任何字符都不可能匹配。这个时候，我们可以将模式串直接往后滑动三位，将模式串滑动到 c 后面的位置，再从模式串的末尾字符开始比较。 模式串中最后一个字符 d，还是无法跟主串中的 a 匹配，这个时候，还能将模式串往后滑动三位吗？答案是不行的。因为这个时候，坏字符 a 在模式串中是存在的，模式串中下标是 0 的位置也是字符 a。这种情况下，我们可以将模式串往后滑动两位，让两个 a 上下对齐，然后再从模式串的末尾字符开始，重新匹配 当发生不匹配的时候，我们把坏字符对应的模式串中的字符下标记作 si。如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作 xi。如果不存在，我们把 xi 记作 -1。那模式串往后移动的位数就等于 si-xi 好后缀规则已经匹配的 bc 叫作好后缀，记作{u}。我们拿它在模式串中查找，如果找到了另一个跟{u}相匹配的子串{u}，那我们就将模式串滑动到子串{u}与主串中{u}对齐的位置 如果在模式串中找不到另一个等于{u}的子串，就直接将模式串，滑动到主串中{u}的后面，因为之前的任何一次往后滑动，都没有匹配主串中{u}的情况 当模式串中不存在等于{u}的子串时，我们直接将模式串滑动到主串{u}的后面。这样做是否有点太过头呢? 如果好后缀在模式串中不存在可匹配的子串，那在我们一步一步往后滑动模式串的过程中，只要主串中的{u}与模式串有重合，那肯定就无法完全匹配。但是当模式串滑动到前缀与主串中{u}的后缀有部分重合的时候，并且重合的部分相等的时候，就有可能会存在完全匹配的情况 要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。所谓某个字符串 s 的后缀子串，就是最后一个字符跟 s 对齐的子串，比如 abc 的后缀子串就包括 c, bc 当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则 BM 算法代码实现12345678private void generateBC(char[] b, int m, int[] bc) &#123; for (int i = 0; i &lt; SIZE; i++) &#123; bc[i] = -1; &#125; for (int i = 0; i &lt; m; i++) &#123; bc[(int) b[i]] = i; &#125;&#125; 123456789101112131415161718public int bm(char[] a, int n, char[] b, int m) &#123; int[] bc = new int[SIZE]; generateBC(b, m, bc); int i = 0; while (i &lt;= n - m) &#123; int j ; for ( j = m - 1; j &gt;= 0; j--) &#123; if (a[i + j] != b[j]) &#123; break; &#125; &#125; if (j &lt; 0) &#123; return i; &#125; i = i+(j-bc[(int)a[i+j]]); &#125; return Integer.MIN_VALUE;&#125; 12345678910111213141516171819202122232425262728293031323334353637// a,b 表示主串和模式串；n，m 表示主串和模式串的长度。 public int bm2(char[] a, int n, char[] b, int m) &#123; int[] bc = new int[SIZE]; // 记录模式串中每个字符最后出现的位置 generateBC(b, m, bc); // 构建坏字符哈希表 int[] suffix = new int[m]; boolean[] prefix = new boolean[m]; generateGS(b, m, suffix, prefix); int i = 0; // j 表示主串与模式串匹配的第一个字符 while (i &lt;= n - m) &#123; int j; for (j = m - 1; j &gt;= 0; --j) &#123; // 模式串从后往前匹配 if (a[i+j] != b[j]) break; // 坏字符对应模式串中的下标是 j &#125; if (j &lt; 0) &#123; return i; // 匹配成功，返回主串与模式串第一个匹配的字符的位置 &#125; int x = j - bc[(int)a[i+j]]; int y = 0; if (j &lt; m-1) &#123; // 如果有好后缀的话 y = moveByGS(j, m, suffix, prefix); &#125; i = i + Math.max(x, y); &#125; return -1; &#125; // j 表示坏字符对应的模式串中的字符下标 ; m 表示模式串长度 private int moveByGS(int j, int m, int[] suffix, boolean[] prefix) &#123; int k = m - 1 - j; // 好后缀长度 if (suffix[k] != -1) return j - suffix[k] +1; for (int r = j+2; r &lt;= m-1; ++r) &#123; if (prefix[m-r] == true) &#123; return r; &#125; &#125; return m; &#125; KMP算法KMP 算法也可以提前构建一个数组，用来存储模式串中每个前缀的最长可匹配前缀子串的结尾字符下标 https://www.zhihu.com/question/21923021","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"30-31图","slug":"aa_category/algorithm/数据结构与算法之美/30-31图","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/30-31图/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/30-31图/","excerpt":"如何理解“图”无向图 图中的元素我们就叫作顶点（vertex） 图中的一个顶点可以与任意其他顶点建立连接关系,这种关系叫边 度（degree），跟顶点相连接的边的条数。","text":"如何理解“图”无向图 图中的元素我们就叫作顶点（vertex） 图中的一个顶点可以与任意其他顶点建立连接关系,这种关系叫边 度（degree），跟顶点相连接的边的条数。 有向图 入度，表示有多少条边指向这个顶点 出度，表示有多少条边是以这个顶点为起点指向其他顶点 带权图（weighted graph） 邻接矩阵存储方法 比较浪费存储空间 定点多关系少 存储方式简单，直观 邻接表存储方法 广度优先搜索（Breadth-First-Search） visited是用来记录已经被访问的顶点，用来避免顶点被重复访问 queue是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点 prev用来记录搜索路径 123456789101112131415161718192021222324252627282930313233public void bfs(int s, int t) &#123; boolean[] visited = new boolean[v]; Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); int[] pre = new int[v]; for (int i = 0; i &lt; v; i++) &#123; pre[i] = -1; &#125; queue.add(s); visited[s] = true; while (!queue.isEmpty()) &#123; Integer p = queue.poll(); for (int i = 0; i &lt; adj[p].size(); i++) &#123; Integer w = adj[p].get(i); if (!visited[w]) &#123; pre[w] = p; visited[w] = true; if (w == t) &#123; print(pre, s, t); return; &#125; queue.add(w); &#125; &#125; &#125;&#125;private void print(int[] prev, int s, int t) &#123; if (s != t &amp;&amp; prev[t] != -1)&#123; print(prev, s, prev[t]); &#125; System.out.print(t+\" \");&#125; 深度优先搜索（DFS） 12345678910111213141516171819202122232425262728boolean found = false;public void dfs(int s, int t)&#123; boolean[] visited = new boolean[v]; int[] prev = new int[v]; for (int i = 0; i &lt; v; i++) &#123; prev[i]=-1; &#125; recurDfs(s, t, visited, prev); print(prev, s, t);&#125;private void recurDfs(int w, int t, boolean[] visited, int[] prev)&#123; if (found) &#123; return ; &#125; visited[w] =true; if (w == t) &#123; found = true; return ; &#125; for (int i = 0; i &lt; adj[w].size(); i++) &#123; Integer q = adj[w].get(i); if (!visited[q]) &#123; prev[q] =w; recurDfs(q, t, visited, prev); &#125; &#125;&#125;","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"27递归树：如何借助树来求解递归算法的时间复杂度","slug":"aa_category/algorithm/数据结构与算法之美/27递归树：如何借助树来求解递归算法的时间复杂度","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/27递归树：如何借助树来求解递归算法的时间复杂度/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/27递归树：如何借助树来求解递归算法的时间复杂度/","excerpt":"","text":"","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"28-29堆和堆排序","slug":"aa_category/algorithm/数据结构与算法之美/28-29堆和堆排序","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/28-29堆和堆排序/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/28-29堆和堆排序/","excerpt":"如何理解“堆” 堆是一个完全二叉树； 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。 每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大顶堆” 对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小顶堆","text":"如何理解“堆” 堆是一个完全二叉树； 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。 每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大顶堆” 对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小顶堆 如何实现一个堆往堆中插入一个元素 1234567891011public boolean insert(int data) &#123; if (count &gt;= n) &#123; return false; &#125; a[++count] = data; int i = count; while (i / 2 &gt; 0 &amp;&amp; a[i] &gt; a[i / 2]) &#123; SortUtil.swap(a, i, i / 2); &#125; return true;&#125; 删除堆顶元素 1234567891011121314151617181920212223242526public void heapify(int[] a, int n, int i) &#123; int maxPos = i; while (true) &#123; if (i * 2 &lt;= n &amp;&amp; a[i] &lt; a[i * 2]) &#123; maxPos = i * 2; &#125; if ((i * 2 + 1) &lt;= n &amp;&amp; a[maxPos] &lt; a[i * 2 + 1]) &#123; maxPos = i * 2 + 1; &#125; if (maxPos == i) &#123; break; &#125; else &#123; SortUtil.swap(a, i, maxPos); i = maxPos; &#125; &#125; &#125;public boolean removeMax() &#123; if (count &lt; 1) &#123; return false; &#125; a[1] = a[count]; count--; heapify(a, count, 1); return true; &#125; 如何基于堆实现排序建堆 12345public void buildHeap(int[] a,int n)&#123; for (int i = a.length / 2; i &gt;= 1; i--) &#123; heapify(a, n, i); &#125;&#125; h=log2nh=log2⁡n，代入公式 SS，就能得到 S=O(n)S=O(n)，所以，建堆的时间复杂度就是 O(n) 排序123456789public void heapSort(int[] a, int n) &#123; buildHeap(a, n); int k = n; while (k &gt; 1) &#123; SortUtil.swap(a, 1, k); k--; heapify(a,k,1); &#125; &#125; 建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 O(nlogn)，所以，堆排序整体的时间复杂度是 O(nlogn) 应用 优先级队列 利用堆求TOP K 维护一个大小为 K 的小顶堆，顺序遍历数组，从数组中取出取数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组 利用堆求中位数 维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据 新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；如果新加入的数据大于等于小顶堆的堆顶元素，我们就将这个新数据插入到小顶堆","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"25-26红黑树","slug":"aa_category/algorithm/数据结构与算法之美/25-26红黑树","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/25-26红黑树/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/25-26红黑树/","excerpt":"什么是“平衡二叉查找树”二叉树中任意一个节点的左右子树的高度相差不能大于 1。从这个定义来看，上一节我们讲的完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。","text":"什么是“平衡二叉查找树”二叉树中任意一个节点的左右子树的高度相差不能大于 1。从这个定义来看，上一节我们讲的完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。 如何定义一棵“红黑树”？ 根节点是黑色的； 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据； 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的； 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点； 左旋（rotate left）、右旋（rotate right）。左旋全称其实是叫围绕某个节点的左旋，那右旋的全称估计你已经猜到了，就叫围绕某个节点的右旋。 插入操作的平衡调整红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。所以，关于插入操作的平衡调整，有这样两种特殊情况，但是也都非常好处理。 如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。 如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。 除此之外，其他情况都会违背红黑树的定义，于是我们就需要进行调整，调整的过程包含两种基础的操作：左右旋转和改变颜色。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"24二叉树基础：有了如此高效的散列表，为什么还需要二叉树","slug":"aa_category/algorithm/数据结构与算法之美/24二叉树基础：有了如此高效的散列表，为什么还需要二叉树","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/24二叉树基础：有了如此高效的散列表，为什么还需要二叉树/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/24二叉树基础：有了如此高效的散列表，为什么还需要二叉树/","excerpt":"二叉查找树（Binary Search Tree） 二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值","text":"二叉查找树（Binary Search Tree） 二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值 二叉查找树的插入操作12345678910111213141516171819202122232425public void inset(int data) &#123; if (tree == null) &#123; tree = new Node(data); return; &#125; Node p = tree; while (p != null) &#123; if (p.data &gt; data) &#123; if (p.left == null) &#123; p.left = new Node(data); return; &#125; else &#123; p = p.left; &#125; &#125; else &#123; if (p.right == null) &#123; p.right = new Node(data); return; &#125; else &#123; p = p.right; &#125; &#125; &#125;&#125; 二叉查找树的查找操作12345678910111213public Node find(int data) &#123; Node node = tree; while (node != null) &#123; if (node.data == data) &#123; return node; &#125; else if (node.data &gt; data) &#123; node = node.left; &#125; else if (node.data &lt; data) &#123; node = node.right; &#125; &#125; return null;&#125; 二叉查找树的删除操作 如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null 如果要删除的节点只有一个子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了 如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点 123456789101112131415161718192021222324252627282930313233343536373839404142public boolean delete(int data) &#123; Node p = tree; Node pp = null; while (p != null &amp;&amp; p.data != data) &#123; pp = p; if (p.data &gt; data) &#123; p = p.left; &#125; else &#123; p = p.right; &#125; &#125; if (p == null) &#123; return false; &#125; if (p.left != null &amp;&amp; p.right != null) &#123; Node minP = p.right; Node minPP = p; while (minP.left != null) &#123; minPP = minP; minP = minP.left; &#125; p.data = minP.data; pp = minPP; p = minP; &#125; Node child = null; if (p.left != null) &#123; child = p.left; &#125; else if (p.right != null) &#123; child = p.right; &#125; else &#123; child = null; &#125; if (pp == null) &#123; tree = null; &#125; else if (pp.left == p) &#123; pp.left = child; &#125; else if (pp.right == p) &#123; pp.right = child; &#125; return true;&#125; 二叉查找树的其他操作 查找最大节点 查找最小节点 查找前驱节点 查找后继节点 中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效。因此，二叉查找树也叫作二叉排序树。 支持重复数据的二叉查找树方法一二叉查找树中每一个节点不仅会存储一个数据，通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上 方法二插入在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树 查找当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止 二叉查找树的时间复杂度分析12n &gt;= 1+2+4+8+...+2^(L-2)+1n &lt;= 1+2+4+8+...+2^(L-2)+2^(L-1) 有了散列表为什么还有二叉树 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序 列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定 笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小 散列表的构造比二叉查找树要复杂，需要考虑的东西很多","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"23 二叉树基础：什么样的二叉树适合用数组来存储","slug":"aa_category/algorithm/数据结构与算法之美/23 二叉树基础：什么样的二叉树适合用数组来存储","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/23 二叉树基础：什么样的二叉树适合用数组来存储/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/23 二叉树基础：什么样的二叉树适合用数组来存储/","excerpt":"树（Tree）","text":"树（Tree） 二叉树（Binary Tree） 编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作满二叉树 编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫作完全二叉树 表示（或者存储）一棵二叉树链式存储法顺序存储法根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置。以此类推，B 节点的左子节点存储在 2 * i = 2 * 2 = 4 的位置，右子节点存储在 2 * i + 1 = 2 * 2 + 1 = 5 的位置 二叉树的遍历 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树 印它的右子树。 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身 1234567891011121314151617181920212223242526public void preOrder(Node node) &#123; if (node == null) &#123; return; &#125; System.out.print(node.data + \" , \"); preOrder(node.left); preOrder(node.right);&#125;public void inOrder(Node node) &#123; if (node == null) &#123; return; &#125; inOrder(node.left); System.out.print(node.data + \" , \"); inOrder(node.right);&#125;public void postOrder(Node node) &#123; if (node == null) &#123; return; &#125; postOrder(node.left); postOrder(node.right); System.out.print(node.data + \" , \");&#125;","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"21-22哈希算法","slug":"aa_category/algorithm/数据结构与算法之美/21-22哈希算法","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/21-22哈希算法/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/21-22哈希算法/","excerpt":"什么是哈希算法 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法） 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小","text":"什么是哈希算法 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法） 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小 安全加密 鸽巢原理，如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个 唯一标识 数据校验 散列函数 负载均衡 数据分片 分布式存储","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"09队列：队列在线程池等有限资源池中的应用","slug":"aa_category/algorithm/数据结构与算法之美/09队列：队列在线程池等有限资源池中的应用","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/09队列：队列在线程池等有限资源池中的应用/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/09队列：队列在线程池等有限资源池中的应用/","excerpt":"如何理解“队列”？ 先进者先出，这就是典型的“队列” 入队 enqueue()，放一个数据到队列尾部；出队 dequeue()，从队列头部取一个元素。","text":"如何理解“队列”？ 先进者先出，这就是典型的“队列” 入队 enqueue()，放一个数据到队列尾部；出队 dequeue()，从队列头部取一个元素。 顺序队列和链式队列 用数组实现的队列叫作顺序队列 用链表实现的队列叫作链式队列 循环队列 队列为空的判断条件仍然是 head == tail 当队满时，(tail+1)%n=head 阻塞队列和并发队列 队列为空的时候，从队头取数据会被阻塞 队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"08栈：如何实现浏览器的前进和后退功能","slug":"aa_category/algorithm/数据结构与算法之美/08栈：如何实现浏览器的前进和后退功能","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/08栈：如何实现浏览器的前进和后退功能/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/08栈：如何实现浏览器的前进和后退功能/","excerpt":"如何理解“栈”？当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构","text":"如何理解“栈”？当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构 如何实现一个“栈”栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈 顺序栈 时间复杂度都是 O(1) 空间复杂度是 O(1) 支持动态扩容的顺序栈栈在函数调用中的应用栈在表达式求值中的应用 编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈.从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。 3+5*8-6 栈在括号匹配中的应用用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"06-07链表","slug":"aa_category/algorithm/数据结构与算法之美/06-07链表","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/06-07链表/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/06-07链表/","excerpt":"理解指针或引用的含义不管是“指针”还是“引用”，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。","text":"理解指针或引用的含义不管是“指针”还是“引用”，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。 警惕指针丢失和内存泄漏利用哨兵简化实现难度重点留意边界条件处理 如果链表为空时，代码是否能正常工作？ 如果链表只包含一个结点时，代码是否能正常工作？ 如果链表只包含两个结点时，代码是否能正常工作？ 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？ 举例画图，辅助思考多写多练，没有捷径","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"03-04如何分析、统计算法的执行效率和资源消耗","slug":"aa_category/algorithm/数据结构与算法之美/03-04复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/03-04复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/03-04复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗/","excerpt":"时间复杂度分析 只关注循环执行次数最多的一段代码 加法法则：总复杂度等于量级最大的那段代码的复杂度 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积","text":"时间复杂度分析 只关注循环执行次数最多的一段代码 加法法则：总复杂度等于量级最大的那段代码的复杂度 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积 几种常见时间复杂度实例分析O(1)只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1) O(logn)、O(nlogn)1234i=1;while (i &lt;= n) &#123; i = i * 2;&#125; 不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。为什么呢？ 对数之间是可以互相转换的，log3n 就等于 log32 * log2n，所以 O(log3n) = O(C * log2n)，其中 C=log32 是一个常量。基于我们前面的一个理论：在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))。所以，O(log2n) 就等于 O(log3n)。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 O(logn)。 O(m+n)、O(m*n)123456789101112131415int cal(int m, int n) &#123; int sum_1 = 0; int i = 1; for (; i &lt; m; ++i) &#123; sum_1 = sum_1 + i; &#125; int sum_2 = 0; int j = 1; for (; j &lt; n; ++j) &#123; sum_2 = sum_2 + j; &#125; return sum_1 + sum_2;&#125; 空间复杂度分析1234567891011void print(int n) &#123; int i = 0; int[] a = new int[n]; for (i; i &lt;n; ++i) &#123; a[i] = i * i; &#125; for (i = n-1; i &gt;= 0; --i) &#123; print out a[i] &#125;&#125; 第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。第 3 行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 最好情况时间复杂度（best case time complexity） 最坏情况时间复杂度（worst case time complexity） 平均情况时间复杂度（average case time complexity） 均摊时间复杂度（amortized time complexity） 1234567891011121314151617181920 // array 表示一个长度为 n 的数组 // 代码中的 array.length 就等于 n int[] array = new int[n]; int count = 0; void insert(int val) &#123; if (count == array.length) &#123; int sum = 0; for (int i = 0; i &lt; array.length; ++i) &#123; sum = sum + array[i]; &#125; array[0] = sum; count = 1; &#125;array[count] = val;++count; &#125;","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"02如何抓住重点，系统高效地学习数据结构与算法","slug":"aa_category/algorithm/数据结构与算法之美/02如何抓住重点，系统高效地学习数据结构与算法","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/02如何抓住重点，系统高效地学习数据结构与算法/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/02如何抓住重点，系统高效地学习数据结构与算法/","excerpt":"学习的重点在什么地方？","text":"学习的重点在什么地方？ 一些可以让你事半功倍的学习技巧 边学边练，适度刷题 多问、多思考、多互动 打怪升级学习法 知识需要沉淀，不要想试图一下子掌握所有","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"16二分查找：如何快速定位IP对应的省份地址","slug":"aa_category/algorithm/数据结构与算法之美/16二分查找：如何快速定位IP对应的省份地址","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/16二分查找：如何快速定位IP对应的省份地址/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/16二分查找：如何快速定位IP对应的省份地址/","excerpt":"查找第一个值等于给定值的元素","text":"查找第一个值等于给定值的元素 12345678910111213141516171819public int search(int[] a, int data) &#123; int low = 0 ; int high = a.length-1; while (low &lt;= high) &#123; int middle = low + ((high - low) &gt;&gt; 1); if (a[middle] &gt; data) &#123; high = middle - 1; &#125; else if (a[middle] &lt; data) &#123; low = middle + 1; &#125; else &#123; if (middle == 0 || a[middle - 1] != data) &#123; return middle; &#125; else &#123; high = middle-1; &#125; &#125; &#125; return Integer.MIN_VALUE;&#125; 查找最后一个值等于给定值的元素12345678910111213141516171819public int search(int[] a, int data) &#123; int low = 0; int high = a.length - 1; while (low &lt;= high) &#123; int middle = low + ((high - low) &gt;&gt; 1); if (a[middle] &gt; data) &#123; high = middle - 1; &#125; else if (a[middle] &lt; data) &#123; low = middle + 1; &#125; else &#123; if (middle == a.length - 1 || a[middle + 1] != data) &#123; return middle; &#125; else &#123; low = middle + 1; &#125; &#125; &#125; return Integer.MIN_VALUE;&#125; 查找第一个大于等于给定值的元素12345678910111213141516171819public int search4(int[] a, int data) &#123; int low = 0; int high = a.length - 1; while (low &lt;= high) &#123; int middle = low + ((high - low) &gt;&gt; 1); if (a[middle] &gt; data) &#123; high = middle - 1; &#125; else if (a[middle] &lt; data) &#123; low = middle + 1; &#125; else &#123; if (middle == a.length - 1 || a[middle + 1] != data) &#123; return middle; &#125; else &#123; low = middle + 1; &#125; &#125; &#125; return Integer.MIN_VALUE;&#125; 查找第一个大于等于给定值的元素1234567891011121314151617public int search(int[] a, int data) &#123; int low = 0; int high = a.length - 1; while (low &lt;= high) &#123; int middle = low + ((high - low) &gt;&gt; 1); if (a[middle] &gt;= data) &#123; if (middle == 0 || a[middle - 1] &lt; data) &#123; return middle; &#125; else &#123; high = middle - 1; &#125; &#125; else if (a[middle] &lt; data) &#123; low = middle + 1; &#125; &#125; return Integer.MIN_VALUE;&#125; 查找最后一个小于等于给定值的元素1234567891011121314151617public int search6(int[] a, int data) &#123; int low = 0; int high = a.length - 1; while (low &lt;= high) &#123; int middle = low + ((high - low) &gt;&gt; 1); if (a[middle] &gt; data) &#123; high = middle - 1; &#125; else if (a[middle] &lt;= data) &#123; if ((middle == a.length - 1) || a[middle + 1] &gt; data) &#123; return middle; &#125; else &#123; low = middle + 1; &#125; &#125; &#125; return Integer.MIN_VALUE;&#125;","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"17跳表：为什么Redis一定要用跳表来实现有序集合","slug":"aa_category/algorithm/数据结构与算法之美/17跳表：为什么Redis一定要用跳表来实现有序集合","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/17跳表：为什么Redis一定要用跳表来实现有序集合/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/17跳表：为什么Redis一定要用跳表来实现有序集合/","excerpt":"理解调表种链表加多级索引的结构，就是跳表","text":"理解调表种链表加多级索引的结构，就是跳表 用跳表查询到底有多快每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2，第二级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是 n/8，依次类推，也就是说，第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2，那第 k级索引结点的个数就是 n/(2^k) 设索引有 h 级，最高级的索引有 2 个结点。通过上面的公式，我们可以得到 n/(2h)=2，从而求得 h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是 log2n。我们在跳表中查询某个数据的时候，如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 O(m*logn) 跳表中查询任意数据的时间复杂度就是 O(logn) 跳表是不是很浪费内存n/2+n/4+n/8…+8+4+2=n-2 n/3+n/9+n/27+…+9+3+1=n/2","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"12排序：如何用快排思想在O(n)内查找第K大元素","slug":"aa_category/algorithm/数据结构与算法之美/12排序：如何用快排思想在O(n)内查找第K大元素","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/12排序：如何用快排思想在O(n)内查找第K大元素/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/12排序：如何用快排思想在O(n)内查找第K大元素/","excerpt":"归并排序（Merge Sort） 归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。","text":"归并排序（Merge Sort） 归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。 你可能已经发现了，merge(A[p…r], A[p…q], A[q+1…r]) 这个函数的作用就是，将已经有序的 A[p…q] 和 A[q+1…r] 合并成一个有序的数组，并且放入 A[p…r]。那这个过程具体该如何做呢？ 如图所示，我们申请一个临时数组 tmp，大小与 A[p…r] 相同。我们用两个游标 i 和 j，分别指向 A[p…q] 和 A[q+1…r] 的第一个元素。比较这两个元素 A[i] 和 A[j]，如果 A[i]&lt;=A[j]，我们就把 A[i] 放入到临时数组 tmp，并且 i 后移一位，否则将 A[j] 放入到数组 tmp，j 后移一位。 继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组 tmp 中的数据拷贝到原数组 A[p…r] 中 123456789101112131415161718192021222324252627282930313233public void mergeSort(int[] a) &#123; int[] temp = new int[a.length]; doSort(a, temp, 0, a.length - 1);&#125;public void doSort(int[] a, int[] t, int start, int end) &#123; if (start &lt; end) &#123; int middle = start + (end - start) / 2; doSort(a, t, start, middle); doSort(a, t, middle + 1, end); merge(a, t, start, middle, end); &#125;&#125;public void merge(int[] a, int[] t, int start, int middle, int end) &#123; System.arraycopy(a, start, t, start, end - start + 1); int i = start; int j = middle + 1; int k = start; while (i &lt;= middle &amp;&amp; j &lt;= end) &#123; if (t[i] &gt; t[j]) &#123; a[k++] = t[j++]; &#125; else &#123; a[k++] = t[i++]; &#125; &#125; while (i &lt;= middle) &#123; a[k++] = t[i++]; &#125; while (j &lt;= end) &#123; a[k++] = t[j++]; &#125;&#125; 归并排序的时间复杂度是多少1234567891011121314151617T(a) = T(b) + T(c) + KT(1) = C； n=1 时，只需要常量级的执行时间，所以表示为 C。T(n) = 2*T(n/2) + n； n&gt;1T(n) = 2*T(n/2) + n = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n ...... = 2^k * T(n/2^k) + k * n ...... T(n) = 2^k*T(n/2^k)+k*nT(n/2^k)=T(1) n/2^k=1 k=log2^n T(n)=C*n+n*log2^nT(n) =O(nlogn) 快速排序的原理快排的思想是这样的：如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点） 我们遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的 12345678910111213141516171819202122232425262728293031public void quickSort(int[] a) &#123; doSort(a, 0, a.length - 1); &#125; public void doSort(int[] a, int start, int end) &#123; if (start &lt; end) &#123; int partition = partition(a, start, end); doSort(a,start,partition); doSort(a, partition + 1, end); &#125; &#125; public int partition(int[] a, int p, int r) &#123; int key = a[p]; int start = p; int end = r; while (start &lt; end) &#123; while ( start &lt; end&amp;&amp;a[start] &lt;= key) &#123; start++; &#125; while ( start &lt; end&amp;&amp; a[end] &gt;= key) &#123; end--; &#125; if (start &lt; end) &#123; SortUtil.swap(a, start, end); &#125; &#125; SortUtil.swap(a, p, end); return end; &#125; 区别","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"学习指导手册","slug":"aa_category/algorithm/数据结构与算法之美/学习指导手册","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/学习指导手册/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/学习指导手册/","excerpt":"学习路线","text":"学习路线 复杂度分析 能自行分析专栏中大部分数据结构和算法的时间、空间复杂度 数组、栈、队列这一部分内容非常简单，初学者学起来也不会很难。但是，作为基础的数据结构，数组、栈、队列，是后续很多复杂数据结构和算法的基础，所以，这些内容你一定要掌握。 掌握程度：能自己实现动态数组、栈、队列 链表链表非常重要！虽然理论内容不多，但链表上的操作却很复杂。所以，面试中经常会考察，你一定要掌握。而且，我这里说“掌握”不只是能看懂专栏中的内容，还能将专栏中提到的经典链表题目，比如链表反转、求中间结点等，轻松无 bug 地实现出来 掌握程度：能轻松写出经典链表题目代码 递归对于初学者来说，递归代码非常难掌握，不管是读起来，还是写起来。但是，这道坎你必须要跨过，跨不过就不能算是入门数据结构和算法。 先在网上找些简单的题目练手，比如斐波那契数列、求阶乘等，然后再慢慢过渡到更加有难度的，比如归并排序、快速排序、二叉树的遍历、求高度，最后是回溯八皇后、背包问题等 掌握程度：轻松写出二叉树遍历、八皇后、背包问题、DFS 的递归代码 排序、二分查找掌握程度：能自己把各种排序算法、二分查找及其变体代码写一遍就可以了 跳表掌握程度：初学者可以先跳过。如果感兴趣，看懂专栏内容即可，不需要掌握代码实现 散列表这块内容理解起来并不难。但是，作为一种应用非常广泛的数据结构，你还是要掌握牢固散列表 掌握程度：对于初学者来说，自己能代码实现一个拉链法解决冲突的散列表即可 哈希算法这部分纯粹是为了开拓思路，初学者可以略过 二叉树这一部分非常重要！二叉树在面试中经常会被考到，所以要重点掌握。 红黑树对于初学者来说，这一节课完全可以不看 B+ 树虽然 B+ 树也算是比较高级的一种数据结构了，但是对初学者来说，也不是重点。有时候面试的时候还是会问的 堆与堆排序这一部分内容不是很难，初学者也是要掌握的 掌握程度：能代码实现堆、堆排序，并且掌握堆的三种应用（优先级队列、Top k、中位数） 图的表示图的内容很多，但是初学者不需要掌握那么多。一般 BAT 等大厂面试，不怎么会面试有关图的内容，因为面试官可能也对这块不会很熟悉哈：）。但是，最基本图的概念、表示方法还是要掌握的 掌握程度：理解图的三种表示方法（邻接矩阵、邻接表、逆邻接表），能自己代码实现 深度广度优先搜索这算是图上最基础的遍历或者说是搜索算法了，所以还是要掌握一下 掌握程度：能代码实现广度优先、深度优先搜索算法 拓扑排序、最短路径、A* 算法这几个算法稍微高级点。如果你能轻松实现深度、广度优先搜索，那看懂这三个算法不成问题 掌握程度：有时间再看，暂时可以不看 字符串匹配（BF、RK）BF 非常简单，RK 稍微复杂点，但都不难。这个最好还是掌握下 掌握程度：能实践 BF 算法，能看懂 RK 算法 字符串匹配（BM、KMP、AC 自动机）这三个算法都挺难的，对于算法有一定基础的人来说，看懂也不容易。所以，对于初学者来说，千万别浪费时间在这上面。即便有余力，看懂就好了，不用非得能自己实现 掌握程度：初学者不用把时间浪费在上面 字符串匹配（Trie）这个还是要能看懂，不过不需要能代码实现。有些面试官喜欢考这个东西，主要是结合应用场景来考察，只是看你知不知道要用 Trie 树这个东西。 掌握程度：能看懂，知道特点、应用场景即可，不要求代码实现 位图位图不是重点，如果有余力最好掌握一下 掌握程度：看懂即可，能自己实现一个位图结构最好 四种算法思想这个是重点，也是难点。贪心、分治、回溯、动态规划，每一个都不简单，其中动态规划又是最难、最烧脑的。要应付 FLAG 这样公司的面试，必须拿下这块内容。但是呢，学习要循序渐进，这块能内容的学习可以放到最后，做个长时间的学习计划来攻克 掌握程度：可以放到最后，但是一定要掌握！做到能实现 Leetcode 上 Medium 难度的题目","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"20散列表（下）：为什么散列表和链表经常会一起使用","slug":"aa_category/algorithm/数据结构与算法之美/20散列表：为什么散列表和链表经常会一起使用","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/20散列表：为什么散列表和链表经常会一起使用/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/20散列表：为什么散列表和链表经常会一起使用/","excerpt":"散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历","text":"散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历 因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"13线性排序：如何根据年龄给100万用户数据排序","slug":"aa_category/algorithm/数据结构与算法之美/13线性排序：如何根据年龄给100万用户数据排序","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/13线性排序：如何根据年龄给100万用户数据排序/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/13线性排序：如何根据年龄给100万用户数据排序/","excerpt":"桶排序（Bucket sort）首先，我们来看桶排序。桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了","text":"桶排序（Bucket sort）首先，我们来看桶排序。桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了 桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中 条件 要排序的数据需要很容易就能划分成 m 个桶 桶与桶之间有着天然的大小顺序 计数排序（Counting sort）假设只有 8 个考生，分数在 0 到 5 分之间。这 8 个考生的成绩我们放在一个数组 A[8] 中，它们分别是：2，5，3，0，2，3，0，3 考生的成绩从 0 到 5 分，我们使用大小为 6 的数组 C[6] 表示桶，其中下标对应分数。不过，C[6] 内存储的并不是考生，而是对应的考生个数。像我刚刚举的那个例子，我们只需要遍历一遍考生分数，就可以得到 C[6] 的值。 123456789101112131415161718192021222324252627282930313233343536/** * 统计最大的值 * 申请数组c[max+1] * 记录每个元素出现的次数 * 累计每个元素出现的次数 * 申请数组t[n] * 数组a复制给t * 计算 * * @param a */public void countSort(int[] a) &#123; int max = a[0]; for (int i = 1; i &lt; a.length; i++) &#123; if (a[i] &gt; max) &#123; max = a[i]; &#125; &#125; int[] c = new int[max + 1]; for (int i = 0; i &lt; a.length; i++) &#123; c[a[i]]++; &#125; for (int i = 1; i &lt; c.length; i++) &#123; c[i] = c[i] + c[i - 1]; &#125; int[] t = new int[a.length]; System.arraycopy(a, 0, t, 0, a.length); for (int i = t.length - 1; i &gt;= 0; i--) &#123; int aValue = t[i]; int index = c[aValue] - 1; a[index] = aValue; c[aValue]--; &#125;&#125; 基数排序（Radix sort）","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"18散列表：Word文档中的单词拼写检查功能是如何实现的","slug":"aa_category/algorithm/数据结构与算法之美/18散列表：Word文档中的单词拼写检查功能是如何实现的","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/18散列表：Word文档中的单词拼写检查功能是如何实现的/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/18散列表：Word文档中的单词拼写检查功能是如何实现的/","excerpt":"散列思想 散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表 散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。","text":"散列思想 散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表 散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。 散列函数 散列函数计算得到的散列值是一个非负整数； 如果 key1 = key2，那 hash(key1) == hash(key2) 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2) 散列冲突开放寻址法如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？ 线性探测（Linear Probing） 开放寻址法往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止 线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据 对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，二次探测（Quadratic probing）和双重散列（Double hashing） 链表法 当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。那查找或删除操作的时间复杂度是多少呢 这两个操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"10递归：如何用三行代码找到“最终推荐人”","slug":"aa_category/algorithm/数据结构与算法之美/10递归：如何用三行代码找到最终推荐人","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/10递归：如何用三行代码找到最终推荐人/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/10递归：如何用三行代码找到最终推荐人/","excerpt":"如何理解“递归”？1f(n)=f(n-1)+1 其中，f(1)=1","text":"如何理解“递归”？1f(n)=f(n-1)+1 其中，f(1)=1 递归需要满足的三个条件 一个问题的解可以分解为几个子问题的解 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 存在递归终止条件 如何编写递归代码 写出递推公式，找到终止条件 写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码 编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤 递归注意事项 递归代码要警惕堆栈溢出 递归代码要警惕重复计算 怎么将递归代码改写为非递归代码使用迭代循环改成非地柜的写法","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"19散列表：如何打造一个工业级水平的散列表","slug":"aa_category/algorithm/数据结构与算法之美/19散列表：如何打造一个工业级水平的散列表","date":"2019-01-24T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/24/aa_category/algorithm/数据结构与算法之美/19散列表：如何打造一个工业级水平的散列表/","link":"","permalink":"https://liyong.ac.cn/2019/01/24/aa_category/algorithm/数据结构与算法之美/19散列表：如何打造一个工业级水平的散列表/","excerpt":"如何设计散列函数 散列函数的设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接的影响到散列表的性能 散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突","text":"如何设计散列函数 散列函数的设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接的影响到散列表的性能 散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突 装载因子过大了怎么办 当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间 如何避免低效地扩容为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。 当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。 这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找 如何选择冲突解决方法开放寻址法当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散列冲突的原因 链表法开放寻址法只能适用装载因子小于 1 的情况。接近 1 时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多 链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对 CPU 缓存是不友好的 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。 工业级散列表举例分析 初始大小 装载因子和动态扩容 散列冲突解决方法 散列函数","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/categories/algorithm/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"algorithm","slug":"algorithm","permalink":"https://liyong.ac.cn/tags/algorithm/"}]},{"title":"redis数据类型","slug":"aa_category/middleware/redis/redis数据类型","date":"2019-01-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/01/aa_category/middleware/redis/redis数据类型/","link":"","permalink":"https://liyong.ac.cn/2019/01/01/aa_category/middleware/redis/redis数据类型/","excerpt":"string、hash、list、set、sorted set","text":"string、hash、list、set、sorted set string这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存 hash这个是类似 map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 redis 里，然后每次读写缓存的时候，可以就操作 hash 里的某个字段 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西 通过 lrange 命令实现分页查询，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西 set基于 set 玩儿交集、并集、差集的操作，比如交集吧，可以把两个人的粉丝列表整一个交集，看看俩人的共同好友是谁","categories":[{"name":"middleware","slug":"middleware","permalink":"https://liyong.ac.cn/categories/middleware/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://liyong.ac.cn/tags/redis/"},{"name":"middleware","slug":"middleware","permalink":"https://liyong.ac.cn/tags/middleware/"}]},{"title":"编码规范","slug":"aa_category/methodology/standard/编码规范","date":"2019-01-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/01/aa_category/methodology/standard/编码规范/","link":"","permalink":"https://liyong.ac.cn/2019/01/01/aa_category/methodology/standard/编码规范/","excerpt":"命名风格 不能以下划线和美元符号作为开始结束符号 禁止中英文形式 类名使用UpperCamelCase 方法名、参数名、成员变量、局部变量统一使用LowerCamelCase 常量大写，单词间下划线隔开，力求语义完成 抽象类用Abstract或者Base开头，异常类以Exception结尾，测试类以要测试类的名称开始，以Test结尾 中括号是数组定义的一部分 布尔类型的变量的命名不要添加is 包名使用小写，点分隔符之间只有一个英语单词。包名必须是单数形式，类名可以有复数形式( MessageUtils)。 避免不规范的缩写 为了达到代码自解释的目的，使用尽量完整的单词组合来命名 如果使用了设计模式，命名时体现具体模式","text":"命名风格 不能以下划线和美元符号作为开始结束符号 禁止中英文形式 类名使用UpperCamelCase 方法名、参数名、成员变量、局部变量统一使用LowerCamelCase 常量大写，单词间下划线隔开，力求语义完成 抽象类用Abstract或者Base开头，异常类以Exception结尾，测试类以要测试类的名称开始，以Test结尾 中括号是数组定义的一部分 布尔类型的变量的命名不要添加is 包名使用小写，点分隔符之间只有一个英语单词。包名必须是单数形式，类名可以有复数形式( MessageUtils)。 避免不规范的缩写 为了达到代码自解释的目的，使用尽量完整的单词组合来命名 如果使用了设计模式，命名时体现具体模式 常量定义 不允许任何魔法值 long、Long初始化赋值，使用大写L 不要使用一个常量维护所有的常量 常量的复用有5层，跨应用共享，应用内共享，子工程内共享，包内共享，类内共享（private static final） 如果变量值仅在一个范围内变化，且带有名称之外的延伸属性，定义为枚举 oop规约 避免通过类的对象引用静态属性和静态方法 重写方法添加@Override 相同参数类型，相同业务含义，才可以使用可变参数 外部正在调用，或者二方库依赖的接口，不允许修改方法签名。接口过时添加@Deprecated,并说明新接口 equals方法应该先写常量，避免空指针 对象之间值的比较使用equals 基本数据类型和包装类型 pojo类属性使用包装类型 RPC方法的出入参使用包装类型 局部变量使用基本数据类型 POJO类，不要设定任何属性默认值 构造愤怒规范禁止加入任何业务逻辑，如果有初始化逻辑，放置到init方法","categories":[{"name":"standard","slug":"standard","permalink":"https://liyong.ac.cn/categories/standard/"}],"tags":[{"name":"standard","slug":"standard","permalink":"https://liyong.ac.cn/tags/standard/"},{"name":"code","slug":"code","permalink":"https://liyong.ac.cn/tags/code/"}]},{"title":"学习的正确姿势","slug":"aa_category/methodology/学习的正确姿势","date":"2019-01-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/01/aa_category/methodology/学习的正确姿势/","link":"","permalink":"https://liyong.ac.cn/2019/01/01/aa_category/methodology/学习的正确姿势/","excerpt":"目标 linux jvm 线程池 juc nio、io 网络 缓存 redis 消息 搜索 javaee规范 设计模式 spring 规范","text":"目标 linux jvm 线程池 juc nio、io 网络 缓存 redis 消息 搜索 javaee规范 设计模式 spring 规范 通过百度、谷歌查看被人的博客梳理知识脉络 通过官网验证知识自己的理解的正确性 官网有助于理解知识","categories":[{"name":"methodology","slug":"methodology","permalink":"https://liyong.ac.cn/categories/methodology/"}],"tags":[{"name":"methodology","slug":"methodology","permalink":"https://liyong.ac.cn/tags/methodology/"}]},{"title":"数据库设计","slug":"aa_category/methodology/数据库设计","date":"2019-01-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/01/aa_category/methodology/数据库设计/","link":"","permalink":"https://liyong.ac.cn/2019/01/01/aa_category/methodology/数据库设计/","excerpt":"字段version用于乐观锁 status逻辑删除 0删除，1正常","text":"字段version用于乐观锁 status逻辑删除 0删除，1正常 索引服务端数据量比较大的添加索引 规则元数据-rule_metadata compent_type 组件类型，单选框，复选框，输入框，下拉框 value_type 值的类型 int boolean string date value relation 值之间的关系，|| &amp;&amp; 规则项-ai_active_rule_item code 编码 name 标签名称 weight 权重 compare_way 比较方式 &gt; &lt; &gt;= &lt;=","categories":[{"name":"methodology","slug":"methodology","permalink":"https://liyong.ac.cn/categories/methodology/"}],"tags":[{"name":"methodology","slug":"methodology","permalink":"https://liyong.ac.cn/tags/methodology/"}]},{"title":"git规范","slug":"aa_category/methodology/standard/git_standard","date":"2019-01-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/01/aa_category/methodology/standard/git_standard/","link":"","permalink":"https://liyong.ac.cn/2019/01/01/aa_category/methodology/standard/git_standard/","excerpt":"","text":"规范 学习gitflow思想，参考网址 https://www.git-tower.com/learn/git/ebook/cn/command-line/advanced-topics/git-flow https://blog.csdn.net/xingbaozhen1210/article/details/81386269 push之前一定要先拉取 push之前确定分支是否正确 本地分支和远程分支不要做merge feature-分支对应的代码上线以后需要删除 两天必须提交push一次代码到远程，push之前确保代码没有编译错误，程序可以正常启动 上线篇 上线完成以后需要记录线上的git version 如果发生线上bug ,从线上指定的git version 拉取 merge先往不重要的分支merge 有 feature-card master 两个分支，需要feature-card 合并到 master 首先确保两个分支都commit了 方法1，先把master合并到feature-card，解决冲突，然后再把feature-card合并到master 方法2 ，直接把feature-card合并到master解决冲突，如果发现合并后有问题 reset –hard(确保合并之前代码commit了) 到合并前的版本 本地解决了冲突了在push","categories":[{"name":"standard","slug":"standard","permalink":"https://liyong.ac.cn/categories/standard/"}],"tags":[{"name":"standard","slug":"standard","permalink":"https://liyong.ac.cn/tags/standard/"},{"name":"git","slug":"git","permalink":"https://liyong.ac.cn/tags/git/"}]},{"title":"redis过期策略","slug":"aa_category/middleware/redis/redis过期策略","date":"2019-01-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/01/aa_category/middleware/redis/redis过期策略/","link":"","permalink":"https://liyong.ac.cn/2019/01/01/aa_category/middleware/redis/redis过期策略/","excerpt":"定期删除+惰性删除","text":"定期删除+惰性删除 定期删除 redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除 惰性删除获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西 内存淘汰机制allkeys-lru当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的） volatile-lru当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适） volatile-ttl当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除","categories":[{"name":"middleware","slug":"middleware","permalink":"https://liyong.ac.cn/categories/middleware/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://liyong.ac.cn/tags/redis/"},{"name":"middleware","slug":"middleware","permalink":"https://liyong.ac.cn/tags/middleware/"}]},{"title":"源码阅读","slug":"aa_category/methodology/源码阅读","date":"2019-01-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/01/aa_category/methodology/源码阅读/","link":"","permalink":"https://liyong.ac.cn/2019/01/01/aa_category/methodology/源码阅读/","excerpt":"spring源码阅读知道有哪些功能只有知道有哪些功能才明白为什么那样写","text":"spring源码阅读知道有哪些功能只有知道有哪些功能才明白为什么那样写 看官方文档看完以后需要总结说明什么意思 猜怎么实现的带着问题看源码 验证自己的猜想","categories":[{"name":"standard","slug":"standard","permalink":"https://liyong.ac.cn/categories/standard/"}],"tags":[{"name":"standard","slug":"standard","permalink":"https://liyong.ac.cn/tags/standard/"}]},{"title":"系统设计","slug":"aa_category/methodology/系统设计","date":"2019-01-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/01/aa_category/methodology/系统设计/","link":"","permalink":"https://liyong.ac.cn/2019/01/01/aa_category/methodology/系统设计/","excerpt":"接口设计幂等一致性token 加缓存的解决方案 重试机制","text":"接口设计幂等一致性token 加缓存的解决方案 重试机制 日志跟踪traceId 异步保存保存到数据库 保存到es中 部分失败 错误码分为两部分 一部分说明成功/失败/部分失败 一部分说明每条数据的成功/失败 错误码 编码 说明 200 成功 201 部分领取成功 5000 第三方服务异常 系统设计","categories":[{"name":"standard","slug":"standard","permalink":"https://liyong.ac.cn/categories/standard/"}],"tags":[{"name":"standard","slug":"standard","permalink":"https://liyong.ac.cn/tags/standard/"}]},{"title":"设计一个高并发系统","slug":"aa_category/methodology/设计一个高并发系统","date":"2019-01-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/01/aa_category/methodology/设计一个高并发系统/","link":"","permalink":"https://liyong.ac.cn/2019/01/01/aa_category/methodology/设计一个高并发系统/","excerpt":"系统拆分 缓存 MQ 分库分表 读写分离 ElasticSearch","text":"系统拆分 缓存 MQ 分库分表 读写分离 ElasticSearch 系统拆分将一个系统拆分为多个子系统，用 dubbo 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么 缓存缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家 redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发 MQMQ，必须得用 MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。用 MQ 吧，大量的写请求灌入 MQ 里，排队慢慢玩儿，后边系统消费后慢慢写，控制在 mysql 承载范围之内 分库分表可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 sql 跑的性能。 读写分离这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库 ElasticSearch","categories":[{"name":"methodology","slug":"methodology","permalink":"https://liyong.ac.cn/categories/methodology/"}],"tags":[{"name":"methodology","slug":"methodology","permalink":"https://liyong.ac.cn/tags/methodology/"}]},{"title":"redis数据类型","slug":"aa_category/middleware/redis/index","date":"2019-01-01T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2019/01/01/aa_category/middleware/redis/index/","link":"","permalink":"https://liyong.ac.cn/2019/01/01/aa_category/middleware/redis/index/","excerpt":"","text":"Redis是单线程的，但Redis为什么这么快 Redis采用的是基于内存的采用的是单进程单线程模型的 KV 数据库，由C语言编写，官方提供的数据是可以达到100000+的QPS 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗 使用多路I/O复用模型，非阻塞IO 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求 redisObject123456789101112131415161718192021222324typedef struct redisObject &#123; /* Object types */#define REDIS_STRING 0#define REDIS_LIST 1#define REDIS_SET 2#define REDIS_ZSET 3#define REDIS_HASH 4unsigned type:4;#define REDIS_ENCODING_RAW 0 /* Raw representation */#define REDIS_ENCODING_INT 1 /* Encoded as integer */#define REDIS_ENCODING_HT 2 /* Encoded as hash table */#define REDIS_ENCODING_ZIPMAP 3 /* Encoded as zipmap */#define REDIS_ENCODING_LINKEDLIST 4 /* Encoded as regular linked list */#define REDIS_ENCODING_ZIPLIST 5 /* Encoded as ziplist */#define REDIS_ENCODING_INTSET 6 /* Encoded as intset */#define REDIS_ENCODING_SKIPLIST 7 /* Encoded as skiplist */unsigned encoding:4;#lru字段表示当内存超限时采用LRU算法清除内存中的对象unsigned lru:REDIS_LRU_BITS; */\\* lru time (relative to server.lruclock) \\*/*# refcount表示对象的引用计数int refcount;# ptr指针指向真正的存储结构void *ptr;&#125; robj; string:raw int Hash:ziplist hash_table list: linkedlist ziplist","categories":[{"name":"middleware","slug":"middleware","permalink":"https://liyong.ac.cn/categories/middleware/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://liyong.ac.cn/tags/redis/"},{"name":"middleware","slug":"middleware","permalink":"https://liyong.ac.cn/tags/middleware/"}]},{"title":"常用","slug":"aa_category/tool/maven/常用jar","date":"2018-10-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/10/12/aa_category/tool/maven/常用jar/","link":"","permalink":"https://liyong.ac.cn/2018/10/12/aa_category/tool/maven/常用jar/","excerpt":"","text":"Commons Codec编码解码 日志JCL全称为”Jakarta Commons Logging”，也可称为”Apache Commons Logging” log4j-taglibjsp打印日志需要 jcl-over-slf4jcommon-logging 实际调用slf4j slf4j-simpleslf4j的简单实现","categories":[{"name":"maven","slug":"maven","permalink":"https://liyong.ac.cn/categories/maven/"}],"tags":[{"name":"plugin","slug":"plugin","permalink":"https://liyong.ac.cn/tags/plugin/"},{"name":"maven","slug":"maven","permalink":"https://liyong.ac.cn/tags/maven/"}]},{"title":"maven插件","slug":"aa_category/tool/maven/plugin","date":"2018-10-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/10/12/aa_category/tool/maven/plugin/","link":"","permalink":"https://liyong.ac.cn/2018/10/12/aa_category/tool/maven/plugin/","excerpt":"maven-resources-plugin复制资源到输出目录","text":"maven-resources-plugin复制资源到输出目录","categories":[{"name":"maven","slug":"maven","permalink":"https://liyong.ac.cn/categories/maven/"}],"tags":[{"name":"plugin","slug":"plugin","permalink":"https://liyong.ac.cn/tags/plugin/"},{"name":"maven","slug":"maven","permalink":"https://liyong.ac.cn/tags/maven/"}]},{"title":"maven概念","slug":"aa_category/tool/maven/maven概念","date":"2018-10-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/10/12/aa_category/tool/maven/maven概念/","link":"","permalink":"https://liyong.ac.cn/2018/10/12/aa_category/tool/maven/maven概念/","excerpt":"继承http://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html","text":"继承http://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html 依赖传递性nearest definition A -&gt; B -&gt; C -&gt; D 2.0 and A -&gt; E -&gt; D 1.0, then D 1.0 层次同样深,使用先出现的 依赖范围compile 默认的作用域 具有传递性 provided Servlet API 作用范围为编译和测试 不具备传递性 runtime 作用范围运行时和编译 test 作用范围测试 不具备传递性 system 支持没有仓库中没有jar的场景 import 只能在dependencyManagement标签中,且type为pom依赖总使用 替换依赖文件中的内容 1234567&lt;dependency&gt; &lt;groupId&gt;javax.sql&lt;/groupId&gt; &lt;artifactId&gt;jdbc-stdext&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;java.home&#125;/lib/rt.jar&lt;/systemPath&gt; &lt;/dependency&gt; Importing Dependencies 解决多继承 具有递归 依赖的逻辑主键 groupId, artifactId, type, classifier http://maven.apache.org/pom.html Distribution Management生命周期默认 validate compile test package verify verify verify clean生命周期 pre-clean pre-clean post-clean site生命周期 pre-site site post-site post-site","categories":[{"name":"maven","slug":"maven","permalink":"https://liyong.ac.cn/categories/maven/"}],"tags":[{"name":"maven","slug":"maven","permalink":"https://liyong.ac.cn/tags/maven/"}]},{"title":"log4j2入门","slug":"aa_category/log/log4j2/log4j2入门","date":"2018-10-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/10/12/aa_category/log/log4j2/log4j2入门/","link":"","permalink":"https://liyong.ac.cn/2018/10/12/aa_category/log/log4j2/log4j2入门/","excerpt":"最终配置12345678910111213141516171819202122232425262728293031323334353637&lt;!--slf4j的配置--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.22&lt;/version&gt;&lt;/dependency&gt;&lt;!--适配器--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!--log4j 实现--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;","text":"最终配置12345678910111213141516171819202122232425262728293031323334353637&lt;!--slf4j的配置--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.22&lt;/version&gt;&lt;/dependency&gt;&lt;!--适配器--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!--log4j 实现--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 引入http://logging.apache.org/log4j/2.x/maven-artifacts.html#Using_Log4j_in_your_Apache_Maven_build 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.11.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.11.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 绑定SLF4Jhttp://logging.apache.org/log4j/2.x/log4j-slf4j-impl/index.html Due to a break in compatibility in the SLF4J binding, as of release 2.11.1 two SLF4J to Log4j Adapters are provided. log4j-slf4j-impl should be used with SLF4J 1.7.x releases or older. log4j-slf4j18-impl should be used with SLF4J 1.8.x releases or newer. 配置文件加载顺序Log4j has the ability to automatically configure itself during initialization. When Log4j starts it will locate all the ConfigurationFactory plugins and arrange them in weighted order from highest to lowest. As delivered, Log4j contains four ConfigurationFactory implementations: one for JSON, one for YAML, one for properties, and one for XML. Log4j will inspect the &quot;log4j.configurationFile&quot; system property and, if set, will attempt to load the configuration using the ConfigurationFactory that matches the file extension. If no system property is set the properties ConfigurationFactory will look for log4j2-test.properties in the classpath. If no such file is found the YAML ConfigurationFactory will look for log4j2-test.yaml or log4j2-test.yml in the classpath. If no such file is found the JSON ConfigurationFactory will look for log4j2-test.json or log4j2-test.jsn in the classpath. If no such file is found the XML ConfigurationFactory will look for log4j2-test.xml in the classpath. If a test file cannot be located the properties ConfigurationFactory will look for log4j2.properties on the classpath. If a properties file cannot be located the YAML ConfigurationFactory will look for log4j2.yaml or log4j2.yml on the classpath. If a YAML file cannot be located the JSON ConfigurationFactory will look for log4j2.json or log4j2.jsn on the classpath. If a JSON file cannot be located the XML ConfigurationFactory will try to locate log4j2.xml on the classpath. If no configuration file could be located the DefaultConfiguration will be used. This will cause logging output to go to the console. Pattern Layouthttp://logging.apache.org/log4j/2.x/manual/layouts.html#PatternLayout FiltersMarkerFilter12&lt;MarkerFilter marker=\"FLOW\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt;private static final Marker FLOW = MarkerFactory.getMarker(\"FLOW\"); ThresholdFilter1&lt;ThresholdFilter level=\"INFO\" onMatch=\"ACCEPT\" onMismatch=\"NEUTRAL\"/&gt; RegexFilter1&lt;RegexFilter regex=\".*LEE.*\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt; regex匹配%msg中的文本 .* 中的点","categories":[{"name":"log","slug":"log","permalink":"https://liyong.ac.cn/categories/log/"}],"tags":[{"name":"log4j2","slug":"log4j2","permalink":"https://liyong.ac.cn/tags/log4j2/"}]},{"title":"storm入门","slug":"aa_category/Bigdata&AI/storm/storm入门","date":"2018-08-31T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/08/31/aa_category/Bigdata&AI/storm/storm入门/","link":"","permalink":"https://liyong.ac.cn/2018/08/31/aa_category/Bigdata&AI/storm/storm入门/","excerpt":"并行度Workers在一个节点上可以运行一个或多个独立的JVM 进程","text":"并行度Workers在一个节点上可以运行一个或多个独立的JVM 进程 Executors在一个worker JVM进程中运行着多个Java线程 TasksTask就是具体的处理逻辑对象,每一个Spout和Bolt会被当作很多task在整个集群里面执行","categories":[{"name":"storm","slug":"storm","permalink":"https://liyong.ac.cn/categories/storm/"}],"tags":[{"name":"storm","slug":"storm","permalink":"https://liyong.ac.cn/tags/storm/"}]},{"title":"Resource","slug":"aa_category/se/Resource","date":"2018-08-31T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/08/31/aa_category/se/Resource/","link":"","permalink":"https://liyong.ac.cn/2018/08/31/aa_category/se/Resource/","excerpt":"","text":"URLUniform Resource Locatorhttp://www.ietf.org/rfc/rfc2396.txt http://www.example.com/docs/resource1.html 协议：//host:port/information relative URL不需要 协议 host port 编码不支持编码和解码 URIUniform Resource Identifier 语法[scheme:]scheme-specific-part[#fragment]Scheme Opaquemailto:java-net@java.sun.com hierarchical URI[scheme:][//authority][path][?query][#fragment] authority(server-based)[user-info@]host[:port] 和URL区别A URI is a uniform resource identifier while a URL is a uniform resource locator. Hence every URL is a URI, but not every URI is a URL.","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"url","slug":"url","permalink":"https://liyong.ac.cn/tags/url/"}]},{"title":"Log","slug":"Log","date":"2018-08-24T06:19:21.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/08/24/Log/","link":"","permalink":"https://liyong.ac.cn/2018/08/24/Log/","excerpt":"SLF4J","text":"SLF4J java.util.logginglogbacklog4j2Which JAR files do I needYou need at least the log4j-api-2.x and the log4j-core-2.x jar files. DefaultConfigurationLog4j will provide a default configuration if it cannot locate a configuration file. The default configuration, provided in the DefaultConfiguration class, will set up:","categories":[{"name":"log","slug":"log","permalink":"https://liyong.ac.cn/categories/log/"}],"tags":[{"name":"log","slug":"log","permalink":"https://liyong.ac.cn/tags/log/"}]},{"title":"Spring_MVC","slug":"Spring-MVC-0","date":"2018-05-16T06:59:53.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/05/16/Spring-MVC-0/","link":"","permalink":"https://liyong.ac.cn/2018/05/16/Spring-MVC-0/","excerpt":"","text":"InternalResourceViewResolver RequestContextUtils Special Bean TypesHandlerMappingRequestMappingHandlerMappingHandlerAdapterHandlerExceptionResolverViewResolverLocaleResolver, LocaleContextResolverThemeResolverMultipartResolverFlashMapManagermvc:annotation-driven@EnableWebMvcThe above registers a number of Spring MVC infrastructure beans also adapting to dependencies available on the classpath: e.g. payload converters for JSON, XML, etc. HttpRequestHandler相当于一个servlet SimpleUrlHandlerMapping根据url找到对应的bean /welcome.html=ticketController BeanNameUrlHandlerMapping根据url找到对应的beand的名称有映射关系 SimpleControllerHandlerAdapter","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"},{"name":"mvc","slug":"mvc","permalink":"https://liyong.ac.cn/tags/mvc/"}]},{"title":"读写","slug":"aa_category/other/读写分离","date":"2018-05-12T02:08:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/05/12/aa_category/other/读写分离/","link":"","permalink":"https://liyong.ac.cn/2018/05/12/aa_category/other/读写分离/","excerpt":"基本常识 Connection代表一个连接，所有对数据库的操作最终都要通过Connection进行 DataSource是池子，里面包含许多connection Transaction事务，通过持有的DataSource获取连接，然后通过连接的comit，rollback实现事务的概念","text":"基本常识 Connection代表一个连接，所有对数据库的操作最终都要通过Connection进行 DataSource是池子，里面包含许多connection Transaction事务，通过持有的DataSource获取连接，然后通过连接的comit，rollback实现事务的概念 设计思路有事务 在一个事务里面获取一个连接 通过ThreadLocal实现上下文信息传输 在创建事务的时候通过判断事务的读写特性，如果是readonly则在ThreadLocal中标记为read,否则writer 获取数据源的时候根据read/writer标记，获取对应的数据源，从而获取相应的连接 源码分析123456789101112131415161718192021222324TransactionAspectSupport#invokeWithinTransactionprotected Object invokeWithinTransaction(Method method, Class&lt;?&gt; targetClass, final InvocationCallback invocation) throws Throwable &#123; // 创建事务 TransactionInfo txInfo = createTransactionIfNecessary(tm, txAttr, joinpointIdentification); Object retVal = null; try &#123; //调用方法 retVal = invocation.proceedWithInvocation(); &#125; catch (Throwable ex) &#123; // 抛出异常回滚（看具体情况） completeTransactionAfterThrowing(txInfo, ex); throw ex; &#125; finally &#123; //清除事务上下文信息，事务信息本身没有清楚 cleanupTransactionInfo(txInfo); &#125; //提交 commitTransactionAfterReturning(txInfo); return retVal; &#125; 创建事务 TransactionAspectSupport#invokeWithinTransaction org.springframework.transaction.PlatformTransactionManager#getTransaction org.springframework.transaction.support.AbstractPlatformTransactionManager#doBegin DynamicDataSourceTransactionManager#doBegin（继承），给ThreadLocal中打上相应的标记 DataSourceTransactionManager#doBegin AbstractRoutingDataSource#getConnection() DynamicDataSource#determineCurrentLookupKey 根据上下文的标记获取相应的数据源，然后给出相应的连接 无事务通过方法注解 通过方法注解 通过aop的前置通知解析注解，给ThreadLocal中设置相应的read/writer标记 获取数据源的时候根据read/writer标记，获取对应的数据源，从而获取相应的连接 单条sql 通过mybatis的插件，给ThreadLocal中设置相应的read/writer标记 获取数据源的时候根据read/writer标记，获取对应的数据源，从而获取相应的连接 使用方式有事务 @Transactional(readOnly = true) 走从库 @Transactional() 走主库 无事务通过方法注解 通过方法注解 @MethodDataSource(DynamicDataSourceType.READ) 走从库 @MethodDataSource(DynamicDataSourceType.WRITE) 走主库 单条sql select走从库 insert/update/delete 走主库 涉及的类TransactionSynchronizationManager 通过ThreadLocal事务上下文信息 DataSourceTransactionManager spring事务管理器，包含提交回滚 SqlSessionFactoryBean 解决非springbean转换成springbean mybatis的所有配置都在这个里面完成 MapperScannerConfigurer 接口的实现类可以是proxy 扫描配置路径下的dao接口 通过接口找到对应的proxy 把代理注册成springbean PlatformTransactionManager spring事务管理器，包含提交回滚 DynamicDataSource 代理数据源，根据上下文找到被代理的数据源 1234@Overridepublic Connection getConnection() throws SQLException &#123; return determineTargetDataSource().getConnection();&#125; 总结 什么场景使用 使用以后带来的好处 使用以后带来的坏处 使用需要注意什么-（只有对读实时性要求不高的场景适合读写分离） ​","categories":[{"name":"other","slug":"other","permalink":"https://liyong.ac.cn/categories/other/"}],"tags":[{"name":"other","slug":"other","permalink":"https://liyong.ac.cn/tags/other/"}]},{"title":"系统设计","slug":"系统设计","date":"2018-05-04T12:41:59.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/05/04/系统设计/","link":"","permalink":"https://liyong.ac.cn/2018/05/04/系统设计/","excerpt":"确认需求用自己的术语和产品经理确认一次，由于每个人的语文水平不同，同样的话术，会有不同的理解","text":"确认需求用自己的术语和产品经理确认一次，由于每个人的语文水平不同，同样的话术，会有不同的理解 分析需求表面需求内在需求分析资源 人 时间点 依赖资源 范围功能性需求非功能性需求 易用性 侵入型 可用性 性能 技术选型已有轮子发明轮子设计实现架构设计详细设计","categories":[{"name":"方法论","slug":"方法论","permalink":"https://liyong.ac.cn/categories/方法论/"}],"tags":[{"name":"方法论","slug":"方法论","permalink":"https://liyong.ac.cn/tags/方法论/"}]},{"title":"vue","slug":"vue","date":"2018-05-03T05:33:54.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/05/03/vue/","link":"","permalink":"https://liyong.ac.cn/2018/05/03/vue/","excerpt":"","text":"Vuex 是一个专为 Vue.js 应用程序开发的状态管理模式。它采用集中式存储管理应用的所有组件的状态，并以相应的规则保证状态以一种可预测的方式发生变化 main.js作为顶级入口 importimport HelloWorld from ‘./components/HelloWorld.vue’import Vue from ‘vue’; Vue.use()通过全局方法 Vue.use() 使用插件 vue-clivue-cli 是一个官方发布 vue.js 项目脚手架，使用 vue-cli 可以快速创建 vue 项目 :class根据值进行动态绑定 # 布局layout2.vue 菜单layout 根据 name找到 routers中的 namepath 浏览器中显示的路径component 请求的组件 webpackwebpack dev server npm i cross envnpm i html-webpack-plugin node核心模块文件模块第三方模块 模块的流程创建模块导出模块加载模块使用模块 1var path = require(\"path\") app-&gt;layout vue-resourceWebpack dev servervue-resourcehttp post","categories":[],"tags":[]},{"title":"Tree","slug":"Tree","date":"2018-04-21T05:34:38.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/04/21/Tree/","link":"","permalink":"https://liyong.ac.cn/2018/04/21/Tree/","excerpt":"二叉查（Binary Tree）A Binary Tree is one of the most typical tree structure. As the name suggests, a binary tree is a tree data structure in which each node has at most two children, which are referred to as the left child and the right child","text":"二叉查（Binary Tree）A Binary Tree is one of the most typical tree structure. As the name suggests, a binary tree is a tree data structure in which each node has at most two children, which are referred to as the left child and the right child B树(B-tree)B树和平衡二叉树稍有不同的是B树属于多叉树又名平衡多路查找树 规则 树种的每个节点最多拥有m个子节点且m&gt;=2,空树除外（注：m阶代表一个树节点最多有多少个查找路径，m阶=m路,当m=2则是2叉树,m=3则是3叉） 除根节点外每个节点的关键字数量大于等于ceil(m/2)-1个小于等于m-1个，非根节点关键字数必须&gt;=2;（注：ceil()是个朝正无穷方向取整的函数 如ceil(1.1)结果为2)B+树B+跟B树不同B+树的非叶子节点不保存关键字记录的指针，这样使得B+树每个节点所能保存的关键字大大增加","categories":[],"tags":[]},{"title":"MySQL优化原理","slug":"MySQL优化原理","date":"2018-04-21T04:36:11.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/04/21/MySQL优化原理/","link":"","permalink":"https://liyong.ac.cn/2018/04/21/MySQL优化原理/","excerpt":"逻辑架构图MySQL逻辑架构整体分为三层","text":"逻辑架构图MySQL逻辑架构整体分为三层 第一层并非MYSQL独有，诸如连接处理、授权认证、安全等在这一层 第二层查询解析，分析，优化，缓存，内置函数。跨存储引擎的功能也在这一层：存储过程、触发器 第三层存储引擎，负责MYSQL中数据的存储和提取 MySQL查询过程 客户端/服务端通信协议 MySQL客户端/服务端通信协议是“半双工”的：在任一时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常 与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送 查询缓存 解析查询语句之前判断缓存是否打开，如果打开，那么判断当前sql是否命中缓存，如果命中在检查一次用户权限以后直接返回查询结果 MySQL将缓存存放在一个引用表(HashMap)，通过查询语句+数据库+客户端版本号计算出索引，通过索引来定位是否命中，所以两次查询有任何不同（空格、注释）都会导致缓存没有命中 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL库中的系统表，其查询结果都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果， 既然是缓存，就会失效，那查询缓存何时失效呢？MySQL的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL必须将对应表的所有缓存都设置为失效。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外：任何的查询语句在开始之前都必须经过检查，即使这条SQL语句永远不会命中缓存如果查询结果可以被缓存，那么执行完成后，会将结果存入缓存，也会带来额外的系统消耗 基于上面的结论，我们要知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如： 用多个小表代替一个大表，注意不要过度设计 批量插入代替循环单条插入 合理控制缓存空间大小，一般来说其大小设置为几十兆比较合适 可以通过SQL_CACHE和SQL_NO_CACHE来控制某个查询语句是否需要进行缓存最后的忠告是不要轻易打开查询缓存，特别是写密集型应用。如果你实在是忍不住，可以将query_cache_type设置为DEMAND，这时只有加入SQL_CACHE的查询才会走缓存，其他查询则不会，这样可以非常自由地控制哪些查询需要被缓存。 语法解析和预处理MySQL通过关键字将SQL语句进行解析，并生成一颗对应的解析树。这个过程解析器主要通过语法规则来验证和解析。比如SQL中是否使用了错误的关键字或者关键字的顺序是否正确等等 查询优化MySQL使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在MySQL可以通过查询当前会话的last_query_cost的值来得到其计算当前查询的成本。有非常多的原因会导致MySQL选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但MySQL值选择它认为成本小的，但成本小并不意味着执行时间短）等等 查询执行引擎查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler API。查询过程中的每一张表由一个handler实例表示。实际上，MySQL在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等 返回结果给客户端查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等结果集返回客户端是一个增量且逐步返回的过程。有可能MySQL在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果 性能优化建议Scheme设计与数据类型优化选择数据类型只要遵循小而简单的原则就好，越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的CPU周期也更少。越简单的数据类型在计算时需要更少的CPU周期比如，整型就比字符操作代价低，因而会使用整型来存储ip地址，使用DATETIME来存储时间，而不是使用字符串 这里总结几个可能容易理解错误的技巧 通常来说把可为NULL的列改为NOT NULL不会对性能提升有多少帮助，只是如果计划在列上创建索引，就应该将该列设置为NOT NULL 对整数类型指定宽度，比如INT(11)，没有任何卵用。INT使用32位（4个字节）存储空间，那么它的表示范围已经确定，所以INT(1)和INT(20)对于存储和计算是相同的 UNSIGNED表示不允许负值，大致可以使正数的上限提高一倍。比如TINYINT存储范围是-128 ~ 127，而UNSIGNED TINYINT存储的范围却是0 - 255 TIMESTAMP使用4个字节存储空间，DATETIME使用8个字节存储空间。因而，TIMESTAMP只能表示1970 - 2038年，比DATETIME表示的范围小得多，而且TIMESTAMP的值因时区不同而不同。 schema的列不要太多。原因是存储引擎的API工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码成各个列，这个转换过程的代价是非常高的。 大表ALTER TABLE非常耗时，MySQL执行大部分修改表结果操作的方法是用新的结构创建一个张空表，从旧表中查出所有的数据插入新表，然后再删除旧表。尤其当内存不足而表又很大 高性能策略非独立的列MySQL不会使用索引的情况：非独立的列 ，“独立的列”是指索引列不能是表达式的一部分，也不能是函数的参数。比如： 1select * from where id + 1 = 5 前缀索引前缀索引仅仅是选择该列的部分字符作为索引，减少索引的字符可以节约索引空间，从而提高索引效率，但这样也会降低索引的选择性 多列索引和索引顺序在多数情况下，在多个列上建立独立的索引并不能提高查询性能。。理由非常简单，MySQL不知道选择哪个索引的查询效率更好，所以在老版本，比如MySQL5.0之前就会随便选择一个列的索引，而新的版本会采用合并索引的策略。举个简单的例子，在一张电影演员表中，在actor_id和film_id两个列上都建立了独立的索引，然后有如下查询： 1select film_id,actor_id from film_actor where actor_id = 1 or film_id = 1 老版本的MySQL会随机选择一个索引，但新版本做如下的优化： 123select film_id,actor_id from film_actor where actor_id = 1 union allselect film_id,actor_id from film_actor where film_id = 1 and actor_id &lt;&gt; 1 当出现多个索引做相交操作时（多个AND条件），通常来说一个包含所有相关列的索引要优于多个独立索引 当出现多个索引做联合操作时（多个OR条件），对结果集的合并、排序等操作需要耗费大量的CPU和内存资源，特别是当其中的某些索引的选择性不高，需要返回合并大量数据时，查询成本更高。所以这种情况下还不如走全表扫描 索引的顺序对于查询是至关重要的，很明显应该把选择性更高的字段放到索引的前面，这样通过第一个字段就可以过滤掉大多数不符合条件的数据。 索引选择性是指不重复的索引值和数据表的总记录数的比值，选择性越高查询效率越高，因为选择性越高的索引可以让MySQL在查询时过滤掉更多的行。唯一索引的选择性是1，这时最好的索引选择性，性能也是最好的。 避免多个范围条件 1select film_id,actor_id from film_actor where actor_id = 1 or film_id = 1 这个查询有一个问题：它有两个范围条件，login_time列和age列，MySQL可以使用login_time列的索引或者age列的索引，但无法同时使用它们。 覆盖索引 索引条目远小于数据行大小，如果只读取索引，极大减少数据访问 索引是有按照列值顺序存储的，对于I/O密集型的范围查询要比随机从磁盘读取每一行数据的IO要少的多 使用索引扫描来排序只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向也一样时，才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有ORDER BY子句引用的字段全部为第一张表时，才能使用索引做排序。ORDER BY子句和查询的限制是一样的，都要满足最左前缀的要求（有一种情况例外，就是最左的列被指定为常数，下面是一个简单的示例），其它情况下都需要执行排序操作，而无法利用索引排序。 冗余和重复索引冗余索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应当尽量避免这种索引，发现后立即删除。比如有一个索引(A,B)，再创建索引(A)就是冗余索引。 删除长期未使用的索引 特定类型查询优化优化COUNT()查询COUNT()可能是被大家误解最多的函数了，它有两种不同的作用，其一是统计某个列值的数量，其二是统计行数。统计列值时，要求列值是非空的，它不会统计NULL。如果确认括号中的表达式不可能为空时,实际上就是在统计行数。最简单的就是当使用COUNT(*)时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计所有的行数 优化关联查询 确保ON和USING字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表A和表B用列c关联的时候，如果优化器关联的顺序是A、B，那么就不需要在A表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引 确保任何的GROUP BY和ORDER BY中的表达式只涉及到一个表中的列，这样MySQL才有可能使用索引来优化 要理解优化关联查询的第一个技巧，就需要理解MySQL是如何执行关联查询的。当前MySQL关联执行的策略非常简单，它对任何的关联都执行嵌套循环关联操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列 123SELECT A.xx,B.yyFROM A INNER JOIN B USING(c)WHERE A.xx IN (5,6) 假设MySQL按照查询中的关联顺序A、B来进行关联操作，那么可以用下面的伪代码表示MySQL如何完成这个查询 123456789101112131415161718192021outer_iterator = SELECT A.xx,A.c FROM A WHERE A.xx IN (5,6);outer_row = outer_iterator.next;while(outer_row) &#123; inner_iterator = SELECT B.yy FROM B WHERE B.c = outer_row.c; inner_row = inner_iterator.next; while(inner_row) &#123; output[inner_row.yy,outer_row.xx]; inner_row = inner_iterator.next; &#125; outer_row = outer_iterator.next;&#125; 可以看到，最外层的查询是根据A.xx列来查询的，A.c上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显B.c上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。 优化LIMIT分页一个常见的问题是当偏移量非常大的时候，比如：LIMIT 10000 20这样的查询，MySQL需要查询10020条记录然后只返回20条记录，前面的10000条都将被抛弃，这样的代价非常高优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列如果这张表非常大，那么这个查询最好改成下面的样子： 1234SELECT film.film_id,film.descriptionFROM film INNER JOIN ( SELECT film_id FROM film ORDER BY title LIMIT 50,5) AS tmp USING(film_id); 优化UNIONMySQL处理UNION的策略是先创建临时表，然后再把各个查询结果插入到临时表中，最后再来做查询。因此很多优化策略在UNION查询中都没有办法很好的时候,经常需要手动将WHERE、LIMIT、ORDER BY等字句“下推”到各个子查询中，以便优化器可以充分利用这些条件先优化。除非确实需要服务器去重，否则就一定要使用UNION ALL，如果没有ALL关键字，MySQL会给临时表加上DISTINCT选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"temp","slug":"temp","date":"2018-04-20T01:11:51.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/04/20/temp/","link":"","permalink":"https://liyong.ac.cn/2018/04/20/temp/","excerpt":"","text":"分库分表Sharding-JDBChttps://juejin.im/entry/5ad91207f265da0b9347d87e?utm_source=gold_browser_extension reidshttps://juejin.im/post/5ad6e4066fb9a028d82c4b66?utm_source=gold_browser_extension mysqlhttps://juejin.im/entry/5ad6e5cd6fb9a028cc61c136?utm_source=gold_browser_extension","categories":[],"tags":[]},{"title":"jvm-options","slug":"jvm-options","date":"2018-04-02T01:09:03.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/04/02/jvm-options/","link":"","permalink":"https://liyong.ac.cn/2018/04/02/jvm-options/","excerpt":"","text":"http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html","categories":[],"tags":[]},{"title":"javap-指令集","slug":"aa_category/se/javap-指令集","date":"2018-04-02T01:08:00.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/04/02/aa_category/se/javap-指令集/","link":"","permalink":"https://liyong.ac.cn/2018/04/02/aa_category/se/javap-指令集/","excerpt":"栈和局部变量操作将常量压入栈的指令aconst_null 将null对象引用压入栈iconst_m1 将int类型常量-1压入栈iconst_0 将int类型常量0压入栈iconst_1 将int类型常量1压入栈iconst_2 将int类型常量2压入栈iconst_3 将int类型常量3压入栈iconst_4 将int类型常量4压入栈iconst_5 将int类型常量5压入栈lconst_0 将long类型常量0压入栈lconst_1 将long类型常量1压入栈fconst_0 将float类型常量0压入栈fconst_1 将float类型常量1压入栈dconst_0 将double类型常量0压入栈dconst_1 将double类型常量1压入栈bipush 将一个8位带符号整数压入栈sipush 将16位带符号整数压入栈ldc 把常量池中的项压入栈ldc_w 把常量池中的项压入栈（使用宽索引）ldc2_w 把常量池中long类型或者double类型的项压入栈（使用宽索引）","text":"栈和局部变量操作将常量压入栈的指令aconst_null 将null对象引用压入栈iconst_m1 将int类型常量-1压入栈iconst_0 将int类型常量0压入栈iconst_1 将int类型常量1压入栈iconst_2 将int类型常量2压入栈iconst_3 将int类型常量3压入栈iconst_4 将int类型常量4压入栈iconst_5 将int类型常量5压入栈lconst_0 将long类型常量0压入栈lconst_1 将long类型常量1压入栈fconst_0 将float类型常量0压入栈fconst_1 将float类型常量1压入栈dconst_0 将double类型常量0压入栈dconst_1 将double类型常量1压入栈bipush 将一个8位带符号整数压入栈sipush 将16位带符号整数压入栈ldc 把常量池中的项压入栈ldc_w 把常量池中的项压入栈（使用宽索引）ldc2_w 把常量池中long类型或者double类型的项压入栈（使用宽索引） 从栈中的局部变量中装载值的指令iload 从局部变量中装载int类型值lload 从局部变量中装载long类型值fload 从局部变量中装载float类型值dload 从局部变量中装载double类型值aload 从局部变量中装载引用类型值（refernce）iload_0 从局部变量0中装载int类型值iload_1 从局部变量1中装载int类型值iload_2 从局部变量2中装载int类型值iload_3 从局部变量3中装载int类型值lload_0 从局部变量0中装载long类型值lload_1 从局部变量1中装载long类型值lload_2 从局部变量2中装载long类型值lload_3 从局部变量3中装载long类型值fload_0 从局部变量0中装载float类型值fload_1 从局部变量1中装载float类型值fload_2 从局部变量2中装载float类型值fload_3 从局部变量3中装载float类型值dload_0 从局部变量0中装载double类型值dload_1 从局部变量1中装载double类型值dload_2 从局部变量2中装载double类型值dload_3 从局部变量3中装载double类型值aload_0 从局部变量0中装载引用类型值aload_1 从局部变量1中装载引用类型值aload_2 从局部变量2中装载引用类型值aload_3 从局部变量3中装载引用类型值iaload 从数组中装载int类型值laload 从数组中装载long类型值faload 从数组中装载float类型值daload 从数组中装载double类型值aaload 从数组中装载引用类型值baload 从数组中装载byte类型或boolean类型值caload 从数组中装载char类型值saload 从数组中装载short类型值 将栈中的值存入局部变量的指令istore 将int类型值存入局部变量lstore 将long类型值存入局部变量fstore 将float类型值存入局部变量dstore 将double类型值存入局部变量astore 将将引用类型或returnAddress类型值存入局部变量istore_0 将int类型值存入局部变量0istore_1 将int类型值存入局部变量1istore_2 将int类型值存入局部变量2istore_3 将int类型值存入局部变量3lstore_0 将long类型值存入局部变量0lstore_1 将long类型值存入局部变量1lstore_2 将long类型值存入局部变量2lstore_3 将long类型值存入局部变量3fstore_0 将float类型值存入局部变量0fstore_1 将float类型值存入局部变量1fstore_2 将float类型值存入局部变量2fstore_3 将float类型值存入局部变量3dstore_0 将double类型值存入局部变量0dstore_1 将double类型值存入局部变量1dstore_2 将double类型值存入局部变量2dstore_3 将double类型值存入局部变量3astore_0 将引用类型或returnAddress类型值存入局部变量0astore_1 将引用类型或returnAddress类型值存入局部变量1astore_2 将引用类型或returnAddress类型值存入局部变量2astore_3 将引用类型或returnAddress类型值存入局部变量3iastore 将int类型值存入数组中lastore 将long类型值存入数组中fastore 将float类型值存入数组中dastore 将double类型值存入数组中aastore 将引用类型值存入数组中bastore 将byte类型或者boolean类型值存入数组中castore 将char类型值存入数组中sastore 将short类型值存入数组中 wide指令wide 使用附加字节扩展局部变量索引 通用(无类型）栈操作nop 不做任何操作pop 弹出栈顶端一个字长的内容pop2 弹出栈顶端两个字长的内容dup 复制栈顶部一个字长内容dup_x1 复制栈顶部一个字长的内容，然后将复制内容及原来弹出的两个字长的内容压入栈dup_x2 复制栈顶部一个字长的内容，然后将复制内容及原来弹出的三个字长的内容压入栈dup2 复制栈顶部两个字长内容dup2_x1 复制栈顶部两个字长的内容，然后将复制内容及原来弹出的三个字长的内容压入栈dup2_x2 复制栈顶部两个字长的内容，然后将复制内容及原来弹出的四个字长的内容压入栈swap 交换栈顶部两个字长内容 类型转换i2l 把int类型的数据转化为long类型i2f 把int类型的数据转化为float类型i2d 把int类型的数据转化为double类型l2i 把long类型的数据转化为int类型l2f 把long类型的数据转化为float类型l2d 把long类型的数据转化为double类型f2i 把float类型的数据转化为int类型f2l 把float类型的数据转化为long类型f2d 把float类型的数据转化为double类型d2i 把double类型的数据转化为int类型d2l 把double类型的数据转化为long类型d2f 把double类型的数据转化为float类型i2b 把int类型的数据转化为byte类型i2c 把int类型的数据转化为char类型i2s 把int类型的数据转化为short类型 整数运算iadd 执行int类型的加法ladd 执行long类型的加法isub 执行int类型的减法lsub 执行long类型的减法imul 执行int类型的乘法lmul 执行long类型的乘法idiv 执行int类型的除法ldiv 执行long类型的除法irem 计算int类型除法的余数lrem 计算long类型除法的余数ineg 对一个int类型值进行取反操作lneg 对一个long类型值进行取反操作iinc 把一个常量值加到一个int类型的局部变量上 逻辑运算移位操作ishl 执行int类型的向左移位操作lshl 执行long类型的向左移位操作ishr 执行int类型的向右移位操作lshr 执行long类型的向右移位操作iushr 执行int类型的向右逻辑移位操作lushr 执行long类型的向右逻辑移位操作 按位布尔运算iand 对int类型值进行“逻辑与”操作land 对long类型值进行“逻辑与”操作ior 对int类型值进行“逻辑或”操作lor 对long类型值进行“逻辑或”操作ixor 对int类型值进行“逻辑异或”操作lxor 对long类型值进行“逻辑异或”操作 浮点运算fadd 执行float类型的加法dadd 执行double类型的加法fsub 执行float类型的减法dsub 执行double类型的减法fmul 执行float类型的乘法dmul 执行double类型的乘法fdiv 执行float类型的除法ddiv 执行double类型的除法frem 计算float类型除法的余数drem 计算double类型除法的余数fneg 将一个float类型的数值取反dneg 将一个double类型的数值取反 对象和数组对象操作指令new 创建一个新对象checkcast 确定对象为所给定的类型getfield 从对象中获取字段putfield 设置对象中字段的值getstatic 从类中获取静态字段putstatic 设置类中静态字段的值instanceof 判断对象是否为给定的类型 数组操作指令newarray 分配数据成员类型为基本上数据类型的新数组anewarray 分配数据成员类型为引用类型的新数组arraylength 获取数组长度multianewarray 分配新的多维数组 控制流条件分支指令ifeq 如果等于0，则跳转ifne 如果不等于0，则跳转iflt 如果小于0，则跳转ifge 如果大于等于0，则跳转ifgt 如果大于0，则跳转ifle 如果小于等于0，则跳转if_icmpcq 如果两个int值相等，则跳转if_icmpne 如果两个int类型值不相等，则跳转if_icmplt 如果一个int类型值小于另外一个int类型值，则跳转if_icmpge 如果一个int类型值大于或者等于另外一个int类型值，则跳转if_icmpgt 如果一个int类型值大于另外一个int类型值，则跳转if_icmple 如果一个int类型值小于或者等于另外一个int类型值，则跳转ifnull 如果等于null，则跳转ifnonnull 如果不等于null，则跳转if_acmpeq 如果两个对象引用相等，则跳转if_acmpnc 如果两个对象引用不相等，则跳转 比较指令lcmp 比较long类型值fcmpl 比较float类型值（当遇到NaN时，返回-1）fcmpg 比较float类型值（当遇到NaN时，返回1）dcmpl 比较double类型值（当遇到NaN时，返回-1）dcmpg 比较double类型值（当遇到NaN时，返回1） 无条件转移指令goto 无条件跳转goto_w 无条件跳转（宽索引） 表跳转指令tableswitch 通过索引访问跳转表，并跳转lookupswitch 通过键值匹配访问跳转表，并执行跳转操作 异常athrow 抛出异常或错误finally子句jsr 跳转到子例程jsr_w 跳转到子例程（宽索引）rct 从子例程返回 方法调用与返回方法调用指令invokcvirtual 运行时按照对象的类来调用实例方法invokespecial 根据编译时类型来调用实例方法invokestatic 调用类（静态）方法invokcinterface 调用接口方法 方法返回指令ireturn 从方法中返回int类型的数据lreturn 从方法中返回long类型的数据freturn 从方法中返回float类型的数据dreturn 从方法中返回double类型的数据areturn 从方法中返回引用类型的数据return 从方法中返回，返回值为void 线程同步montiorenter 进入并获取对象监视器monitorexit 释放并退出对象监视器 JVM指令助记符 变量到操作数栈：iload,iload_,lload,lload_,fload,fload_,dload,dload_,aload,aload_ 操作数栈到变量：istore,istore_,lstore,lstore_,fstore,fstore_,dstore,dstor_,astore,astore_ 常数到操作数栈：bipush,sipush,ldc,ldc_w,ldc2_w,aconst_null,iconst_ml,iconst_,lconst_,fconst_,dconst_ 加：iadd,ladd,fadd,dadd 减：isub,lsub,fsub,dsub 乘：imul,lmul,fmul,dmul 除：idiv,ldiv,fdiv,ddiv 余数：irem,lrem,frem,drem 取负：ineg,lneg,fneg,dneg 移位：ishl,lshr,iushr,lshl,lshr,lushr 按位或：ior,lor 按位与：iand,land 按位异或：ixor,lxor 类型转换：i2l,i2f,i2d,l2f,l2d,f2d(放宽数值转换)i2b,i2c,i2s,l2i,f2i,f2l,d2i,d2l,d2f(缩窄数值转换) 创建类实便：new 创建新数组：newarray,anewarray,multianwarray 访问类的域和类实例域：getfield,putfield,getstatic,putstatic 把数据装载到操作数栈：baload,caload,saload,iaload,laload,faload,daload,aaload 从操作数栈存存储到数组：bastore,castore,sastore,iastore,lastore,fastore,dastore,aastore 获取数组长度：arraylength检相类实例或数组属性：instanceof,checkcast 操作数栈管理：pop,pop2,dup,dup2,dup_xl,dup2_xl,dup_x2,dup2_x2,swap 有条件转移：ifeq,iflt,ifle,ifne,ifgt,ifge,ifnull,ifnonnull,if_icmpeq,if_icmpene,if_icmplt,if_icmpgt,if_icmple,if_icmpge,if_acmpeq,if_acmpne,lcmp,fcmplfcmpg,dcmpl,dcmpg 复合条件转移：tableswitch,lookupswitch 无条件转移：goto,goto_w,jsr,jsr_w,ret 调度对象的实便方法：invokevirtual 调用由接口实现的方法：invokeinterface 调用需要特殊处理的实例方法：invokespecial 调用命名类中的静态方法：invokestatic 方法返回：ireturn,lreturn,freturn,dreturn,areturn,return 异常：athrowfinally关键字的实现使用：jsr,jsr_w,ret","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"虚拟机","slug":"虚拟机","permalink":"https://liyong.ac.cn/tags/虚拟机/"},{"name":"指令","slug":"指令","permalink":"https://liyong.ac.cn/tags/指令/"}]},{"title":"mysql-explain","slug":"mysql-explain","date":"2018-03-30T11:03:45.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/03/30/mysql-explain/","link":"","permalink":"https://liyong.ac.cn/2018/03/30/mysql-explain/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"PPT","slug":"PPT","date":"2018-03-30T03:05:08.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/03/30/PPT/","link":"","permalink":"https://liyong.ac.cn/2018/03/30/PPT/","excerpt":"","text":"ECharts百度识图如果有一张模糊的图片找到清晰的图片 袋鼠输入(百度)手机遥控电脑工具 腾讯智图无损压缩图片 canvas图片设计平台http://canvas.qq.com/index iconfont阿里巴巴出品的矢量图标库http://iconfont.cn/home/index 百度图说http://tushuo.baidu.com/","categories":[],"tags":[]},{"title":"Object","slug":"aa_category/se/source/Object-源码分析","date":"2018-03-23T00:49:10.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/03/23/aa_category/se/source/Object-源码分析/","link":"","permalink":"https://liyong.ac.cn/2018/03/23/aa_category/se/source/Object-源码分析/","excerpt":"registerNativesHere’s the associated C code (from OpenJDK 6): static JNINativeMethod methods[] = { {“hashCode”, “()I”, (void *)&amp;JVM_IHashCode}, {“wait”, “(J)V”, (void *)&amp;JVM_MonitorWait}, {“notify”, “()V”, (void *)&amp;JVM_MonitorNotify}, {“notifyAll”, “()V”, (void *)&amp;JVM_MonitorNotifyAll}, {“clone”, “()Ljava/lang/Object;”, (void *)&amp;JVM_Clone},};","text":"registerNativesHere’s the associated C code (from OpenJDK 6): static JNINativeMethod methods[] = { {“hashCode”, “()I”, (void *)&amp;JVM_IHashCode}, {“wait”, “(J)V”, (void *)&amp;JVM_MonitorWait}, {“notify”, “()V”, (void *)&amp;JVM_MonitorNotify}, {“notifyAll”, “()V”, (void *)&amp;JVM_MonitorNotifyAll}, {“clone”, “()Ljava/lang/Object;”, (void *)&amp;JVM_Clone},}; JNIEXPORT void JNICALLJava_java_lang_Object_registerNatives(JNIEnv env, jclass cls){ (env)-&gt;RegisterNatives(env, cls, methods, sizeof(methods)/sizeof(methods[0]));} getClasshashCodeconsistentequals相同hashcode必须相同unequal，则hashcode可以相同，但是最好不同 equalsreflexivesymmetrictransitiveconsistent toString文本的形式，表达这个对象，应该简单但是I信息丰富，且易于人类阅读 notify唤醒等待对象监控的一个线程，随意选择一个线程唤醒。唤醒的线程竞争对象的监控，直到当前线程丢弃对象的监控唤醒的线程和其他线程竞争对象的锁，唤醒的线程没有任何特权或者低权限这个方法只能被获取对象监控的线程调用，获取线程监控的方式有三，1 调用同步代码块，调用同步对象方法，调用同步类方法 notifyAll唤醒所有等待对象监控的线程 wait当前线程必须拥有对象的monitor把当前线程放置到对象的等待队列，唤醒线程的方式，notify，notifyAll，Thread#interrupt，real time has elapsed, clonex.clone() != xx.clone().getClass() == x.getClass()x.clone().equals(x)","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"source","slug":"source","permalink":"https://liyong.ac.cn/tags/source/"}]},{"title":"English-01","slug":"English-01","date":"2018-03-22T13:12:06.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/03/22/English-01/","link":"","permalink":"https://liyong.ac.cn/2018/03/22/English-01/","excerpt":"","text":"wantwould likefeel likego forbe in the mood forbe cravingdying forhit the spotmy kingdom for comecome up with 提出，想出 moneycome up against 遇上麻烦 obstaclecome acrosscome around","categories":[],"tags":[]},{"title":"log4j2配置详解","slug":"log4j2配置详解","date":"2018-03-07T08:09:33.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/03/07/log4j2配置详解/","link":"","permalink":"https://liyong.ac.cn/2018/03/07/log4j2配置详解/","excerpt":"","text":"slf4j-log4j12 适配","categories":[{"name":"log","slug":"log","permalink":"https://liyong.ac.cn/categories/log/"}],"tags":[{"name":"log4j2","slug":"log4j2","permalink":"https://liyong.ac.cn/tags/log4j2/"}]},{"title":"Redis入门","slug":"Redis入门","date":"2018-03-05T08:31:16.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/03/05/Redis入门/","link":"","permalink":"https://liyong.ac.cn/2018/03/05/Redis入门/","excerpt":"什么是RedisRedis 是一个使用 C 语言写成的，开源的 key-value 数据库。。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave","text":"什么是RedisRedis 是一个使用 C 语言写成的，开源的 key-value 数据库。。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave Redis与Memcached的区别与比较 Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String Redis支持数据的备份，即master-slave模式的数据备份 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中 redis的速度比memcached快很多 Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的IO复用模型 Redis常见数据结构使用场景String 常用命令: set,get,decr,incr,mget 等 String数据结构是简单的key-value类型，value其实不仅可以是String，也可以是数字。 常规key-value缓存应用； 常规计数：微博数，粉丝数等 Hash 常用命令： hget,hset,hgetall 等 Hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 比如我们可以Hash数据结构来存储用户信息，商品信息等等。 List 常用命令: lpush,rpush,lpop,rpop,lrange等 list就是链表，Redis list的应用场景非常多，也是Redis最重要的数据结构之一，比如微博的关注列表，粉丝列表，最新消息排行等功能都可以用Redis的list结构来实现。Redis list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。 Set 常用命令： sadd,spop,smembers,sunion 等 set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以自动排重的。 当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis可以非常方便的实现如共同关注、共同喜好、二度好友等功能。 Sorted Set 常用命令： zadd,zrange,zrem,zcard等 和set相比，sorted set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列举例： 在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息，适合使用Redis中的SortedSet结构进行存储。 Redis有哪些数据淘汰策略 volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"},{"name":"redis","slug":"redis","permalink":"https://liyong.ac.cn/tags/redis/"}]},{"title":"NIO:Socket","slug":"NIO-Socket","date":"2018-02-04T02:50:37.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/02/04/NIO-Socket/","link":"","permalink":"https://liyong.ac.cn/2018/02/04/NIO-Socket/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"NIO:Channel","slug":"NIO-Channel","date":"2018-02-03T11:08:54.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/02/03/NIO-Channel/","link":"","permalink":"https://liyong.ac.cn/2018/02/03/NIO-Channel/","excerpt":"所有的 NIO 的 I/O 操作都是从 Channel 开始的. 一个 channel 类似于一个 stream.java Stream 和 NIO Channel 对比 channel可以进行读写，而stream只能进行读或者写 channel可以同步和异步，而stream只能是同步 channel是从buffer中读取数据，把数据写入buffer中","text":"所有的 NIO 的 I/O 操作都是从 Channel 开始的. 一个 channel 类似于一个 stream.java Stream 和 NIO Channel 对比 channel可以进行读写，而stream只能进行读或者写 channel可以同步和异步，而stream只能是同步 channel是从buffer中读取数据，把数据写入buffer中 Channel 类型有:FileChannel, 文件操作DatagramChannel, UDP 操作SocketChannel, TCP 操作ServerSocketChannel, TCP 操作, 使用在服务器端.这些通道涵盖了 UDP 和 TCP网络 IO以及文件 IO FileChannel1234567891011121314151617181920212223242526272829303132333435363738394041public static void main(String[] args) throws IOException &#123; String path = ChannelDemo1.class.getResource(\"\").getPath(); System.out.println(path); RandomAccessFile r = new RandomAccessFile( \"ChannelDemo1\", \"rw\"); read(r); write(r); r.close(); &#125; private static void write(RandomAccessFile r) throws IOException &#123; FileChannel channel = r.getChannel(); String addStr = \"hello writer\\r\\n\"; ByteBuffer buffer = ByteBuffer.allocate(1044); buffer.clear(); buffer.put(addStr.getBytes()); buffer.flip(); while (buffer.hasRemaining())&#123; channel.write(buffer); &#125; &#125; private static void read(RandomAccessFile r) throws IOException &#123; FileChannel fileChannel = r.getChannel(); ByteBuffer byteBuffer = ByteBuffer.allocate(48); int read = fileChannel.read(byteBuffer); while (read!=-1)&#123; byteBuffer.flip(); while (byteBuffer.hasRemaining())&#123; byte b = byteBuffer.get(); System.out.print((char)b); &#125; System.out.println(); byteBuffer.clear(); read = fileChannel.read(byteBuffer); &#125; &#125; FileChannel.read(ByteBuffer)1、申请一块和缓存同大小的DirectByteBuffer bb。2、读取数据到缓存bb，底层由NativeDispatcher的read实现。3、把bb的数据读取到dst（用户定义的缓存，在jvm中分配内存）。 FileChannel.write(ByteBuffer)1、申请一块DirectByteBuffer，bb大小为byteBuffer中的limit - position。2、复制byteBuffer中的数据到bb中。3、把数据从bb中写入到文件，底层由NativeDispatcher的write实现 SocketChannel1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class server &#123; public static void main(String[] args) &#123; ServerSocket serverSocket = null; InputStream in = null; try &#123; serverSocket = new ServerSocket(8080); int recvMsgSize = 0; byte[] recvBuf = new byte[1024]; while(true)&#123; Socket clntSocket = serverSocket.accept(); SocketAddress clientAddress = clntSocket.getRemoteSocketAddress(); System.out.println(\"Handling client at \"+clientAddress); in = clntSocket.getInputStream(); while((recvMsgSize=in.read(recvBuf))!=-1)&#123; byte[] temp = new byte[recvMsgSize]; System.arraycopy(recvBuf, 0, temp, 0, recvMsgSize); System.out.println(new String(temp)); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally&#123; try&#123; if(serverSocket!=null)&#123; serverSocket.close(); &#125; if(in!=null)&#123; in.close(); &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125; &#125;public class Client &#123; public static void main(String[] args) &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); SocketChannel socketChannel = null; try &#123; socketChannel = SocketChannel.open(); socketChannel.configureBlocking(false); socketChannel.connect(new InetSocketAddress(\"localhost\",8080)); while (!socketChannel.finishConnect())&#123; TimeUnit.SECONDS.sleep(1); &#125; int i=0; while(true) &#123; TimeUnit.SECONDS.sleep(1); String info = \"I'm \"+i+++\"-th information from client\"; buffer.clear(); buffer.put(info.getBytes()); buffer.flip(); while(buffer.hasRemaining())&#123; System.out.println(buffer); socketChannel.write(buffer); &#125; &#125; &#125; catch (IOException | InterruptedException e) &#123; e.printStackTrace(); &#125; finally&#123; try&#123; if(socketChannel!=null)&#123; socketChannel.close(); &#125; &#125;catch(IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; DatagramChannel打开 12DatagramChannel channel = DatagramChannel.open();channel.socket().bind(new InetSocketAddress(9999)); 读取数据 123ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();channel.receive(buf); 读取数据 12345678String newData = \"New String to write to file...\" + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();int bytesSent = channel.send(buf, new InetSocketAddress(\"example.com\", 80)); 连接到指定地址 1channel.connect(new InetSocketAddress(\"example.com\", 80));","categories":[{"name":"nio","slug":"nio","permalink":"https://liyong.ac.cn/categories/nio/"}],"tags":[{"name":"nio","slug":"nio","permalink":"https://liyong.ac.cn/tags/nio/"}]},{"title":"NIO:Buffer","slug":"NIO-Buffer","date":"2018-02-03T11:08:36.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/02/03/NIO-Buffer/","link":"","permalink":"https://liyong.ac.cn/2018/02/03/NIO-Buffer/","excerpt":"channle和数据的交互都是使用buffer进行的，buffer其实就是一块内存区域，我们可以在这块内存区域中进行数据的读写，NIO Buffer 其实是这样的内存块的一个封装, 并提供了一些操作方法让我们能够方便地进行数据的读写.Buffer 类型有: ByteBufferCharBufferDoubleBufferFloatBufferIntBufferLongBufferShortBuffer这些 Buffer 覆盖了能从 IO 中传输的所有的 Java 基本数据类型.","text":"channle和数据的交互都是使用buffer进行的，buffer其实就是一块内存区域，我们可以在这块内存区域中进行数据的读写，NIO Buffer 其实是这样的内存块的一个封装, 并提供了一些操作方法让我们能够方便地进行数据的读写.Buffer 类型有: ByteBufferCharBufferDoubleBufferFloatBufferIntBufferLongBufferShortBuffer这些 Buffer 覆盖了能从 IO 中传输的所有的 Java 基本数据类型. NIO Buffer 的基本使用 将数据写入到 Buffer 中 调用 Buffer.flip()方法, 将 NIO Buffer 转换为读模式 从 Buffer 中读取数据 调用 Buffer.clear() 或 Buffer.compact()方法, 将 Buffer 转换为写模式. 12345678910public class Test &#123; public static void main(String[] args) &#123; IntBuffer intBuffer = IntBuffer.allocate(2); intBuffer.put(12345678); intBuffer.put(2); intBuffer.flip(); System.err.println(intBuffer.get()); System.err.println(intBuffer.get()); &#125;&#125; Buffer 属性1234567891011private int mark = -1;private int position = 0;private int limit;private int capacity;public final Buffer flip() &#123; limit = position; position = 0; mark = -1; return this;&#125; markmark 用于备份当前的position，以便后续的重置 1234567891011public final Buffer mark() &#123; mark = position; return this;&#125;public final Buffer reset() &#123; int m = mark; if (m &lt; 0) throw new InvalidMarkException(); position = m; return this;&#125; capacity缓存数组大小 position 写模式下position++ flip方法会重置position=0 读模式下position++ limit 写模式，代表最多能写多少单位数据和容量是一样的 读模式，代表最多能读多少单位数据，和之前写入的单位数据量一致 分配 Buffer12ByteBuffer buf = ByteBuffer.allocate(48);CharBuffer buf = CharBuffer.allocate(1024); Direct Buffer 和 Non-Direct Buffer 的区别Direct Buffer 所分配的内存不在 JVM 堆上, 不受 GC 的管理.(但是 Direct Buffer 的 Java 对象是由 GC 管理的, 因此当发生 GC, 对象被回收时, Direct Buffer 也会被释放) 因为 Direct Buffer 不在 JVM 堆上分配, 因此 Direct Buffer 对应用程序的内存占用的影响就不那么明显(实际上还是占用了这么多内存, 但是 JVM 不好统计到非 JVM 管理的内存.) 申请和释放 Direct Buffer 的开销比较大. 因此正确的使用 Direct Buffer 的方式是在初始化时申请一个 Buffer, 然后不断复用此 buffer, 在程序结束后才释放此 buffer. 使用 Direct Buffer 时, 当进行一些底层的系统 IO 操作时, 效率会比较高, 因为此时 JVM 不需要拷贝 buffer 中的内存到中间临时缓冲区中. Non-Direct Buffer 直接在 JVM 堆上进行内存的分配, 本质上是 byte[] 数组的封装. 因为 Non-Direct Buffer 在 JVM 堆中, 因此当进行操作系统底层 IO 操作中时, 会将此 buffer 的内存复制到中间临时缓冲区中. 因此 Non-Direct Buffer 的效率就较低","categories":[{"name":"nio","slug":"nio","permalink":"https://liyong.ac.cn/categories/nio/"}],"tags":[{"name":"nio","slug":"nio","permalink":"https://liyong.ac.cn/tags/nio/"}]},{"title":"Netty:ChannelPipeline","slug":"Netty-ChannelPipeline","date":"2018-02-03T06:36:13.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/02/03/Netty-ChannelPipeline/","link":"","permalink":"https://liyong.ac.cn/2018/02/03/Netty-ChannelPipeline/","excerpt":"概述每个channel维护一个ChannelPipeline,ChannelPipeline中维护了一个由ChannelContext组成的双向链表，表头是HeadContext,表尾是TailContext,而每个ChannelContext又维护一个ChannelHandler","text":"概述每个channel维护一个ChannelPipeline,ChannelPipeline中维护了一个由ChannelContext组成的双向链表，表头是HeadContext,表尾是TailContext,而每个ChannelContext又维护一个ChannelHandler 初始化123456protected AbstractChannel(Channel parent) &#123; this.parent = parent; id = newId(); unsafe = newUnsafe(); pipeline = newChannelPipeline(); //DefaultChannelPipeline &#125; 从上面的构造方法可以看出channel持有一个pipeline对象的引用 1234567891011protected DefaultChannelPipeline(Channel channel) &#123; this.channel = ObjectUtil.checkNotNull(channel, \"channel\"); succeededFuture = new SucceededChannelFuture(channel, null); voidPromise = new VoidChannelPromise(channel, true); tail = new TailContext(this); head = new HeadContext(this); head.next = tail; tail.prev = head; &#125; HeadContext 和 TailContext 继承于 AbstractChannelHandlerContext 的同时也实现了 ChannelHandler 接口了, 因此它们有 Context 和 Handler 的双重属性 head 和 tail的类层次结构 ChannelInitializerChannelInitializer 继承了ChannelInboundHandlerAdapter，说明其实际就是一个channel，ChannelInitializer是在Bootstrap.init 方法中添加到 ChannelPipeline 12345678910111213141516171819Bootstrap b = new Bootstrap();b.group(group) .channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline p = ch.pipeline(); p.addLast(new EchoClientHandler()); &#125; &#125;);@Override@SuppressWarnings(\"unchecked\")void init(Channel channel) throws Exception &#123; ChannelPipeline p = channel.pipeline(); p.addLast(handler()); ...&#125; 有朋友可能就有疑惑了, 我明明插入的是一个 ChannelInitializer 实例, 为什么在 ChannelPipeline 中的双向链表中的元素却是一个 ChannelHandlerContext? 为了解答这个问题, 我们继续在代码中寻找答案吧.上面的代码会调用到 123456789101112131415161718192021222324252627282930313233 @Override public final ChannelPipeline addLast(EventExecutorGroup group, String name, ChannelHandler handler) &#123; final AbstractChannelHandlerContext newCtx; synchronized (this) &#123; checkMultiplicity(handler); // 关键，通过handler生成一个context newCtx = newContext(group, filterName(name, handler), handler); addLast0(newCtx); &#125; callHandlerAdded0(newCtx); return this; &#125;private AbstractChannelHandlerContext newContext(EventExecutorGroup group, String name, ChannelHandler handler) &#123; return new DefaultChannelHandlerContext(this, childExecutor(group), name, handler); &#125;DefaultChannelHandlerContext( DefaultChannelPipeline pipeline, EventExecutor executor, String name, ChannelHandler handler) &#123; super(pipeline, executor, name, isInbound(handler), isOutbound(handler)); if (handler == null) &#123; throw new NullPointerException(\"handler\"); &#125; this.handler = handler; &#125;private void addLast0(AbstractChannelHandlerContext newCtx) &#123; AbstractChannelHandlerContext prev = tail.prev; newCtx.prev = prev; newCtx.next = tail; prev.next = newCtx; tail.prev = newCtx; &#125; ChannelHandler AbstractBootstrap.initAndRegister通过 group().register(channel), 调用 MultithreadEventLoopGroup.register 方法 MultithreadEventLoopGroup.register 中, 通过 next() 获取一个可用的 SingleThreadEventLoop, 然后调用它的 register SingleThreadEventLoop.register 中, 通过 channel.unsafe().register(this, promise) 来获取 channel 的 unsafe() 底层操作对象, 然后调用它的 register unsafe.register 方法中, 调用 AbstractUnsafe.register0 方法注册 Channel 在 AbstractUnsafe.register0 中, 调用 AbstractNioChannel.doRegister 方法 AbstractNioChannel.doRegister 方法通过 javaChannel().register(eventLoop().selector, 0, this) 将 Channel 对应的 Java NIO SockerChannel 注册到一个 eventLoop 的 Selector 中, 并且将当前 Channel 作为 attachment. ChannelHandler 的添加过程, 发生在 AbstractUnsafe.register0 中,io.netty.channel.AbstractChannel.AbstractUnsafe#register0io.netty.channel.DefaultChannelPipeline#fireChannelRegisteredio.netty.channel.AbstractChannelHandlerContext#invokeChannelRegistered 123456789101112131415161718 private void invokeChannelRegistered() &#123; if (invokeHandler()) &#123; try &#123; ((ChannelInboundHandler) handler()).channelRegistered(this); &#125; catch (Throwable t) &#123; notifyHandlerException(t); &#125; &#125; else &#123; fireChannelRegistered(); &#125; &#125;// HeadContext public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; invokeHandlerAddedIfNeeded(); ctx.fireChannelRegistered(); &#125; 传播机制ChannelHandlerContextinbound 为true代表入事件outbound 为true代表出事件 Inbound 事件传播方法有123456789ChannelHandlerContext.fireChannelRegistered()ChannelHandlerContext.fireChannelActive()ChannelHandlerContext.fireChannelRead(Object)ChannelHandlerContext.fireChannelReadComplete()ChannelHandlerContext.fireExceptionCaught(Throwable)ChannelHandlerContext.fireUserEventTriggered(Object)ChannelHandlerContext.fireChannelWritabilityChanged()ChannelHandlerContext.fireChannelInactive()ChannelHandlerContext.fireChannelUnregistered() Oubound 事件传输方法有1234567ChannelHandlerContext.bind(SocketAddress, ChannelPromise)ChannelHandlerContext.connect(SocketAddress, SocketAddress, ChannelPromise)ChannelHandlerContext.write(Object, ChannelPromise)ChannelHandlerContext.flush()ChannelHandlerContext.read()ChannelHandlerContext.disconnect(ChannelPromise)ChannelHandlerContext.close(ChannelPromise) 概述12345678910111213141516171819202122232425262728293031323334353637383940 I/O Request via Channel or ChannelHandlerContext |+---------------------------------------------------+---------------+| ChannelPipeline | || \\|/ || +---------------------+ +-----------+----------+ || | Inbound Handler N | | Outbound Handler 1 | || +----------+----------+ +-----------+----------+ || /|\\ | || | \\|/ || +----------+----------+ +-----------+----------+ || | Inbound Handler N-1 | | Outbound Handler 2 | || +----------+----------+ +-----------+----------+ || /|\\ . || . . || ChannelHandlerContext.fireIN_EVT() ChannelHandlerContext.OUT_EVT()|| [ method call] [method call] || . . || . \\|/ || +----------+----------+ +-----------+----------+ || | Inbound Handler 2 | | Outbound Handler M-1 | || +----------+----------+ +-----------+----------+ || /|\\ | || | \\|/ || +----------+----------+ +-----------+----------+ || | Inbound Handler 1 | | Outbound Handler M | || +----------+----------+ +-----------+----------+ || /|\\ | |+---------------+-----------------------------------+---------------+ | \\|/+---------------+-----------------------------------+---------------+| | | || [ Socket.read() ] [ Socket.write() ] || || Netty Internal I/O Threads (Transport Implementation) |+-------------------------------------------------------------------+ inbound 事件的流行是从下至上, 而 outbound 刚好相反, 是从上到下. 并且 inbound 的传递方式是通过调用相应的 ChannelHandlerContext.fireIN_EVT() 方法, 而 outbound 方法的的传递方式是通过调用 ChannelHandlerContext.OUT_EVT() 方法. 例如 ChannelHandlerContext.fireChannelRegistered() 调用会发送一个 ChannelRegistered 的 inbound 给下一个ChannelHandlerContext, 而 ChannelHandlerContext.bind 调用会发送一个 bind 的 outbound 事件给 下一个 ChannelHandlerContext.例如: 123456789101112131415public class MyInboundHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) &#123; System.out.println(\"Connected!\"); ctx.fireChannelActive(); &#125;&#125;public clas MyOutboundHandler extends ChannelOutboundHandlerAdapter &#123; @Override public void close(ChannelHandlerContext ctx, ChannelPromise promise) &#123; System.out.println(\"Closing ..\"); ctx.close(promise); &#125;&#125; Outbound 操作Outbound 事件都是请求事件(request event), 即请求某件事情的发生, 然后通过 Outbound 事件进行通知.Outbound 事件的传播方向是 tail -&gt; customContext -&gt; head.接下来以 connect 事件为例, 分析一下 Outbound 事件的传播机制.Bootstrap.connect -&gt; Bootstrap.doConnect -&gt; AbstractChannel.connect-&gt; DefaultChannelPipeline.connect-&gt;tail.connectoutbound 事件(这里是 connect 事件)传递到 Pipeline 后, 它其实是以 tail 为起点开始传播的.而 tail.connect 其实调用的是 AbstractChannelHandlerContext.connect 方法,然后事件一直传递，直至到了HeadContext 12345678// HeadContext @Override public void connect( ChannelHandlerContext ctx, SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) throws Exception &#123; unsafe.connect(remoteAddress, localAddress, promise); &#125; 当 Connect 这个 Outbound 传播到 unsafe 后, 其实是在 AbstractNioUnsafe.connect 方法中进行处理的,在 AbstractNioUnsafe.connect 中, 首先调用 doConnect 方法进行实际上的 Socket 连接, 当连接上后, 会调用 fulfillConnectPromise 方法 123456789101112131415161718192021@Overridepublic final void connect( final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) &#123; ... if (doConnect(remoteAddress, localAddress)) &#123; fulfillConnectPromise(promise, wasActive); &#125; else &#123; ... &#125; ...&#125;private void fulfillConnectPromise(ChannelPromise promise, boolean wasActive) &#123; ... // Regardless if the connection attempt was cancelled, channelActive() event should be triggered, // because what happened is what happened. if (!wasActive &amp;&amp; isActive()) &#123; pipeline().fireChannelActive(); &#125; ...&#125; 我们看到, 在 fulfillConnectPromise 中, 会通过调用 pipeline().fireChannelActive() 将通道激活的消息(即 Socket 连接成功)发送出去.而这里, 当调用 pipeline.fireXXX 后, 就是 Inbound 事件的起点 Inbound 事件Inbound 事件是一个通知事件, 即某件事已经发生了, 然后通过 Inbound 事件进行通知. Inbound 通常发生在 Channel 的状态的改变或 IO 事件就绪Inbound 的特点是它传播方向是 head -&gt; customContext -&gt; tail 在Outbound事件中分析到了pipeline.fireXXX，接着往下分析 123456 @Override public final ChannelPipeline fireChannelActive() &#123;// Inbound 事件在 Pipeline 中传输的起点是 head AbstractChannelHandlerContext.invokeChannelActive(head); return this; &#125; Outbound事件一直传输，直到了TailContext,通过查看代码发现TailContext什么也没有做","categories":[{"name":"netty","slug":"netty","permalink":"https://liyong.ac.cn/categories/netty/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"https://liyong.ac.cn/tags/源码分析/"},{"name":"netty","slug":"netty","permalink":"https://liyong.ac.cn/tags/netty/"}]},{"title":"Netty:Future","slug":"Netty-Future","date":"2018-02-01T09:22:33.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/02/01/Netty-Future/","link":"","permalink":"https://liyong.ac.cn/2018/02/01/Netty-Future/","excerpt":"","text":"jdk Future异步结果的计算cancel isCancelled isDone get netty FutureisSuccess isCancellable cause addListener removeListener sync await Promise继承 netty FuturesetSuccess trySuccess setFailure ChannelFuturechannelisVoid ChannelPromise extends ChannelFuture, PromisesetSuccess trySuccess unvoid DefaultChannelPromise","categories":[{"name":"netty","slug":"netty","permalink":"https://liyong.ac.cn/categories/netty/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"https://liyong.ac.cn/tags/源码分析/"},{"name":"netty","slug":"netty","permalink":"https://liyong.ac.cn/tags/netty/"}]},{"title":"Netty源码分析01","slug":"Netty源码分析01","date":"2018-01-31T05:59:09.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/31/Netty源码分析01/","link":"","permalink":"https://liyong.ac.cn/2018/01/31/Netty源码分析01/","excerpt":"","text":"ConstantPool 常量池，内部持有一个map, 通过子类复写newConstant实现自定义 ChannelOption 通道的选项 ChannelPipeline 责任链的链条 EventLoopGroup 池化思想 ChannelHandler 处理器 ChannelOutboundInvoker 通道出栈调用者 SelectionKey 就是组合了下Selector和channelSelector 持有SelectionKey集合的引用 io.netty.bootstrap.ServerBootstrap#init初始化 ChannelOption，主要用来存储功能参数，比如系统的设置初始化 AttributeKey，主要用来存储业务属性，目的不同将ServerBootstrap的handle添加到pipline io.netty.bootstrap.AbstractBootstrap#doBind0io.netty.channel.AbstractChannel.AbstractUnsafe#register0io.netty.channel.nio.AbstractNioChannel#doRegister java.nio.channels.spi.AbstractSelectableChannel#register io.netty.channel.nio.AbstractNioMessageChannel.NioMessageUnsafe#readio.netty.channel.socket.nio.NioServerSocketChannel#doReadMessages io.netty.bootstrap.ServerBootstrap.ServerBootstrapAcceptor#channelRead将启动时传入的 childHandler 加入到客户端 SocketChannel 的ChannelPipeline 中设置客户端 SocketChannel 的 TCP 参数注册 SocketChannel 到多路复用器 io.netty.channel.AbstractChannel#connect(java.net.SocketAddress, java.net.SocketAddress, io.netty.channel.ChannelPromise) DefaultEventExecutorChooserFactory工厂模式和单例模式一起出现 DefaultSelectStrategy实现选择的策略 SELECT 阻塞的 CONTINUE 非阻塞的 DefaultThreadFactory 线程工厂类线程优先级 priority是否为守护线程 daemon线程所属组 System.getSecurityManager() == null ?Thread.currentThread().getThreadGroup() : System.getSecurityManager().getThreadGroup() ThreadPerTaskExecutor实现Executor接口执行execute方法 ExecutorService扩展 Executor接口，提供了submit方法和shutdown等相关方法 ScheduledExecutorService集成ExecutorService，扩展了定时任务 EventExecutorGroupEventExecutorGroup extends ScheduledExecutorService, Iterable提供了next方法和生命周期的管理 EventExecutor接口可以多继承继承EventExecutorGroup，提供了写方便的方法，比如判断inEventLoop判断当前线程是否在event loop中执行 WindowsSelectorProviderSelectorProvider提供了创建selector、ServerSocketChannel、SocketChanne等的方法 provider方法首先通过java.nio.channels.spi.SelectorProvider加载其次通过 spi(ServiceLoader)加载 NioEventLoopNioEventLoop -&gt; SingleThreadEventLoop -&gt; SingleThreadEventExecutor -&gt; AbstractScheduledEventExecutorCLEANUP_INTERVAL 清除间隔DISABLE_KEYSET_OPTIMIZATION 不可用的key优化 SingleThreadEventLooptailTasks=LinkedBlockingQueue SingleThreadEventExecutoraddTaskWakesUp=falsemaxPendingTasksexecutor=MpscChunkedArrayQueuerejectedExecutionHandler=RejectedExecutionHandlerstaskQueue=队列 AbstractScheduledEventExecutorparent =NioEventLoopGroup SelectedSelectionKeySet持有SelectionKey的集合 SelectionKey聚合了Channel和Selector PipeSelectedSelectionKeySetSelector委托模式，增强了WindowsSelectorImpl jdk Future异步结果的计算getcancel netty Future添加listener添加同步机制sync GenericFutureListener回调函数 FutureListener和GenericFutureListener差不多，隐藏了类型参数 Promise继承 jdk Future添加 设置成功失败的方法 AbstractBootstrap.initAndRegister -&gt; MultithreadEventLoopGroup.register -&gt; SingleThreadEventLoop.register -&gt; AbstractUnsafe.register Unsafe","categories":[],"tags":[]},{"title":"Mysql性能优化","slug":"Mysql性能优化","date":"2018-01-28T11:10:47.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/28/Mysql性能优化/","link":"","permalink":"https://liyong.ac.cn/2018/01/28/Mysql性能优化/","excerpt":"","text":"sakilahttps://dev.mysql.com/doc/sakila/en/sakila-installation.htmlhttps://dev.mysql.com/doc/index-other.html 慢查询日志show variables like ‘%query%’;set global slow_query_log=ON;–必须是重新连接或者新打开一个会话set global long_query_time=4; 分析工具mysqldumpslowpt-query-digestexplain函数索引 子查询优化优化成join查询，需要注意一对多 索引在 where从句，group by 从句，order by 从句 on从句出现的列索引字段越小越好离散对大的列放到联合索引的前面 数据库查询，需要分析使用哪个索引，所以不是索引越多越好 重复，冗余索引 pt-duplicate-key-checkerpt-index-usage通过慢查询日志配合pt-index-usage工具类进行索引使用情况的分析 数据库结构优化选择合适的数据类型 可以存放数据的最小数据类型 使用简单的数据类型。Int要比varchar类型在mysql中处理简单 尽可能使用not null定义字段 尽量少用text类型，非用不可，最好考虑分表 ip地址使用bigint存储INET_ACON ip字符串地址转换成bigintINET_NTOA bigint转换成字符串 结构优化数据库表中不存在非关键字段对任意候选字段的传递函数依赖原则 配置工具https://www.percona.com/ 查询行转列 cross join sum case when 列转行","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"内存间交互","slug":"aa_category/se/内存间交互","date":"2018-01-28T03:18:46.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/28/aa_category/se/内存间交互/","link":"","permalink":"https://liyong.ac.cn/2018/01/28/aa_category/se/内存间交互/","excerpt":"","text":"主内存和工作内存交互指令lock,作用于主线程变量，把变量标志为一条线程独占unlock,作用于主线程变量，去掉线程独占标志read,作用于主线程变量，把变量的值从主内存传输到工作线程，以便后面的load使用load,作用于工作线程变量，把read从主内存获取的变量赋值给工作线程的变量副本use,作用于工作线程变量，把变量传给执行引擎，当虚拟机遇到需要使用变量值的字节码的指令的时候使用assgin,作用于工作内存变量，从执行引擎接受到的值赋值给工作内存store,作用于工作内存变量，把工作内存的值传递给工作内从write,作用于主线程存变量，把store获取的工作线程变量赋值给主内存变量 read,load 和 store，write必须成对出现 不允许线程丢弃最近的assgin的操作，即assgin以后必须把改变同步回主内存 对一个线程执行unlock之前必须把改变同步回主内存","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"thread","slug":"thread","permalink":"https://liyong.ac.cn/tags/thread/"},{"name":"jmm","slug":"jmm","permalink":"https://liyong.ac.cn/tags/jmm/"}]},{"title":"Java类加载","slug":"aa_category/se/Java类加载","date":"2018-01-21T06:38:42.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/21/aa_category/se/Java类加载/","link":"","permalink":"https://liyong.ac.cn/2018/01/21/aa_category/se/Java类加载/","excerpt":"","text":"类的生命周期加载 验证 准备 解析 初始化 使用 卸载 加载 通过类的全限定名获取此类的二进制流 通过二进制流所代表的静态结构保存为方法区运行时数据结构 在java堆中生成一个代表这个类的java.lang.Class对象，作为方法区访问的入口 虚拟机设计团队把加载动作放到JVM外部实现，以便让应用程序决定如何获取所需的类，实现这个动作的代码称为“类加载器”，JVM提供了3种类加载器： 启动类加载器（BootStrap ClassLoader）:负责加载JAVAHOME\\lib目录，或通过-Xbootclasspath参数指定路径中的，且被虚拟机认可（按文件名识别，如rt.jar）的类。 扩展类加载器（Extends ClassLoader）:负责加载 JAVAHOME\\ext目录，或通过java.ext.dirs系统变量指定路径中的类库。 应用程序类加载器（Application ClassLoader）：负责加载用户路径（classpath）上的类库。 当一个类收到加载任务的时候，优先交给父类加载器去完成，只有父类加载器无法完成任务的时候自己才会尝试去加载 验证格式验证、元数据验证、字节码验证、符号引用验证 准备为类变量在方法区分配内存，并设置初始值 1234// 初始化为 0private static int var = 100;//初始化为100private static final int var =100; 解析解析是将常量池中的符号引用替换为直接引用的过程 符号引用使用一组符号来描述所引用的目标，可以是任何形式的字面常量，定义在Class文件格式中。 直接引用可以是直接指向目标的指针、相对偏移量或则能间接定位到目标的句柄 初始化执行类构造器方法的过程，方法是由类变量和静态代码块在类中定义的顺序组合的，该组合动作是由类编译器完成的 类构造方法不是必须的，只有当类中有静态变量或者静态代码块，则编译器不会生成 类构造器与对象构造器不同，不需要显示调用父类的类构造方法，虚拟机保证先执行父类的类构造方法 为了防止多次执行，在多线程环境会正确加锁 类初始化场景 执行 new、getstatic、putstatic和invokestatic 使用reflect对类镜像反射调用 优先初始化父类 启动虚拟机是，需要初始化包含main方法的类 在JDK1.7中，如果java.lang.invoke.MethodHandler实例最后的解析结果REFgetStatic、REFputStatic、REF_invokeStatic的方法句柄，并且这个方法句柄对应的类没有进行初始化 不会触发初始化的场景 通过子类引用父类的静态字段，只会触发父类的初始化，而不会触发子类的初始化。 定义对象数组，不会触发该类的初始化。1234567891011121314151617181920public class ParentInit01 &#123; public static int a = 0; static &#123; System.out.println(\"parentInit01 init\"); &#125;&#125;public class ChilderInit01 extends ParentInit01&#123; public static int b = 2; static &#123; System.out.println(\"ChilderInit01 init\"); &#125;&#125; public static void main(String[] args) &#123; final int b = ChilderInit01.a; System.out.println(b); ChilderInit01[] cs = new ChilderInit01[10]; &#125; 输出parentInit01 init0 常量在编译期间会存入调用类的常量池中，本质上并没有直接引用定义常量的类，不会触发定义常量所在的类 1234567891011public class Const &#123; public static final int a =100; static &#123; System.out.println(\"const class init is called\"); &#125;&#125;public class ConstMain &#123; public static void main(String[] args) &#123; System.out.println(Const.a); &#125;&#125; 输出100 通过类名获取Class对象，不会触发类的初始化通过Class.forName加载指定类时，如果指定参数initialize为false时，也不会触发类初始化，其实这个参数是告诉虚拟机，是否要对类进行初始化 通过ClassLoader默认的loadClass方法，也不会触发初始化动作","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"}]},{"title":"HashMap:源码分析","slug":"HashMap-源码分析","date":"2018-01-21T05:52:50.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/21/HashMap-源码分析/","link":"","permalink":"https://liyong.ac.cn/2018/01/21/HashMap-源码分析/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"ThreadLocal:源码分析","slug":"aa_category/se/source/ThreadLocal-源码分析","date":"2018-01-21T02:45:49.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/21/aa_category/se/source/ThreadLocal-源码分析/","link":"","permalink":"https://liyong.ac.cn/2018/01/21/aa_category/se/source/ThreadLocal-源码分析/","excerpt":"","text":"ThreadLocal是什么它是一个数据结构，有点像HashMap，可以保存”key : value”键值对，但是一个ThreadLocal只能保存一个，并且各个线程的数据互不干扰 set&amp;set方法set 方法12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125; get方法1234567891011121314public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; &#125; &#125; //初始化map，且设置值为null return setInitialValue(); &#125; ThreadLocalMap可以发现每个线程都有ThreadLocalMap，当执行set的时候保存到当前线程的threadLocals变量中，所以线程之间不会相互影响 123456789101112131415161718192021222324252627282930313233private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; // We don't use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; //根据ThreadLocal对象的hash值，定位到table中的位置i int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); //如果key正好是当前ThreadLoack则替换值 if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; // 当前位置为空，初始化Entry放置到位置i tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); &#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"source","slug":"source","permalink":"https://liyong.ac.cn/tags/source/"}]},{"title":"ThreadPoolExecutor:源码分析","slug":"aa_category/se/source/ThreadPoolExecutor-源码分析","date":"2018-01-19T05:21:23.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/19/aa_category/se/source/ThreadPoolExecutor-源码分析/","link":"","permalink":"https://liyong.ac.cn/2018/01/19/aa_category/se/source/ThreadPoolExecutor-源码分析/","excerpt":"private static final int RUNNING = -1 &lt;&lt; COUNT_BITS; private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS; private static final int STOP = 1 &lt;&lt; COUNT_BITS; private static final int TIDYING = 2 &lt;&lt; COUNT_BITS; private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; runState RUNNING:接受任务，并处理队列中的任务SHUTDOWN：关闭接收新任务，但是处理已有队列中的任务STOP：不接收新的任务，不处理队列中已有的任务TIDYING：所有任务已经处理完成，触发钩子方法terminatedTERMINATED：方法terminated调用完成时候的状态","text":"private static final int RUNNING = -1 &lt;&lt; COUNT_BITS; private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS; private static final int STOP = 1 &lt;&lt; COUNT_BITS; private static final int TIDYING = 2 &lt;&lt; COUNT_BITS; private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; runState RUNNING:接受任务，并处理队列中的任务SHUTDOWN：关闭接收新任务，但是处理已有队列中的任务STOP：不接收新的任务，不处理队列中已有的任务TIDYING：所有任务已经处理完成，触发钩子方法terminatedTERMINATED：方法terminated调用完成时候的状态 take poll 都是从去 execute 方法 如果池中数量小于核心线程数，新建一个线程执行任务 如果线程池中任务数量大于核心线程数量，并且状态为运行状态，则把任务添加到任务队列 非以上两种情况。长沙那个是新建一个线程，并把任务添加到新线程中，如果失败，执行拒绝策略123456789101112131415161718192021222324252627 public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); // 获取ctl对应的int值。该int值保存了\"线程池中任务的数量\"和\"线程池状态\"信息 int c = ctl.get();// 获取线程池中任务的数量 if (workerCountOf(c) &lt; corePoolSize) &#123;// 新建一个线程，把任务添加到线程中，启动线程从而执行任务 if (addWorker(command, true)) return; c = ctl.get(); &#125;// isRunning 判断线程池状态是否为Running,把任务添加到任务队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get();// 再次检查线程池状态，如果不是运行时，删除任务 if (! isRunning(recheck) &amp;&amp; remove(command))// 执行拒绝策略 reject(command);// 如果线程池中的任务为0，添加一个空的任务 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125;// 队类满的情况 else if (!addWorker(command, false)) reject(command); &#125; addWorker方法 将任务添加到线程池中，并启动 core为true，则以corePoolSize为界限，若”线程池中已有任务数量&gt;=corePoolSize”，则返回false；core为false的话，则以maximumPoolSize为界限，若”线程池中已有任务数量&gt;=maximumPoolSize”，则返回false。 addWorker会先通过for循环不断尝试更新ctl状态，ctl记录了线程池中任务数量和线程池状态，更新成功之后，再通过try模块来将任务添加到线程池中，并启动任务所在的线程 从addWorker()中，我们能清晰的发现：线程池在添加任务时，会创建任务对应的Worker对象；而一个Workder对象包含一个Thread对象。(01) 通过将Worker对象添加到”线程的workers集合”中，从而实现将任务添加到线程池中。 (02) 通过启动Worker对应的Thread线程，则执行该任务。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374 private boolean addWorker(Runnable firstTask, boolean core) &#123;// 标签，go的云法 retry: for (;;) &#123; int c = ctl.get();// 运行状态 int rs = runStateOf(c); // 有效性检查 if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c);// 如果\"线程池中任务的数量\"超过限制，则返回false。 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false;// 通过cas将线程池中的数量+1，如果添加失败，退出循环，从retry重新开始 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl// 检查线程池状态，如果和之前的状态不同，从retry重新开始 if (runStateOf(c) != rs) continue retry; &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; java.util.concurrent.ThreadPoolExecutor.Worker w = null;// 添加任务到线程池，并启动线程 try &#123; w = new java.util.concurrent.ThreadPoolExecutor.Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); // 再次确认线程池状态 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException();// 把任务添加到任务集合中 workers.add(w);// 更新largeestPoolSize int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125;// 返回任务是否启动成功 return workerStarted; &#125; submit方法123456public Future&lt;?&gt; submit(Runnable task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); execute(ftask); return ftask; &#125; shutdown方法1234567891011121314151617 public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123;// 当前线程是否有终止权限 checkShutdownAccess();//设置线程池的状态为关闭状态 advanceRunState(SHUTDOWN);// 中断空闲的线程 interruptIdleWorkers();// 钩子函数，在ThreadPoolExecutor中没有任何动 onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); &#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"source","slug":"source","permalink":"https://liyong.ac.cn/tags/source/"}]},{"title":"Idea使用教程","slug":"Idea使用教程","date":"2018-01-14T07:26:37.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/14/Idea使用教程/","link":"","permalink":"https://liyong.ac.cn/2018/01/14/Idea使用教程/","excerpt":"","text":"整体介绍File 主要对文件操作Edit 主要是对文本操作code 源码的操作 搜索找类 ctrl+N找文件 ctrl+shift+N函数查找 ctrl+shift+alt+N字符串搜索 ctrl+shift+f查看方法被哪里调用 alt+F7在方法中查看接口的实现类：Ctrl + Alt + 鼠标左键点方法 Open Implementation查看类继承结构 ctrl+h方法调用层次 Ctrl+Alt+hctrl+F12 查看类中有哪些方法ctrl+H 可以查看一个类继承关系ctrl+shift+B 可以查看一个类的subtype(s)Ctrl+R，替换文本Ctrl+F，查找文本 Navigate 跳转窗口跳转 alt+对应的窗口编码 esc返回编辑窗口多窗口跳转 CtrL+alt+] 文件直接的跳转最近的打开的文件文件ctrl+e最近改变的文件ctrl+shift+e最近编辑的位置ctrl+shift+backspace最近浏览的位置ctrl+alt+左箭头利用书签跳转 bookmarks crtl+f11 ctrl+对应的标签数字跳转添加favorite alt+shift+f 列操作大小写转换 ctrl+shift+f选中某一单词 ctrl+shift+]选中相同的单词 ctrl+shift+alt+j选中多列 alt+鼠标左击 live templatepostfixforsoutfieldnnmain 方法 psvmSystem.out.println();在IntellJ中是输入sout 抽取抽取局部变量 ctrl+alt+v抽取静态变量 ctrl+alt+c抽取成员变量 ctrl+alt+f抽取方法参数 ctrl+alt+p抽取方法 ctrl+alt+M重命名 shift+f6重构方法 ctrl+f6重构一切：Ctrl+Shift+Alt+T 断点调试查看所有断点 ctrl+shift+f8禁止所有断点条件断点表达式求值 alt+f8运行到光标所在行设置值 alt+enter自动创建函数repalce with fromat实现接口 文件复制当前文件 F5移动文件 F6创建文件 Ctrl+Alt+insert复制文件名 Ctrl+c复制完整路径 ctrl+shift+c剪切板 ctrl+shift+v文件结构 Ctrl+F12 杂查看调用关系 Navigate | Call HierarchyAlt+Insert，可以生成构造器/Getter/Setter等本地修改轨迹查看maven 依赖，类图","categories":[{"name":"idea","slug":"idea","permalink":"https://liyong.ac.cn/categories/idea/"}],"tags":[{"name":"idea","slug":"idea","permalink":"https://liyong.ac.cn/tags/idea/"}]},{"title":"LockSupport:源码分析","slug":"aa_category/se/source/LockSupport-源码分析","date":"2018-01-10T05:34:35.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/10/aa_category/se/source/LockSupport-源码分析/","link":"","permalink":"https://liyong.ac.cn/2018/01/10/aa_category/se/source/LockSupport-源码分析/","excerpt":"","text":"代码版本1.8.0_131 sun.misc.Unsafe因为LockSupport的核心函数都是基于Unsafe类中定义的park和unpark函数，下面给出两个函数的定义。 public native void park(boolean isAbsolute, long time);public native void unpark(Thread thread); park函数，阻塞线程，在以下情况发生之前会一直阻塞 调用unpark函数 线程被中断 等待时间到了，并且，当time为绝对时间时，isAbsolute为true，否则，isAbsolute为false。当time为0时，表示无限等待，直到unpark发生。 unpark函数，释放线程的许可，即激活调用park后阻塞的线程。这个函数不是安全的，调用这个函数时要确保线程依旧存活。","categories":[{"name":"thread","slug":"thread","permalink":"https://liyong.ac.cn/categories/thread/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"},{"name":"source","slug":"source","permalink":"https://liyong.ac.cn/tags/source/"}]},{"title":"AbstractQueuedSynchronizer:源码分析","slug":"AbstractQueuedSynchronizer-源码分析","date":"2018-01-09T02:10:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/09/AbstractQueuedSynchronizer-源码分析/","link":"","permalink":"https://liyong.ac.cn/2018/01/09/AbstractQueuedSynchronizer-源码分析/","excerpt":"","text":"代码版本1.8.0_131 概述AbstractQueuedSynchronizer主要提供了两类功能，共享锁和独占锁，所有的子类，要么使用了共享锁，要么使用了独占锁，没有任何类同时使用的情况。即使ReentrantReadWriteLock，也是通过两个子类实现的。 NodewaitStatus CANCELLED = 1 This node is cancelled due to timeout or interrupt. SIGNAL = -1 The successor of this node is (or will soon be) blocked (via park), so the current node must unpark its successor when it releases or cancels CONDITION = -2 This node is currently on a condition queue. It will not be used as a sync queue node until transferred, at which time the status will be set to 0. PROPAGATE = -3 A releaseShared should be propagated to other nodes. This is set (for head node only) in doReleaseShared to ensure propagation continues, even if other operations have since intervened. AbstractQueuedSynchronizer.addWaiter(Node)返回新增的节点 1234567891011121314private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125; AbstractQueuedSynchronizer.enq(Node)给队列尾添加节点，并返回之前的尾 123456789101112131415161718private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // 队列为空创建一个标志结点作为head结点，同时tail结点也指向head if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; // 当前节点的prev指向tail node.prev = t; // 设置当前结点为tail节点 if (compareAndSetTail(t, node)) &#123; t.next = node; // 返回tail节点 return t; &#125; &#125; &#125;&#125; AbstractQueuedSynchronizer.acquireQueued(Node, int)从队列里面获取资源，只有当前节点的前置节点为head,获取资源以后把当前节点从队列中移除 123456789101112131415161718192021222324252627282930313233final boolean acquireQueued(final Node node, int arg) &#123; // 标记是否成功获取资源 boolean failed = true; try &#123; // 标记是否被中断 boolean interrupted = false; // 自旋（死循环） for (;;) &#123; // 拿到当前节点的前置节点 final Node p = node.predecessor(); // 如果前置节点是head，那么当前节点有资格获取资源 if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 当前节点指向head setHead(node); // 当前节点从链中山粗，help GC p.next = null; // help GC failed = false; return interrupted; &#125; // 如果可以休息了，就到了waiting状态，直到被unpark if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) &#123; // 如果等待中被中断了，哪怕一次，也会将 interrupted 标记为true interrupted = true; &#125; &#125; &#125; finally &#123; if (failed) &#123; // 取消获取资源 cancelAcquire(node); &#125; &#125;&#125; AbstractQueuedSynchronizer.shouldParkAfterFailedAcquire(Node, Node)当前线程获取资源失败以后是否需要阻塞 12345678910111213141516171819202122232425private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 前置节点是SIGNAL状态（当前置节点使用完资源以后会通知后续的节点），则当前节点可以安全休息 return true; if (ws &gt; 0) &#123; /* * Predecessor was cancelled. Skip over predecessors and indicate retry. * 如果前置节点废弃了，那就一直往前找，一直找到状态正常的节点 */ do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; /* * waitStatus must be 0 or PROPAGATE. Indicate that we need a signal, but don't * park yet. Caller will need to retry to make sure it cannot acquire before * parking. * 如果前置节点正常，就把状态设置为SIGNAL，高速它拿完号通知我，有可能失败，人家可能刚刚释放完 */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; AbstractQueuedSynchronizer.parkAndCheckInterrupt()当前线程处于wating状态，检查是否被中断 123456private final boolean parkAndCheckInterrupt() &#123; // 让当前线程处于等待状态 LockSupport.park(this); // 如果当前线程被唤醒，查看是否被中断的（park会让当前线程处于waiting状态，唤醒线程的方式有 unpark 或者 interrupt） return Thread.interrupted();&#125; AbstractQueuedSynchronizer.release(int)12345678910public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) // 唤醒等待队列里面下一个线程 unparkSuccessor(h); return true; &#125; return false;&#125; AbstractQueuedSynchronizer.unparkSuccessor(Node)唤醒等待的线程 1234567891011121314151617181920212223242526private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try to clear in * anticipation of signalling. It is OK if this fails or if status is changed by * waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally just the next node. * But if cancelled or apparently null, traverse backwards from tail to find the * actual non-cancelled successor. * 如果当前节点的下个节点为空，或者状态为取消 */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125;","categories":[{"name":"thread","slug":"thread","permalink":"https://liyong.ac.cn/categories/thread/"}],"tags":[{"name":"thread","slug":"thread","permalink":"https://liyong.ac.cn/tags/thread/"},{"name":"源码分析","slug":"源码分析","permalink":"https://liyong.ac.cn/tags/源码分析/"}]},{"title":"CyclicBarrier:源码分析","slug":"CyclicBarrier-源码分析","date":"2018-01-09T01:46:33.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/09/CyclicBarrier-源码分析/","link":"","permalink":"https://liyong.ac.cn/2018/01/09/CyclicBarrier-源码分析/","excerpt":"代码版本1.8.0_131 域123456private final ReentrantLock lock = new ReentrantLock();//所有方法都通过这个锁来同步。 private final Condition trip = lock.newCondition();//通过lock得到的一个状态变量private final int parties;//通过构造器传入的参数，表示总的等待线程的数量。private int count;//初始化和parties一致，parties不参与运算，count参与运算private final Runnable barrierCommand;//当屏障正常打开后运行的程序，通过最后一个调用await的线程来执行。private Generation generation = new Generation();当前的Generation。每当屏障失效或者开闸之后都会自动替换掉。从而实现重置的功能。","text":"代码版本1.8.0_131 域123456private final ReentrantLock lock = new ReentrantLock();//所有方法都通过这个锁来同步。 private final Condition trip = lock.newCondition();//通过lock得到的一个状态变量private final int parties;//通过构造器传入的参数，表示总的等待线程的数量。private int count;//初始化和parties一致，parties不参与运算，count参与运算private final Runnable barrierCommand;//当屏障正常打开后运行的程序，通过最后一个调用await的线程来执行。private Generation generation = new Generation();当前的Generation。每当屏障失效或者开闸之后都会自动替换掉。从而实现重置的功能。 CyclicBarrier.await()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public int await() throws InterruptedException, BrokenBarrierException &#123; try &#123; return dowait(false, 0L); &#125; catch (TimeoutException toe) &#123; throw new Error(toe); // cannot happen &#125; &#125; /** * Main barrier code, covering the various policies. */ private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; final Generation g = generation; //如果当前Generation是处于打破状态则传播这个BrokenBarrierExcption if (g.broken) throw new BrokenBarrierException(); if (Thread.interrupted()) &#123; breakBarrier(); throw new InterruptedException(); &#125; int index = --count;// tripped//如果当前状态将为0，则Generation处于开闸状态。运行可能存在的command，设置下一个Generation。相当于每次开闸之后都进行了一次reset。 if (index == 0) &#123; // tripped boolean ranAction = false; try &#123; final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; nextGeneration(); return 0; &#125; finally &#123; if (!ranAction) breakBarrier(); &#125; &#125; // loop until tripped, broken, interrupted, or timed out for (;;) &#123; try &#123; if (!timed) trip.await(); else if (nanos &gt; 0L) nanos = trip.awaitNanos(nanos); &#125; catch (InterruptedException ie) &#123; if (g == generation &amp;&amp; ! g.broken) &#123; breakBarrier(); throw ie; &#125; else &#123; // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // \"belong\" to subsequent execution. Thread.currentThread().interrupt(); &#125; &#125; if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed &amp;&amp; nanos &lt;= 0L) &#123; breakBarrier(); throw new TimeoutException(); &#125; &#125; &#125; finally &#123; lock.unlock(); &#125; &#125;","categories":[{"name":"thread","slug":"thread","permalink":"https://liyong.ac.cn/categories/thread/"}],"tags":[{"name":"thread","slug":"thread","permalink":"https://liyong.ac.cn/tags/thread/"},{"name":"源码分析","slug":"源码分析","permalink":"https://liyong.ac.cn/tags/源码分析/"}]},{"title":"NIO:Selector","slug":"NIO-Selector","date":"2018-01-02T09:50:27.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2018/01/02/NIO-Selector/","link":"","permalink":"https://liyong.ac.cn/2018/01/02/NIO-Selector/","excerpt":"","text":"之前进行socket编程时，accept方法会一直阻塞，直到有客户端请求的到来，并返回socket进行相应的处理。整个过程是流水线的，处理完一个请求，才能去获取并处理后面的请求，当然也可以把获取socket和处理socket的过程分开，一个线程负责accept，一个线程池负责处理请求","categories":[],"tags":[]},{"title":"Oracle:awrrpt","slug":"Oracle-awrrpt","date":"2017-12-28T08:48:27.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/28/Oracle-awrrpt/","link":"","permalink":"https://liyong.ac.cn/2017/12/28/Oracle-awrrpt/","excerpt":"把目录切换到生成报告的地址 12sqlplus / as sysdba;@?/rdbms/admin/awrrpt.sql","text":"把目录切换到生成报告的地址 12sqlplus / as sysdba;@?/rdbms/admin/awrrpt.sql AWR 报告分析","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"oracle","slug":"oracle","permalink":"https://liyong.ac.cn/tags/oracle/"}]},{"title":"常用工具","slug":"常用工具","date":"2017-12-28T03:09:36.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/28/常用工具/","link":"","permalink":"https://liyong.ac.cn/2017/12/28/常用工具/","excerpt":"","text":"Postman 模拟请求，前后端联调利器 有APP版和chrome浏览器插件版redis-desktop-managerredis 管理工作SecureCRTWindows下登录UNIX或Linux服务器主机的软件。fiddler抓包利器clcl 复制粘贴 CLCL is clipboard caching utilityLaunchyLaunchy indexes the programs in your start menu and can launch your documents, project files, folders, and bookmarks with just a few keystrokes!","categories":[{"name":"工具","slug":"工具","permalink":"https://liyong.ac.cn/categories/工具/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://liyong.ac.cn/tags/工具/"}]},{"title":"Oracle:性能分析及优化","slug":"Oracle-性能分析及优化","date":"2017-12-28T02:21:16.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/28/Oracle-性能分析及优化/","link":"","permalink":"https://liyong.ac.cn/2017/12/28/Oracle-性能分析及优化/","excerpt":"书写 SQL语句转换成大写，可缩短些SQL的解析时间。通过统一成大写，可提高SQL的再利用率，缩短SQL解析时间。 1234--高效的写法SELECT * FROM A_TABLE--低效的写法select * from a_table","text":"书写 SQL语句转换成大写，可缩短些SQL的解析时间。通过统一成大写，可提高SQL的再利用率，缩短SQL解析时间。 1234--高效的写法SELECT * FROM A_TABLE--低效的写法select * from a_table 涉及到多表检索时，明确地为每个字段指定表名,通过在A_TABLE、B_TABLE中指定别名｢A｣、｢B｣,就可不再需要调查A_ITEM、B_ITEM、A_KEY、B_KEY是哪儿个表中的项目,从而缩短SQL解析时间 12345678--高效的写法： SELECT A.A_ITEM, B.B_ITEM FROM A_TABLE A, B_TABLE B WHERE A.A_KEY = B.B_KEY;--低效的写法： SELECT A_ITEM FROM A_TABLE, B_TABLE WHERE A_KEY = B_KEY; ORACLE在解析的过程中, 会将’*’ 依次转换成所有的列名, 这个工作是通过查询数据字典完成的, 这意味着将耗费更多的时间 1234高效的写法： SELECT AAC001,AAC002,AAC003 FROM AC01;低效的写法： SELECT * FROM AC01; 条件 ORACLE的解析器按照从右到左的顺序处理FROM子句中的表名,因此FROM子句中写在最后的表(基础表 driving table)将被最先处理. 在FROM子句中包含多个表的情况下,你必须选择记录条数最少的表作为基础表.当ORACLE处理多个表时, 会运用排序及合并的方式连接它们.首先,扫描第一个表(FROM子句中最后的那个表)并对记录进行排序,然后扫描第二个表(FROM子句中最后第二个表),最后将所有从第二个表中检索出的记录与第一个表中合适记录进行合并 12345678--表 ac01有 16,384 条记录 --表 ab01 有1 条记录--高效的写法：SELECT COUNT(*) FROM AC01,AB01 ;--执行时间0.96秒--低效的写法：SELECT COUNT(*) FROM AB01,AC01 ;--执行时间26.09秒 只在基于规则的优化器rule中有效。ORACLE建议按此方式书写。减少多表关联:表关联的越多，查询速度就越慢，尽量减少多个表的关联，建议表关联不要超过3个（子查询也属于表关联）。数据转换上会存在大数据量表的关联，关联多了会影响索引的效率，可以采用建立临时表的办法，有时更能提高速度. ORACLE采用自下而上的顺序解析WHERE子句,根据这个原理,表之间的连接必须写在其他WHERE条件之前, 那些可以过滤掉最大数量记录的条件必须写在WHERE子句的末尾 1234--高效的写法：SELECT * FROM AC01 WHERE AAC004='1' AND AAC003='周东芝';--低效的写法：SELECT * FROM AC01 WHERE AAC003='周东芝' AND AAC004='1'; ORACLE采用自下而上的顺序解析WHERE子句,根据这个原理,表之间的连接必须写在其他WHERE条件之前。 12345678910--高效的写法：SELECT ab01.aab001,ab02.aab051 FROM ab01,ab02 WHERE ab01.aab001=ab02.aab001 AND ab02.aae140=’31’;--低效的写法：SELECT ab01.aab001,ab02.aab051 FROM ab01,ab02 WHERE ab02.aae140=’31’ AND ab01.aab001=ab02.aab001; 不要通过LIKE运算来执行中间一致或后方一致的检索 1234SELECT * FROM AC01 WHERE AAC003 LIKE '梁海%';低效的写法：SELECT * FROM AC01 WHERE AAC003 LIKE '%梁海';SELECT * FROM AC01 WHERE AAC003 LIKE '%梁海%'; 索引索引列，不要使用”NOT”、”!=”、”&lt;&gt;”比较运算 show parameter memory alter system set memory_target=4g scope=spfile; shutdown immediate show parameter sga alter system register;","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"oracle","slug":"oracle","permalink":"https://liyong.ac.cn/tags/oracle/"}]},{"title":"Oracle:index","slug":"Oracle-index","date":"2017-12-28T01:28:49.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/28/Oracle-index/","link":"","permalink":"https://liyong.ac.cn/2017/12/28/Oracle-index/","excerpt":"索引建立在排序的基础上 若没有索引，搜索某个记录时（例如查找name=’wish’）需要搜索所有的记录，因为不能保证只有一个wish，必须全部搜索一遍 一个表能建多少个索引一张表推荐多少个字段，如果多了，会有什么问题","text":"索引建立在排序的基础上 若没有索引，搜索某个记录时（例如查找name=’wish’）需要搜索所有的记录，因为不能保证只有一个wish，必须全部搜索一遍 一个表能建多少个索引一张表推荐多少个字段，如果多了，会有什么问题 索引类别B树索引列基数（列不重复值的个数）大时适合使用B数索引 位图索引对于基数小的列适合简历位图索引（例如性别等） 单列索引和复合索引函数索引12create index fbi on student (upper(name));select * from student where upper(name) ='WISH' 索引建立原则总结如果有两个或者以上的索引，其中有一个唯一性索引，而其他是非唯一，这种情况下oracle将使用唯一性索引而完全忽略非唯一性索引 至少要包含组合索引的第一列（即如果索引建立在多个列上，只有它的第一个列被where子句引用时，优化器才会使用该索引）小表不要建立索引经常进行连接查询的列应该创建索引使用create index时要将最常查询的列放在最前面限制表中索引的数量（创建索引耗费时间，并且随数据量的增大而增大；索引会占用物理空间；当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，降低了数据的维护速度） 注意事项1234567select * from student where not (score=100);select * from student where score &lt;&gt; 100;--替换为select * from student where score&gt;100 or score &lt;100 先执行From -&gt;Where -&gt;Group By-&gt;Order By执行From 子句是从右往左进行执行。因此必须选择记录条数最少的表放在右边","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"oracle","slug":"oracle","permalink":"https://liyong.ac.cn/tags/oracle/"}]},{"title":"spring:mvc","slug":"spring-mvc","date":"2017-12-24T15:30:45.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/24/spring-mvc/","link":"","permalink":"https://liyong.ac.cn/2017/12/24/spring-mvc/","excerpt":"","text":"初始化 外部容器传给springmvc初始化事件 以分散重写的手段初始化 初始化spring ioc 初始化spring mvc 九大对象的初始化","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"},{"name":"mvc","slug":"mvc","permalink":"https://liyong.ac.cn/tags/mvc/"}]},{"title":"Netty:入门","slug":"Netty-入门","date":"2017-12-24T14:25:01.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/24/Netty-入门/","link":"","permalink":"https://liyong.ac.cn/2017/12/24/Netty-入门/","excerpt":"Netty是一个高性能、异步事件驱动的NIO框架，提供了对TCP、UDP和文件传输的支持，作为一个异步NIO框架，Netty的所有IO操作都是异步非阻塞的，通过Future-Listener机制，用户可以方便的主动获取或者通过通知机制获得IO操作结果。","text":"Netty是一个高性能、异步事件驱动的NIO框架，提供了对TCP、UDP和文件传输的支持，作为一个异步NIO框架，Netty的所有IO操作都是异步非阻塞的，通过Future-Listener机制，用户可以方便的主动获取或者通过通知机制获得IO操作结果。 Reactor模型Reator模型主要由多路复用器(Acceptor)、事件分发器（Dispatcher）、事件处理器（Handler） 单线程模型所有I/O操作都在一个线程上，即多路复用、事件分发和处理都是在一个Reactor线程上完成的三层协议协议PiplineReactor 传输服务协议支持核心 架构分析Reactor通信调度层 模型 ——》NIO监听网络读写连接调度业务处理NioSockertChannelByteBuffer池化支持，用用手动切换标志位，零拷贝 Pipline 职责链条传递拦截处理向前向后事件外部传入的消息对象，有POJO信息抽象，上层只需要处理逻辑 构建逻辑业务处理层 ServerChannelFactoryNioServerSocketChannelFactory Executor bossExecutorExecutor workerExecutororg.jboss.netty.channel.Channels.pipeline(ChannelHandler…) 优点总结 api使用简单，封装完善，开发门槛低功能强大，预置了多种编码解码功能，多种主流协议支持定制能力强，可以对channelHander对框架的灵活扩展性能高，Reator线程模型调度+ChannelFuture+Listener,通过Listener机制主动推送结果 典型的网络事件如下：链路注册链路激活链路端口接收到请求消息求求消息接受并处理完毕发送应答消息链路发生异常发生用户自定义事件","categories":[{"name":"netty","slug":"netty","permalink":"https://liyong.ac.cn/categories/netty/"}],"tags":[{"name":"netty","slug":"netty","permalink":"https://liyong.ac.cn/tags/netty/"}]},{"title":"Mysql:index","slug":"Mysql-index","date":"2017-12-22T01:48:07.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/22/Mysql-index/","link":"","permalink":"https://liyong.ac.cn/2017/12/22/Mysql-index/","excerpt":"概要 索引需要存储，也就需要空间，索引实际就是一张表，字段更行会有性能损耗 btree","text":"概要 索引需要存储，也就需要空间，索引实际就是一张表，字段更行会有性能损耗 btree Innodb聚集索引有事务适合写多，读少 MyISAM非聚集所以没有事务适合读多，写少 建索引原则推荐 频繁作为where条件的字段 关联字段可以建索引，例如外键 排序字段，例如order by name,group by(先排序，后分组) 不适合 where条件用不到的 频繁更新的字段 数据值发布比较均匀的不适合建索引，例如男女，这家 表达数据可以确定行数，二期数据量很少 其他## 索引失效 123456789user: id,name,ageindex(name,age) 复合索引select * from user order by age 索引失效select * from user where age=19 and name='lee' 索引失效index(age,name) 复合索引select * from user order by age 索引有效select * from user where age=19 and name='lee' 索引有效 Btree hashMySql常用30种SQL查询语句优化方法 应尽量避免在 where 子句中使用!=或&lt;&gt; where 及 order by 涉及的列上建立索引 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描 尽量避免在 where 子句中使用 or 来连接条件如：1select id from t where num=10 or num=20 可以这样查询： 123select id from t where num=10union allselect id from t where num=20 in 和 not in 也要慎用对于连续的数值，能用 between 就不要用 in 了应尽量避免在 where 子句中对字段进行表达式操作 12select id from t where num/2=100 Nselect id from t where num=100*2 Y 应尽量避免在where子句中对字段进行函数操作 12select id from t where substring(name,1,3)=’abc’–name以abc开头的id Nselect id from t where name like ‘abc%’ Y 在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使 用，并且应尽可能的让字段顺序与索引顺序相一致 当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段 sex，male、female几乎各一半 索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。 尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会 逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://liyong.ac.cn/tags/mysql/"},{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"}]},{"title":"JIRA","slug":"JIRA","date":"2017-12-18T08:48:31.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/18/JIRA/","link":"","permalink":"https://liyong.ac.cn/2017/12/18/JIRA/","excerpt":"","text":"项目版本状态 未发布发布构建并发布归档","categories":[{"name":"项目管理","slug":"项目管理","permalink":"https://liyong.ac.cn/categories/项目管理/"}],"tags":[{"name":"项目管理","slug":"项目管理","permalink":"https://liyong.ac.cn/tags/项目管理/"}]},{"title":"生产者消费者","slug":"生产者消费者","date":"2017-12-12T09:55:23.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/12/生产者消费者/","link":"","permalink":"https://liyong.ac.cn/2017/12/12/生产者消费者/","excerpt":"生产者消费者的事例代码","text":"生产者消费者的事例代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package demo1.resume;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.atomic.AtomicInteger;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.ReentrantLock; public class StoreHouse &#123; static ReentrantLock lock = new ReentrantLock(); static Condition notFull = lock.newCondition(); static Condition notEmpty = lock.newCondition(); static BlockingQueue&lt;Integer&gt; queue = new ArrayBlockingQueue&lt;Integer&gt;(10); static AtomicInteger ai = new AtomicInteger(); static int capacity = 10; public static class Producer implements Runnable &#123; @Override public void run() &#123; try &#123; lock.lockInterruptibly(); while (true) &#123; if (queue.size() == capacity) &#123; notFull.await(); &#125; Thread.sleep(1000); int incrementAndGet = ai.incrementAndGet(); queue.put(incrementAndGet); System.out.println(\"生产面包\" + incrementAndGet + \" 当前仓库总数\" + queue.size()); notEmpty.signal(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; if (lock.isHeldByCurrentThread()) &#123; lock.unlock(); &#125; &#125; &#125; &#125; public static class Consumer implements Runnable &#123; @Override public void run() &#123; try &#123; lock.lockInterruptibly(); while (true) &#123; if (queue.size() == 0) &#123; notEmpty.await(); &#125; Thread.sleep(1000); Integer take = queue.take(); System.out.println(\"消费面包\" + take + \" 当前仓库总数\" + queue.size()); notFull.signal(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; if (lock.isHeldByCurrentThread()) &#123; lock.unlock(); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; Producer producer = new Producer(); Consumer consumer = new Consumer(); ExecutorService pool = Executors.newFixedThreadPool(50); for (int i = 0; i &lt; 50; i++) &#123; if (i % 2 == 0) &#123; pool.execute(consumer); &#125; else &#123; pool.execute(producer); &#125; &#125; pool.shutdown(); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://liyong.ac.cn/categories/设计模式/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"},{"name":"设计模式","slug":"设计模式","permalink":"https://liyong.ac.cn/tags/设计模式/"}]},{"title":"spring:annotation","slug":"spring-annotation","date":"2017-12-07T13:11:25.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/07/spring-annotation/","link":"","permalink":"https://liyong.ac.cn/2017/12/07/spring-annotation/","excerpt":"@Autowired12345678910111213141516171819202122public class MovieRecommender &#123;@Autowiredprivate MovieCatalog[] movieCatalogs;// ...&#125;public class MovieRecommender &#123;private Set&lt;MovieCatalog&gt; movieCatalogs;@Autowiredpublic void setMovieCatalogs(Set&lt;MovieCatalog&gt; movieCatalogs) &#123;this.movieCatalogs = movieCatalogs;&#125;// ...&#125;public class MovieRecommender &#123;private Map&lt;String, MovieCatalog&gt; movieCatalogs;@Autowiredpublic void setMovieCatalogs(Map&lt;String, MovieCatalog&gt; movieCatalogs) &#123;this.movieCatalogs = movieCatalogs; &#125;&#125;","text":"@Autowired12345678910111213141516171819202122public class MovieRecommender &#123;@Autowiredprivate MovieCatalog[] movieCatalogs;// ...&#125;public class MovieRecommender &#123;private Set&lt;MovieCatalog&gt; movieCatalogs;@Autowiredpublic void setMovieCatalogs(Set&lt;MovieCatalog&gt; movieCatalogs) &#123;this.movieCatalogs = movieCatalogs;&#125;// ...&#125;public class MovieRecommender &#123;private Map&lt;String, MovieCatalog&gt; movieCatalogs;@Autowiredpublic void setMovieCatalogs(Map&lt;String, MovieCatalog&gt; movieCatalogs) &#123;this.movieCatalogs = movieCatalogs; &#125;&#125; 自动引入容器中的对象You can also use @Autowired for interfaces that are well-known resolvable dependencies: BeanFactory , ApplicationContext , Environment , ResourceLoader ,ApplicationEventPublisher , and MessageSource . These interfaces and their extended interfaces,such as ConfigurableApplicationContext or ResourcePatternResolver , are automatically resolved, with no special setup necessary 1234567public class MovieRecommender &#123;@Autowiredprivate ApplicationContext context;public MovieRecommender() &#123;&#125;// ...&#125; @Primary123456789@Configurationpublic class MovieConfiguration &#123;@Bean@Primary //如果只需要注入一个bean,这个优先注入public MovieCatalog firstMovieCatalog() &#123; ... &#125;@Beanpublic MovieCatalog secondMovieCatalog() &#123; ... &#125;// ...&#125; @Qualifier","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"},{"name":"annotation","slug":"annotation","permalink":"https://liyong.ac.cn/tags/annotation/"}]},{"title":"Spring:lifecycle","slug":"Spring-lifecycle","date":"2017-12-07T11:12:27.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/07/Spring-lifecycle/","link":"","permalink":"https://liyong.ac.cn/2017/12/07/Spring-lifecycle/","excerpt":"Lifecycle12345public interface Lifecycle &#123;void start();void stop();boolean isRunning();&#125; 1234public interface LifecycleProcessor extends Lifecycle &#123;void onRefresh();void onClose();&#125;","text":"Lifecycle12345public interface Lifecycle &#123;void start();void stop();boolean isRunning();&#125; 1234public interface LifecycleProcessor extends Lifecycle &#123;void onRefresh();void onClose();&#125; 优先级 主键 接口实现 全局配置的方法名If multiple lifecycle mechanisms are configured for a bean, and each mechanism is configured with a different method name, then each configured method is executed in the order listed below. However, if the same method name is configured - for example,init() for an initialization method - for more than one of these lifecycle mechanisms,that method is executed once, as explained in the preceding section. You can also use @Autowired for interfaces that are well-known resolvable dependencies:BeanFactory , ApplicationContext , Environment , ResourceLoader ,ApplicationEventPublisher , and MessageSource . These interfaces and their extended interfaces,such as ConfigurableApplicationContext or ResourcePatternResolver , are automaticallyresolved, with no special setup necessary","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"lifecycle","slug":"lifecycle","permalink":"https://liyong.ac.cn/tags/lifecycle/"},{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"}]},{"title":"Spring:Modules","slug":"Spring-Modules","date":"2017-12-05T09:28:34.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/12/05/Spring-Modules/","link":"","permalink":"https://liyong.ac.cn/2017/12/05/Spring-Modules/","excerpt":"摘抄自Spring Framework Reference Documentation 4.3.13.RELEASE","text":"摘抄自Spring Framework Reference Documentation 4.3.13.RELEASE Core Container The spring-core and spring-beans modules provide the fundamental parts of the framework, The Context ( spring-context ) module builds on the solid base provided by the Core and Beans spring-context-support provides support for integrating common third-party libraries into a Spring application context for caching (EhCache, Guava, JCache) The spring-expression module provides a powerful Expression Language for querying andmanipulating an object graph at runtime. AOP and Instrumentation The spring-aop module provides an AOP Alliance-compliant aspect-oriented programming The separate *spring-aspects *module provides integration with AspectJ. The spring-instrument module provides class instrumentation support and classloader implementations to be used in certain application servers. The spring-instrument-tomcat module contains Spring’s instrumentation agent for Tomcat MessagingSpring Framework 4 includes a spring-messaging module with key abstractions from the Spring Integration project such as Message , MessageChannel ,MessageHandler , and others to serve as a foundation for messaging-based applications. Data Access/Integration The spring-jdbc module provides a JDBC-abstraction layer that removes the need to do tedious JDBC coding and parsing of database-vendor specific error codes. The spring-tx module supports programmatic and declarative transaction management for classes The spring-orm module provides integration layers for popular object-relational mapping APIs,including JPA, JDO, and Hibernate. The spring-oxm module provides an abstraction layer that supports Object/XML mapping implementations such as JAXB, Castor, XMLBeans, JiBX and XStream The spring-jms module (Java Messaging Service) contains features for producing and consumingmessages. Web The ** ** module provides basic web-oriented integration features such as multipart file upload functionality and the initialization of the IoC container using Servlet listeners and a web-oriented application context. It also contains an HTTP client and the web-related parts of Spring’s remoting support. The spring-webmvc module (also known as the Web-Servlet module) contains Spring’s model-viewcontroller (MVC) and REST Web Services implementation for web applications. The spring-webmvc-portlet module (also known as the Web-Portlet module) provides the MVC implementation to be used in a Portlet environment and mirrors the functionality of the Servlet-based spring-webmvc module.TestThe spring-test module supports the unit testing and integration testing of Spring components withJUnit or TestNG","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"}]},{"title":"Mybatis:返回值","slug":"Mybatis-返回值","date":"2017-11-29T01:51:44.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/29/Mybatis-返回值/","link":"","permalink":"https://liyong.ac.cn/2017/11/29/Mybatis-返回值/","excerpt":"从源码分析mybatis怎么处理返回值","text":"从源码分析mybatis怎么处理返回值 PreparedStatementHandler.query开始分析调用栈123456789/** * 传入的resultHandler为空，没有使用 */@Override public &lt;E&gt; List&lt;E&gt; PreparedStatementHandler.query(Statement statement, ResultHandler resultHandler) throws SQLException &#123; PreparedStatement ps = (PreparedStatement) statement; ps.execute(); return resultSetHandler.&lt;E&gt; handleResultSets(ps); &#125; DefaultResultSetHandler.handleResultSets分析 DefaultResultSetHandler.handleResultSets(Statement) DefaultResultSetHandler.getFirstResultSet(Statement)，根据Statement-&gt;ResultSet-&gt;ResultSetWrapper,简单来说就是把ResultSet包装成ResultSetWrapper DefaultResultSetHandler.handleResultSet(ResultSetWrapper, ResultMap, List, ResultMapping) DefaultResultSetHandler.handleRowValues(ResultSetWrapper, ResultMap, ResultHandler&lt;?&gt;, RowBounds, ResultMapping) DefaultResultSetHandler.handleRowValuesForSimpleResultMap(ResultSetWrapper, ResultMap, ResultHandler&lt;?&gt;, RowBounds, ResultMapping) DefaultResultSetHandler.skipRows(ResultSet, RowBounds)跳过小于Offset的数据 DefaultResultSetHandler.resolveDiscriminatedResultMap(ResultSet, ResultMap, String)方法处理返回结果包含Discriminated的场景 DefaultResultSetHandler.getRowValue(ResultSetWrapper, ResultMap) DefaultResultSetHandler.applyAutomaticMappings(ResultSetWrapper, ResultMap, MetaObject, String) DefaultResultSetHandler.createAutomaticMappings(ResultSetWrapper, ResultMap, MetaObject, String) ResultSetWrapper.getUnmappedColumnNames(ResultMap, String) ResultSetWrapper.loadMappedAndUnmappedColumnNames(ResultMap, String)加载映射过的列和没有映射过的列 DefaultResultSetHandler.createResultObject(ResultSetWrapper, ResultMap, ResultLoaderMap, String) DefaultResultSetHandler.createResultObject(ResultSetWrapper, ResultMap, List&lt;Class&lt;?&gt;&gt;, List, String)使用objectFactory DefaultResultSetHandler.handleResultSet分析1234567891011121314151617private void handleResultSet(ResultSetWrapper rsw, ResultMap resultMap, List&lt;Object&gt; multipleResults, ResultMapping parentMapping) throws SQLException &#123; try &#123; if (parentMapping != null) &#123; handleRowValues(rsw, resultMap, null, RowBounds.DEFAULT, parentMapping); &#125; else &#123; if (resultHandler == null) &#123; DefaultResultHandler defaultResultHandler = new DefaultResultHandler(objectFactory); handleRowValues(rsw, resultMap, defaultResultHandler, rowBounds, null); multipleResults.add(defaultResultHandler.getResultList()); &#125; else &#123; handleRowValues(rsw, resultMap, resultHandler, rowBounds, null); &#125; &#125; &#125; finally &#123; closeResultSet(rsw.getResultSet()); &#125;&#125; DefaultResultSetHandler.applyAutomaticMappings分析12345678910111213141516171819private boolean applyAutomaticMappings(ResultSetWrapper rsw, ResultMap resultMap, MetaObject metaObject, String columnPrefix) throws SQLException &#123; //创建没有映射字段的自动映射 List&lt;UnMappedColumnAutoMapping&gt; autoMapping = createAutomaticMappings(rsw, resultMap, metaObject, columnPrefix); boolean foundValues = false; if (autoMapping.size() &gt; 0) &#123; for (UnMappedColumnAutoMapping mapping : autoMapping) &#123; //使用typeHandler，根据列名和ResultSet查找对应的值 final Object value = mapping.typeHandler.getResult(rsw.getResultSet(), mapping.column); if (value != null) &#123; foundValues = true; &#125; if (value != null || (configuration.isCallSettersOnNulls() &amp;&amp; !mapping.primitive)) &#123; //对象的对应属性赋值 metaObject.setValue(mapping.property, value); &#125; &#125; &#125; return foundValues;&#125; DefaultResultSetHandler.createAutomaticMappings分析1234567891011121314151617181920212223242526272829private List&lt;UnMappedColumnAutoMapping&gt; createAutomaticMappings(ResultSetWrapper rsw, ResultMap resultMap, MetaObject metaObject, String columnPrefix) throws SQLException &#123; final String mapKey = resultMap.getId() + \":\" + columnPrefix; List&lt;UnMappedColumnAutoMapping&gt; autoMapping = autoMappingsCache.get(mapKey); if (autoMapping == null) &#123; autoMapping = new ArrayList&lt;UnMappedColumnAutoMapping&gt;(); //查找没有映射的字段 final List&lt;String&gt; unmappedColumnNames = rsw.getUnmappedColumnNames(resultMap, columnPrefix); for (String columnName : unmappedColumnNames) &#123; String propertyName = columnName; final String property = metaObject.findProperty(propertyName, configuration.isMapUnderscoreToCamelCase()); if (property != null &amp;&amp; metaObject.hasSetter(property)) &#123; //定位字段的类型 final Class&lt;?&gt; propertyType = metaObject.getSetterType(property); if (typeHandlerRegistry.hasTypeHandler(propertyType, rsw.getJdbcType(columnName))) &#123; //根据JavaType和jdbcType确定TypeHandler final TypeHandler&lt;?&gt; typeHandler = rsw.getTypeHandler(propertyType, columnName); //添加到UnMappedColumnAutoMapping集合中 autoMapping.add(new UnMappedColumnAutoMapping(columnName, property, typeHandler, propertyType.isPrimitive())); &#125; &#125; else &#123; // 根据配置参数 NONE, WARNING, FAILING 做出相应的测落 configuration.getAutoMappingUnknownColumnBehavior() .doAction(mappedStatement, columnName, (property != null) ? property : propertyName, null); &#125; &#125; autoMappingsCache.put(mapKey, autoMapping); &#125; return autoMapping; &#125; DefaultResultSetHandler.storeObject DefaultResultSetHandler.storeObject-&gt;DefaultResultSetHandler.callResultHandler-&gt;ResultHandler.handleResult 方法实现把查询的对象存放到ResultHandler","categories":[],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"},{"name":"return","slug":"return","permalink":"https://liyong.ac.cn/tags/return/"}]},{"title":"Mybatis:Plugin","slug":"Mybatis-Plugin","date":"2017-11-29T00:59:02.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/29/Mybatis-Plugin/","link":"","permalink":"https://liyong.ac.cn/2017/11/29/Mybatis-Plugin/","excerpt":"从源码分析Mybatis的插件 熟悉Java代理模式的应用场景","text":"从源码分析Mybatis的插件 熟悉Java代理模式的应用场景 概述 Configuration持有对象InterceptorChain 通过Configuration的addInterceptor注入拦截器（插件） 需要拦截的对象（Executor 、ParameterHandler 、ResultSetHandler 、StatementHandler ）创建完毕以后通过调用interceptorChain.pluginAll（target）对应的代理对象 进行实际执行的时候触发的就是代理对象 获取ParameterHandler的代理对象1234567891011121314151617181920212223242526272829303132333435363738394041//创建target对象parameterHandlerParameterHandler parameterHandler = mappedStatement.getLang().createParameterHandler(mappedStatement, parameterObject, boundSql); //获取target对应的proxy对象 parameterHandler = (ParameterHandler) interceptorChain.pluginAll(parameterHandler); /** *进行遍历拦截，从源码上分析，同一个对象如果被多个插件拦截，最后插入的优先级最高 */ public Object pluginAll(Object target) &#123; for (Interceptor interceptor : interceptors) &#123; target = interceptor.plugin(target); &#125; return target; &#125; /** *返回当前对象代理对象 */@Override public Object PageInterceptor.plugin(Object target) &#123; return Plugin.wrap(target, this); &#125; /** * 根据接口返回代理对象 */ public static Object Plugin.wrap(Object target, Interceptor interceptor) &#123; //根据@Intercepts、@Signature获取要代理的接口以及对应的方法 Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = getSignatureMap(interceptor); Class&lt;?&gt; type = target.getClass(); //查看当前类型是否有对应的接口 Class&lt;?&gt;[] interfaces = getAllInterfaces(type, signatureMap); //如果有就返回代理对象，否则返回target本身 if (interfaces.length &gt; 0) &#123; return Proxy.newProxyInstance( type.getClassLoader(), interfaces, new Plugin(target, interceptor, signatureMap)); &#125; return target; &#125; Plugin分析 所有target返回的代理对象为Plugin对象 Plugin只有要拦截的target，已经拦截器interceptor，以及要拦截target的哪些方法 12345678910111213141516public class Plugin implements InvocationHandler &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; try &#123; Set&lt;Method&gt; methods = signatureMap.get(method.getDeclaringClass()); //判断是否需要拦截目标对象的此方法，如果需要进行拦截，如果没必要直接调用target的对应方法 if (methods != null &amp;&amp; methods.contains(method)) &#123; //实际处理逻辑留给interceptor的实现类处理 return interceptor.intercept(new Invocation(target, method, args)); &#125; return method.invoke(target, args); &#125; catch (Exception e) &#123; throw ExceptionUtil.unwrapThrowable(e); &#125; &#125;&#125; 接口Interceptor分析123456789101112/** * 拦截器 */public interface Interceptor &#123; //拦截用到的方法 //Invocation对象包含target,method,args Object intercept(Invocation invocation) throws Throwable; //返回target对应的Proxy Object plugin(Object target); //初始化的时候注入一些额外需要的属性 void setProperties(Properties properties);&#125; 总结俗话说的好学以致用，通过Plugin的分析，对于同事或者领导说的给代码留个口子，有一定的体会","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"},{"name":"plugin","slug":"plugin","permalink":"https://liyong.ac.cn/tags/plugin/"}]},{"title":"Mybatis:Select","slug":"Mybatis-Select","date":"2017-11-28T07:20:09.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/28/Mybatis-Select/","link":"","permalink":"https://liyong.ac.cn/2017/11/28/Mybatis-Select/","excerpt":"分析Mapper查询的执行过程 有关Mapper的构建过程请参考Mybatis-Builder","text":"分析Mapper查询的执行过程 有关Mapper的构建过程请参考Mybatis-Builder Mapper接口适配SqlSession接口 session.getMapper获取到代理类 MapperProxy org.apache.ibatis.binding.MapperProxy.invoke(Object, Method, Object[]) org.apache.ibatis.binding.MapperMethod.execute(SqlSession, Object[]) 解析参数org.apache.ibatis.binding.MapperMethod.MethodSignature.convertArgsToSqlCommandParam(Object[]) org.apache.ibatis.session.SqlSession.selectOne(String, Object) 参数解析ParamNameResolver构造函数逻辑 首先通过@Param解析参数index对应的参数name 其次通过反射解析参数index对应的参数name 最后设置name=index ParamNameResolver.getNamedParams(Object[])逻辑12345678910111213141516171819202122232425public Object getNamedParams(Object[] args) &#123; final int paramCount = names.size(); //如果参数为空直接返回 if (args == null || paramCount == 0) &#123; return null; //如果没有参数注解且参数个数为1返回第一个参数 &#125; else if (!hasParamAnnotation &amp;&amp; paramCount == 1) &#123; return args[names.firstKey()]; &#125; else &#123; final Map&lt;String, Object&gt; param = new ParamMap&lt;Object&gt;(); int i = 0; // names=[0=id,1=name] or names=[0=0,1=0] for (Map.Entry&lt;Integer, String&gt; entry : names.entrySet()) &#123; //entry.getValue()=id,entry.getKey()=0 ,args[entry.getKey()]=idValue param.put(entry.getValue(), args[entry.getKey()]); final String genericParamName = GENERIC_NAME_PREFIX + String.valueOf(i + 1); // ensure not to overwrite parameter named with @Param if (!names.containsValue(genericParamName)) &#123; param.put(genericParamName, args[entry.getKey()]); &#125; i++; &#125; return param; &#125;&#125; SqlSeesion执行过程SqlSeesion-&gt;Executor-&gt;StatementHandler-&gt;Statement org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(String, Object) org.apache.ibatis.executor.CachingExecutor.query( , Object, RowBounds, ResultHandler) 根据BoundSql获取BoundSql 创建CacheKey CachingExecutor.query(MappedStatement, Object, RowBounds, ResultHandler, CacheKey, BoundSql) BaseExecutor.query(MappedStatement, Object, RowBounds, ResultHandler, CacheKey, BoundSql) BaseExecutor.queryFromDatabase(MappedStatement, Object, RowBounds, ResultHandler, CacheKey, BoundSql) SimpleExecutor.doQuery(MappedStatement, Object, RowBounds, ResultHandler, BoundSql) 创建StatementHandler 准备Statement 获取Connection 初始化Statement 设置超时 设置,stmt.setFetchSize 设置用户传入的参数 org.apache.ibatis.executor.statement.StatementHandler.update(Statement) 123456789101112@Override public &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException &#123; Statement stmt = null; try &#123; Configuration configuration = ms.getConfiguration(); StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql); stmt = prepareStatement(handler, ms.getStatementLog()); return handler.&lt;E&gt;query(stmt, resultHandler); &#125; finally &#123; closeStatement(stmt); &#125; &#125; 创建StatementHandler 创建RoutingStatementHandler 根据StatementType选择实际的PreparedStatementHandler PreparedStatementHandler构造器调用BaseStatementHandler的构造器 创建parameterHandler 创建resultSetHandler 设置用户传入的参数 org.apache.ibatis.executor.statement.StatementHandler.parameterize(Statement) org.apache.ibatis.executor.statement.RoutingStatementHandler.parameterize(Statement) org.apache.ibatis.executor.statement.PreparedStatementHandler.parameterize(Statement) org.apache.ibatis.scripting.defaults.DefaultParameterHandler.setParameters(PreparedStatement) org.apache.ibatis.type.BaseTypeHandler.setParameter(PreparedStatement, int, T, JdbcType) org.apache.ibatis.type.UnknownTypeHandler.setNonNullParameter(PreparedStatement, int, Object, JdbcType) org.apache.ibatis.type.BaseTypeHandler.setParameter(PreparedStatement, int, T, JdbcType) 返回结果处理 org.apache.ibatis.executor.SimpleExecutor.doQuery(MappedStatement, Object, RowBounds, ResultHandler, BoundSql) org.apache.ibatis.executor.statement.RoutingStatementHandler.query(Statement, ResultHandler) org.apache.ibatis.executor.statement.PreparedStatementHandler.query(Statement, ResultHandler) org.apache.ibatis.executor.resultset.DefaultResultSetHandler.handleResultSets(Statement) 123456789/** * 传入的resultHandler为空，没有使用 */@Override public &lt;E&gt; List&lt;E&gt; PreparedStatementHandler.query(Statement statement, ResultHandler resultHandler) throws SQLException &#123; PreparedStatement ps = (PreparedStatement) statement; ps.execute(); return resultSetHandler.&lt;E&gt; handleResultSets(ps); &#125; 详情请参考Mybatis:返回值","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"},{"name":"select","slug":"select","permalink":"https://liyong.ac.cn/tags/select/"}]},{"title":"Mybatis:Configuration分析说明","slug":"Mybatis-Configuration分析说明","date":"2017-11-28T06:08:29.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/28/Mybatis-Configuration分析说明/","link":"","permalink":"https://liyong.ac.cn/2017/11/28/Mybatis-Configuration分析说明/","excerpt":"","text":"//namespace为cache-ref所在的mapper的namespace，referencedNamespace是引用的空间 cacheRefMap.put(namespace, referencedNamespace); useActualParamName：jdk8以后可以使用，使用实际的方法参数名 //插件链InterceptorChain interceptorChain = new InterceptorChain();","categories":[],"tags":[]},{"title":"Mybatis:Lifecycle","slug":"Mybatis-Lifecycle","date":"2017-11-28T01:40:21.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/28/Mybatis-Lifecycle/","link":"","permalink":"https://liyong.ac.cn/2017/11/28/Mybatis-Lifecycle/","excerpt":"","text":"LifecycleSqlSessionFactoryBuilder构建完SqlSessionFactory生命结束 SqlSessionFactoryOnce created, the SqlSessionFactory should exist for the duration of your application execution. SqlSessionEach thread should have its own instance of SqlSession. Instances of SqlSession are not to be shared and are not thread safe.Therefore the best scope is request or method scope. Mapper Instances和SqlSession一致或者更小","categories":[],"tags":[{"name":"lifecycle","slug":"lifecycle","permalink":"https://liyong.ac.cn/tags/lifecycle/"}]},{"title":"Mybatis:spring","slug":"Mybatis-spring","date":"2017-11-27T07:20:15.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/27/Mybatis-spring/","link":"","permalink":"https://liyong.ac.cn/2017/11/27/Mybatis-spring/","excerpt":"","text":"sqlSessionFactoryBean创建sqlSessionFactoryB","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"},{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"}]},{"title":"Mybatis:Cache","slug":"Mybatis-Cache","date":"2017-11-27T05:44:29.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/27/Mybatis-Cache/","link":"","permalink":"https://liyong.ac.cn/2017/11/27/Mybatis-Cache/","excerpt":"Mybatis的缓存分为一级缓存和二级缓存Cache 就是 Map","text":"Mybatis的缓存分为一级缓存和二级缓存Cache 就是 Map 获取会话DefaultSqlSessionFactory.openSession()DefaultSqlSessionFactory.openSessionFromDataSource() 12345678private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); &#125; Configuration.newExecutor(Transaction, ExecutorType) 12345678910111213141516171819public Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; if (cacheEnabled) &#123; // 只有cacheEnabled==true才可以使用二级缓存 //此种写法使用了设计模式之装饰模式 executor = new CachingExecutor(executor); &#125; executor = (Executor) interceptorChain.pluginAll(executor); return executor;&#125; 相关知识查询先看二级缓存，再看一级缓存] MappedStatement.cache的构建CacheNamespace1234567891011121314151617181920@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface CacheNamespace &#123; Class&lt;? extends org.apache.ibatis.cache.Cache&gt; implementation() default PerpetualCache.class; Class&lt;? extends org.apache.ibatis.cache.Cache&gt; eviction() default LruCache.class; long flushInterval() default 0; int size() default 1024; boolean readWrite() default true; boolean blocking() default false; /** * 1、这个属性通过反射机制实现了，不同实现类设置不同的属性 * */ Property[] properties() default &#123;&#125;;&#125; 使用CacheNamespace构建过程 MapperAnnotationBuilder.parseCache解析CacheNamespace注解 MapperBuilderAssistant.useNewCache CacheBuilder的构建 1234567891011121314151617public Cache build() &#123; setDefaultImplementations(); Cache cache = newBaseCacheInstance(implementation, id); // 需要着重关注 setCacheProperties(cache); // issue #352, do not apply decorators to custom caches if (PerpetualCache.class.equals(cache.getClass())) &#123; for (Class&lt;? extends Cache&gt; decorator : decorators) &#123; cache = newCacheDecoratorInstance(decorator, cache); setCacheProperties(cache); &#125; cache = setStandardDecorators(cache); &#125; else if (!LoggingCache.class.isAssignableFrom(cache.getClass())) &#123; cache = new LoggingCache(cache); &#125; return cache;&#125; CacheNamespace.properties的应用场景 12345678910111213141516171819private void setCacheProperties(Cache cache) &#123; if (properties != null) &#123; MetaObject metaCache = SystemMetaObject.forObject(cache); for (Map.Entry&lt;Object, Object&gt; entry : properties.entrySet()) &#123; String name = (String) entry.getKey(); String value = (String) entry.getValue(); if (metaCache.hasSetter(name)) &#123; Class&lt;?&gt; type = metaCache.getSetterType(name); if (String.class == type) &#123; metaCache.setValue(name, value); &#125; else if (int.class == type || Integer.class == type) &#123; metaCache.setValue(name, Integer.valueOf(value)); &#125; else &#123; throw new CacheException(\"Unsupported property type for cache: '\" + name + \"' of type \" + type); &#125; &#125; &#125; &#125; @CacheNamespaceReforg.apache.ibatis.builder.annotation.MapperAnnotationBuilder.parseCacheRef()org.apache.ibatis.builder.MapperBuilderAssistant.useCacheRef(String) 12345678public Cache useCacheRef(String namespace) &#123; unresolvedCacheRef = true; // 关键 Cache cache = configuration.getCache(namespace); currentCache = cache; unresolvedCacheRef = false; return cache;&#125; 一级缓存一级缓存称为本地缓存，属于sqlSession级别的缓存，在一个SqlSession内有效SqlSession:Executor:Cache=1:1:1 BaseExecutor12345678910111213141516171819202122@SuppressWarnings(\"unchecked\") @Override public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; List&lt;E&gt; list; try &#123; //如果一级缓存中存在则从一级缓存中获取，否则从数据库中获取 list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null; if (list != null) &#123; handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); &#125; else &#123; list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); &#125; &#125; finally &#123; queryStack--; &#125; // 通过设置LocalCacheScope.STATEMENT可以去掉一级缓存 if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123; // issue #482 clearLocalCache(); &#125; return list; &#125; 清除一级缓存session.clearCache()只会清除一级缓存去掉一级缓存 localCacheScope设置成STATEMENT 二级缓存二级缓存的存放路径MappedStatement-&gt;SynchronizedCache-&gt;LoggingCache-&gt;SerializedCache-&gt;LruCache-&gt;PerpetualCache-&gt;Map 概念 二级缓存是全局缓存 二级缓存基于namespace或则mapper 如果会话关闭就好把一级缓存中的数据保存到二级缓存 条件 开启二级缓存配置cacheEnabled mapper中配置cache POJO需要实现序列化接口 select.useCache设置二级缓存 sql.flushCache会清理一级缓存和二级缓存都清空 select.flushCache则不会使用缓存 一级缓存转化二级缓存 SqlSession的commit或者close方法触发CachingExecutor对应的commit和close方法 CachingExecutor的commit或者close方法触发TransactionalCacheManager.commit方法 触发TransactionalCache.commit方法 触发TransactionalCache.flushPendingEntries 123456789101112private void flushPendingEntries() &#123; for (Map.Entry&lt;Object, Object&gt; entry : entriesToAddOnCommit.entrySet()) &#123; //delegate==MappedStatement.SynchronizedCache //每每看到这，感叹设计之巧妙，牛 delegate.putObject(entry.getKey(), entry.getValue()); &#125; for (Object entry : entriesMissedInCache) &#123; if (!entriesToAddOnCommit.containsKey(entry)) &#123; delegate.putObject(entry, null); &#125; &#125;&#125; 整合Ecache1、导入第三方包2、导入适配包3、mapper.xml中引用缓存 xml 注解优先级","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"},{"name":"cache","slug":"cache","permalink":"https://liyong.ac.cn/tags/cache/"}]},{"title":"Mybatis:通用Mapper入口解析","slug":"Mybatis-通用Mapper入口解析","date":"2017-11-24T05:48:43.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/24/Mybatis-通用Mapper入口解析/","link":"","permalink":"https://liyong.ac.cn/2017/11/24/Mybatis-通用Mapper入口解析/","excerpt":"通用Mapper是怎么适配Mybatis的","text":"通用Mapper是怎么适配Mybatis的 mapperHelper.registerMapper分析 tk.mybatis.mapper.mapperhelper.MapperHelper.registerMapper(Class&lt;?&gt;) tk.mybatis.mapper.mapperhelper.MapperHelper.fromMapperClass(Class&lt;?&gt;)，注册Mapper 和 MapperTemplate之间的映射关系 如果有父接口递归调用MapperHelper.registerMapper mapperHelper.processConfiguration 分析 tk.mybatis.mapper.mapperhelper.MapperHelper.processConfiguration(Configuration) tk.mybatis.mapper.mapperhelper.MapperHelper.processConfiguration(Configuration, Class&lt;?&gt;) tk.mybatis.mapper.mapperhelper.MapperHelper.setSqlSource(MappedStatement) tk.mybatis.mapper.mapperhelper.MapperTemplate.setSqlSource(MappedStatement) tk.mybatis.mapper.mapperhelper.MapperTemplate.createSqlSource(MappedStatement, String) tk.mybatis.mapper.mapperhelper.MapperTemplate.setSqlSource(MappedStatement, SqlSource) MapperTemplate.setSqlSource(MappedStatement)分析1234567891011121314public void setSqlSource(MappedStatement ms) throws Exception &#123; //根据方法名找到方法类对象 Method method = methodMap.get(getMethodName(ms)); try &#123; //调用MapperTemplate的对应方法，返回XML形式的sql String xmlSql = (String) method.invoke(this, ms); // 根据XML形式的sql生成SqlSource对象 SqlSource sqlSource = createSqlSource(ms, xmlSql); //替换原有的SqlSource setSqlSource(ms, sqlSource); checkCache(ms); &#125; catch (IllegalAccessException e) &#123; throw new MapperException(e); &#125; &#125; MapperTemplate.createSqlSource分析对象持有关系 DocumentBuilderFactory-&gt;DocumentBuilder-&gt;Document XMLLanguageDriver-&gt;XPathParser-&gt;XPath XMLScriptBuilder-&gt;XNode MapperTemplate生成的sql123456789101112131415&lt;script&gt; &lt;bind name=\"id_cache\" value=\"id\"/&gt;INSERT INTO &lt;if test=\"@tk.mybatis.mapper.util.OGNL@isDynamicParameter(_parameter) and dynamicTableName != null and dynamicTableName != ''\"&gt;$&#123;dynamicTableName&#125;&lt;/if&gt;&lt;if test=\"@tk.mybatis.mapper.util.OGNL@isNotDynamicParameter(_parameter) or dynamicTableName == null or dynamicTableName == ''\"&gt;blog&lt;/if&gt; &lt;trim prefix=\"(\" suffix=\")\" suffixOverrides=\",\"&gt;id,name,&lt;/trim&gt;&lt;trim prefix=\"VALUES(\" suffix=\")\" suffixOverrides=\",\"&gt; &lt;if test=\"id_cache != null\"&gt;#&#123;id_cache,javaType=java.lang.String&#125;, &lt;/if&gt;&lt;if test=\"id_cache == null\"&gt;#&#123;id,javaType=java.lang.String&#125;, &lt;/if&gt;&lt;if test=\"name != null\"&gt;#&#123;name,javaType=java.lang.String&#125;,&lt;/if&gt; &lt;if test=\"name == null\"&gt;#&#123;name,javaType=java.lang.String&#125;,&lt;/if&gt;&lt;/trim&gt;&lt;/script&gt; 调用栈 org.apache.ibatis.scripting.xmltags.XMLLanguageDriver.createSqlSource(Configuration, String, Class&lt;?&gt;) 创建org.apache.ibatis.parsing.XPathParser(String, boolean, Properties, EntityResolver)对象 org.apache.ibatis.scripting.xmltags.XMLLanguageDriver.createSqlSource(Configuration, XNode, Class&lt;?&gt;) org.apache.ibatis.scripting.xmltags.XMLScriptBuilder.parseScriptNode() org.apache.ibatis.scripting.xmltags.XMLScriptBuilder.parseDynamicTags(XNode) XMLScriptBuilder.parseScriptNode 分析123456789101112 public SqlSource parseScriptNode() &#123; List&lt;SqlNode&gt; contents = parseDynamicTags(context); MixedSqlNode rootSqlNode = new MixedSqlNode(contents); SqlSource sqlSource = null;// 根据isDynamic创建的SqlSource if (isDynamic) &#123; sqlSource = new DynamicSqlSource(configuration, rootSqlNode); &#125; else &#123; sqlSource = new RawSqlSource(configuration, rootSqlNode, parameterType); &#125; return sqlSource; &#125; XMLScriptBuilder.parseDynamicTags(XNode)1234567891011121314151617181920212223List&lt;SqlNode&gt; parseDynamicTags(XNode node) &#123; List&lt;SqlNode&gt; contents = new ArrayList&lt;SqlNode&gt;(); NodeList children = node.getNode().getChildNodes(); for (int i = 0; i &lt; children.getLength(); i++) &#123; XNode child = node.newXNode(children.item(i)); if (child.getNode().getNodeType() == Node.CDATA_SECTION_NODE || child.getNode().getNodeType() == Node.TEXT_NODE) &#123; String data = child.getStringBody(\"\"); TextSqlNode textSqlNode = new TextSqlNode(data); if (textSqlNode.isDynamic()) &#123; contents.add(textSqlNode); isDynamic = true; &#125; else &#123; contents.add(new StaticTextSqlNode(data)); &#125; &#125; else if (child.getNode().getNodeType() == Node.ELEMENT_NODE) &#123; // issue #628 String nodeName = child.getNode().getNodeName(); NodeHandler handler = nodeHandlers(nodeName); handler.handleNode(child, contents); isDynamic = true; &#125; &#125; return contents;&#125; MapperTemplate.setSqlSource(MappedStatement, SqlSource)分析1234567891011protected void setSqlSource(MappedStatement ms, SqlSource sqlSource) &#123; MetaObject msObject = SystemMetaObject.forObject(ms);//通过反射替换sqlSource msObject.setValue(\"sqlSource\", sqlSource); //如果是Jdbc3KeyGenerator，就设置为MultipleJdbc3KeyGenerator KeyGenerator keyGenerator = ms.getKeyGenerator(); if (keyGenerator instanceof Jdbc3KeyGenerator) &#123; //通过反射keyGenerator msObject.setValue(\"keyGenerator\", new MultipleJdbc3KeyGenerator()); &#125; &#125; 总结 mapperHelper.registerMapper注册Mapper方法对于的MapperTemplate mapperHelper.processConfiguration处理 首先根据方法名找到对应MapperTemplate 执行MapperTemplate对应方法，创建SqlSource对象 替换原先的SqlSource对象对象","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"},{"name":"mapper","slug":"mapper","permalink":"https://liyong.ac.cn/tags/mapper/"}]},{"title":"Mybatis:Builder","slug":"Mybatis-Builder","date":"2017-11-24T03:04:31.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/24/Mybatis-Builder/","link":"","permalink":"https://liyong.ac.cn/2017/11/24/Mybatis-Builder/","excerpt":"通过源码分析的构建过程","text":"通过源码分析的构建过程 Annotation Builder概述 MapperMethod:MapperAnnotationBuilder:MapperBuilderAssistantMappedStatement.Builder:MappedStatement= 1:1：1:1:1 调用Configuration的添加Mapper的方法 Configuration-&gt;MapperRegistry-&gt;MapperAnnotationBuilder解析 解析方法所在Mapper全称对应的xml（loadXmlResource） 解析@CacheNamespace 解析@CacheNamespaceRef 构建MappedStatement 获取LanguageDriver 构建SqlSource 解析Options 解析SelectKey 解析ResultMap 生成MappedStatement 入口及传递1234//Configurationpublic &lt;T&gt; void addMapper(Class&lt;T&gt; type)public void addMappers(String packageName)public void addMappers(String packageName, Class&lt;?&gt; superType) Configuration-&gt;MapperRegistry-&gt;MapperAnnotationBuilder解析 123456789101112131415161718192021public void parse() &#123; String resource = type.toString(); if (!configuration.isResourceLoaded(resource)) &#123; loadXmlResource(); configuration.addLoadedResource(resource); assistant.setCurrentNamespace(type.getName()); parseCache(); parseCacheRef(); Method[] methods = type.getMethods(); for (Method method : methods) &#123; try &#123; if (!method.isBridge()) &#123; parseStatement(method); &#125; &#125; catch (IncompleteElementException e) &#123; configuration.addIncompleteMethod(new MethodResolver(this, method)); &#125; &#125; &#125; parsePendingMethods();&#125; 解析方法所在Mapper全称对应的xml123456789101112131415161718private void loadXmlResource() &#123; // Spring may not know the real resource name so we check a flag // to prevent loading again a resource twice // this flag is set at XMLMapperBuilder#bindMapperForNamespace if (!configuration.isResourceLoaded(\"namespace:\" + type.getName())) &#123; String xmlResource = type.getName().replace('.', '/') + \".xml\"; InputStream inputStream = null; try &#123; inputStream = Resources.getResourceAsStream(type.getClassLoader(), xmlResource); &#125; catch (IOException e) &#123; // ignore, resource is not required &#125; if (inputStream != null) &#123; XMLMapperBuilder xmlParser = new XMLMapperBuilder(inputStream, assistant.getConfiguration(), xmlResource, configuration.getSqlFragments(), type.getName()); xmlParser.parse(); &#125; &#125;&#125; 解析@CacheNamespace相关细节在缓存篇描述 解析@CacheNamespaceRef相关细节在缓存篇描述 构建MappedStatement获取LanguageDriver默认的是XMLLanguageDriver 构建SqlSource1234567891011121314151617181920private SqlSource getSqlSourceFromAnnotations(Method method, Class&lt;?&gt; parameterType, LanguageDriver languageDriver) &#123; try &#123; Class&lt;? extends Annotation&gt; sqlAnnotationType = getSqlAnnotationType(method); Class&lt;? extends Annotation&gt; sqlProviderAnnotationType = getSqlProviderAnnotationType(method); if (sqlAnnotationType != null) &#123; if (sqlProviderAnnotationType != null) &#123; throw new BindingException(\"You cannot supply both a static SQL and SqlProvider to method named \" + method.getName()); &#125; Annotation sqlAnnotation = method.getAnnotation(sqlAnnotationType); final String[] strings = (String[]) sqlAnnotation.getClass().getMethod(\"value\").invoke(sqlAnnotation); return buildSqlSourceFromStrings(strings, parameterType, languageDriver); &#125; else if (sqlProviderAnnotationType != null) &#123; Annotation sqlProviderAnnotation = method.getAnnotation(sqlProviderAnnotationType); return new ProviderSqlSource(assistant.getConfiguration(), sqlProviderAnnotation); &#125; return null; &#125; catch (Exception e) &#123; throw new BuilderException(\"Could not find value method on SQL annotation. Cause: \" + e, e); &#125;&#125; 解析Options解析SelectKey解析ResultMap生成MappedStatementMapperBuilderAssistant.addMappedStatement-&gt;MappedStatement.Builder-&gt;configuration.addMappedStatement XML Builder org.apache.ibatis.session.SqlSessionFactoryBuilder.build(InputStream) org.apache.ibatis.session.SqlSessionFactoryBuilder.build(InputStream, String, Properties) Configuration org.apache.ibatis.builder.xml.XMLConfigBuilder.parse() org.apache.ibatis.session.SqlSessionFactoryBuilder.build(Configuration) 分析XMLConfigBuilder.parse() XMLConfigBuilder-&gt;XPathParser-&gt;XPath org.apache.ibatis.builder.xml.XMLConfigBuilder.parse() org.apache.ibatis.builder.xml.XMLConfigBuilder.parseConfiguration(XNode) 1234567891011121314151617181920private void parseConfiguration(XNode root) &#123; try &#123; propertiesElement(root.evalNode(\"properties\")); Properties settings = settingsAsProperties(root.evalNode(\"settings\")); loadCustomVfs(settings); typeAliasesElement(root.evalNode(\"typeAliases\")); pluginElement(root.evalNode(\"plugins\")); objectFactoryElement(root.evalNode(\"objectFactory\")); objectWrapperFactoryElement(root.evalNode(\"objectWrapperFactory\")); reflectorFactoryElement(root.evalNode(\"reflectorFactory\")); settingsElement(settings); // read it after objectFactory and objectWrapperFactory issue #631 environmentsElement(root.evalNode(\"environments\")); databaseIdProviderElement(root.evalNode(\"databaseIdProvider\")); typeHandlerElement(root.evalNode(\"typeHandlers\")); mapperElement(root.evalNode(\"mappers\")); &#125; catch (Exception e) &#123; throw new BuilderException(\"Error parsing SQL Mapper Configuration. Cause: \" + e, e); &#125;&#125; 解析properties 属性url和resource不能同时存在 相同属性优先级由低到高 ChildrenProperties-&gt;resource(url)-&gt;Variables 注册别名 首先用xml中的alias 其次用@Alias的值 最后使用全限定类名 alias package方式解析mappers 获取配置的包名 调用org.apache.ibatis.session.Configuration.addMappers(String) class方式解析mapper 根据全限定接口名称获取对应的class对象 调用org.apache.ibatis.session.Configuration.addMapper(Class&lt;?&gt;) resource&amp;url方式解析mapper12&lt;mapper url=\"file:///var/mappers/PostMapper.xml\"/&gt;&lt;mapper resource=\"org/mybatis/builder/PostMapper.xml\"/&gt; 根据配置获取输入流 构建XMLMapperBuilder org.apache.ibatis.builder.xml.XMLMapperBuilder.parse() 重新解析的原因是某些依赖的元素还没有解析 12345678910111213141516171819202122232425262728public class XMLMapperBuilder extends BaseBuilder &#123; private XPathParser parser; private MapperBuilderAssistant builderAssistant; private Map&lt;String, XNode&gt; sqlFragments; private String resource; &#125; /** * 1、把解析失败的对象放到没有完成的集合中 * 2、解析完文件之后重新执行未完成集合的解析 * 3、如果解析失败不做任何处理 * 4、如果解析成功从集合中移除 */ public void parse() &#123; if (!configuration.isResourceLoaded(resource)) &#123; // 解析mapper configurationElement(parser.evalNode(\"/mapper\")); configuration.addLoadedResource(resource); // 1、通过namespace找Mapper // 2、然后调用org.apache.ibatis.session.Configuration.addMapper(Class&lt;?&gt;)，具体下文请参考**Annotation Builder** bindMapperForNamespace(); &#125; // 重新解析，之前解析失败的 ResultMap parsePendingResultMaps(); // 重新解析，之前解析失败的 cache-ref parsePendingCacheRefs(); // 重新解析，之前解析失败的 Statements parsePendingStatements(); &#125; 分析 configurationElement cache-ref和cache同时出现则使用cache 123456789101112131415161718private void configurationElement(XNode context) &#123; try &#123; String namespace = context.getStringAttribute(\"namespace\"); if (namespace == null || namespace.equals(\"\")) &#123; throw new BuilderException(\"Mapper's namespace cannot be empty\"); &#125; builderAssistant.setCurrentNamespace(namespace); //解析cache-ref cacheRefElement(context.evalNode(\"cache-ref\")); cacheElement(context.evalNode(\"cache\")); parameterMapElement(context.evalNodes(\"/mapper/parameterMap\")); resultMapElements(context.evalNodes(\"/mapper/resultMap\")); sqlElement(context.evalNodes(\"/mapper/sql\")); buildStatementFromContext(context.evalNodes(\"select|insert|update|delete\")); &#125; catch (Exception e) &#123; throw new BuilderException(\"Error parsing Mapper XML. Cause: \" + e, e); &#125;&#125; 构建SqlSource org.apache.ibatis.builder.xml.XMLMapperBuilder.buildStatementFromContext(List, String) org.apache.ibatis.builder.xml.XMLStatementBuilder.parseStatementNode() org.apache.ibatis.scripting.xmltags.XMLLanguageDriver.createSqlSource(Configuration, XNode, Class&lt;?&gt;) org.apache.ibatis.scripting.xmltags.XMLScriptBuilder.parseScriptNode() 创建RawSqlSource org.apache.ibatis.builder.SqlSourceBuilder.parse(String, Class&lt;?&gt;, Map&lt;String, Object&gt;) org.apache.ibatis.parsing.GenericTokenParser.parse(String)","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"},{"name":"builder","slug":"builder","permalink":"https://liyong.ac.cn/tags/builder/"}]},{"title":"Mybatis:主键生成","slug":"Mybatis-主键生成","date":"2017-11-21T08:44:19.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/21/Mybatis-主键生成/","link":"","permalink":"https://liyong.ac.cn/2017/11/21/Mybatis-主键生成/","excerpt":"主要分析mybatis默认的主键生成方式，以及我们怎么扩展","text":"主要分析mybatis默认的主键生成方式，以及我们怎么扩展 XMLStatementBuilder通过一下分析指导MappedStatement是怎么构建KeyGenerator的 1234567891011121314 public void parseStatementNode() &#123; KeyGenerator keyGenerator; String keyStatementId = id + SelectKeyGenerator.SELECT_KEY_SUFFIX; keyStatementId = builderAssistant.applyCurrentNamespace(keyStatementId, true); //根据key判断缓存中是否有对应的KeyGenerator if (configuration.hasKeyGenerator(keyStatementId)) &#123; keyGenerator = configuration.getKeyGenerator(keyStatementId); &#125; else &#123; //如果有属性useGeneratedKeys且是insert语句则使用Jdbc3KeyGenerator类 keyGenerator = context.getBooleanAttribute(\"useGeneratedKeys\", configuration.isUseGeneratedKeys() &amp;&amp; SqlCommandType.INSERT.equals(sqlCommandType)) ? Jdbc3KeyGenerator.INSTANCE : NoKeyGenerator.INSTANCE; &#125;&#125; KeyGenerator接口分析1234567public interface KeyGenerator &#123; void processBefore(Executor executor, MappedStatement ms, Statement stmt, Object parameter); void processAfter(Executor executor, MappedStatement ms, Statement stmt, Object parameter);&#125; Mybatis自带的实现类Jdbc3KeyGenerator 是通过数据的某些特性或者自增序列实现的 对于processBefore没有处理 NoKeyGenerator分析顾名思义，这个类不做任何处理，是空指针的一种解决方案 实现自定义的KeyGenerator 获取mapper对应的 MappedStatement 修改对应的 KeyGenerator属性 通用Mapper中对主键的扩展UUID通过Java生成的方式 @GeneratedValue(generator = “UUID”) 这种方式通过 标签实现，不能给对象设置对应的返回值1234567891011 // 如果generator = \"UUID\"，则设置uuid=true GeneratedValue generatedValue = field.getAnnotation(GeneratedValue.class); if (generatedValue.generator().equals(\"UUID\")) &#123; entityColumn.setUuid(true);// 如果列是uuid,则生成一个&lt;bind&gt;标签BaseInsertProvider.insert(MappedStatement)&#123; if (column.isUuid()) &#123; //uuid的情况，直接插入bind节点 sql.append(SqlHelper.getBindValue(column, getUUID())); &#125;&#125; 生成的xml, 123456789101112&lt;bind name=\"id_bind\" value='@java.util.UUID@randomUUID().toString().replace(\"-\", \"\")' /&gt;INSERT INTO blog&lt;trim prefix=\"(\" suffix=\")\" suffixOverrides=\",\"&gt;id,name,&lt;/trim&gt;&lt;trim prefix=\"VALUES(\" suffix=\")\" suffixOverrides=\",\"&gt; &lt;if test=\"id != null\"&gt;#&#123;id,javaType=java.lang.String&#125;,&lt;/if&gt; // 如果id==null,则会用id_bind的值插入数据库，但是不会给实体中返回对应的值 &lt;if test=\"id == null\"&gt;#&#123;id_bind,javaType=java.lang.String&#125;,&lt;/if&gt; &lt;if test=\"name != null\"&gt;#&#123;name,javaType=java.lang.String&#125;,&lt;/if&gt; &lt;if test=\"name == null\"&gt;#&#123;name,javaType=java.lang.String&#125;,&lt;/if&gt;&lt;/trim&gt; 通过DB生成的方式 @GeneratedValue(strategy = GenerationType.IDENTITY, generator = “selectreplace(uuid(), ‘-‘, ‘’)”) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 if (generatedValue.strategy() == GenerationType.IDENTITY) &#123; //mysql的自动增长 entityColumn.setIdentity(true); if (!generatedValue.generator().equals(\"\")) &#123; String generator = null; IdentityDialect identityDialect = IdentityDialect.getDatabaseDialect(generatedValue.generator()); if (identityDialect != null) &#123; generator = identityDialect.getIdentityRetrievalStatement(); &#125; else &#123; generator = generatedValue.generator(); &#125; entityColumn.setGenerator(generator); &#125; &#125;BaseInsertProvider.insert(MappedStatement)&#123;if (column.isIdentity()) &#123; newSelectKeyMappedStatement(ms, column); hasIdentityKey = true; &#125;&#125;//替换MappedStatement原有的keyGeneratorMapperTemplate.newSelectKeyMappedStatement(MappedStatement, EntityColumn)&#123; MappedStatement keyStatement = configuration.getMappedStatement(keyId, false); keyGenerator = new tk.mybatis.mapper.mapperhelper.SelectKeyGenerator.SelectKeyGenerator(MappedStatement, boolean); try &#123; configuration.addKeyGenerator(keyId, keyGenerator); &#125; catch (Exception e) &#123; //ignore &#125;&#125;BaseStatementHandler.generateKeys(Object) &#123; KeyGenerator keyGenerator = mappedStatement.getKeyGenerator(); ErrorContext.instance().store(); keyGenerator.processBefore(executor, mappedStatement, null, parameter); ErrorContext.instance().recall(); &#125;SelectKeyGenerator.processGeneratedKeys(Executor, MappedStatement, Object)&#123;if (parameter != null &amp;&amp; keyStatement != null &amp;&amp; keyStatement.getKeyProperties() != null) &#123; String[] keyProperties = keyStatement.getKeyProperties(); final Configuration configuration = ms.getConfiguration(); final MetaObject metaParam = configuration.newMetaObject(parameter); if (keyProperties != null) &#123; // Do not close keyExecutor. // The transaction will be closed by parent executor. Executor keyExecutor = configuration.newExecutor(executor.getTransaction(), ExecutorType.SIMPLE); List&lt;Object&gt; values = keyExecutor.query(keyStatement, parameter, RowBounds.DEFAULT, Executor.NO_RESULT_HANDLER); MetaObject metaResult = configuration.newMetaObject(values.get(0)); // 给实体中对应的属性赋值 setValue(metaParam, keyProperties[0], metaResult.getValue(keyProperties[0])); &#125;&#125; 通过Java给属性设置的方式1234567891011121314151617181920212223242526public class LeeMapperHelper extends MapperHelper &#123; public void processConfiguration(Configuration configuration, Class&lt;?&gt; mapperInterface) &#123; String prefix; if (mapperInterface != null) &#123; prefix = mapperInterface.getCanonicalName(); &#125; else &#123; prefix = \"\"; &#125; for (Object object : new ArrayList&lt;Object&gt;(configuration.getMappedStatements())) &#123; if (object instanceof MappedStatement) &#123; MappedStatement ms = (MappedStatement) object; if (ms.getId().startsWith(prefix) &amp;&amp; isMapperMethod(ms.getId())) &#123; if (ms.getSqlSource() instanceof ProviderSqlSource) &#123; setSqlSource(ms); MetaObject msObject = SystemMetaObject.forObject(ms); //通过反射修改KeyGenrator msObject.setValue(\"keyGenerator\", new UUIDKeyGenerator()); &#125; &#125; &#125; &#125; &#125;&#125; KeyGenerator的实现类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class UUIDKeyGenerator implements KeyGenerator &#123; private boolean executeBefore; public UUIDKeyGenerator() &#123; this.executeBefore = true; &#125; @Override public void processBefore(Executor executor, MappedStatement ms, Statement stmt, Object parameter) &#123; if (executeBefore) &#123; processGeneratedKeys(executor, ms, parameter); &#125; &#125; @Override public void processAfter(Executor executor, MappedStatement ms, Statement stmt, Object parameter) &#123; if (!executeBefore) &#123; processGeneratedKeys(executor, ms, parameter); &#125; &#125; private void processGeneratedKeys(Executor executor, MappedStatement ms, Object parameter) &#123; try &#123; final Configuration configuration = ms.getConfiguration(); final MetaObject metaParam = configuration.newMetaObject(parameter); String value = UUID.randomUUID().toString().replaceAll(\"-\", \"\")+\"lee\"; setValue(metaParam, \"id\", value); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private void setValue(MetaObject metaParam, String property, Object value) &#123; if (metaParam.hasSetter(property)) &#123; if (metaParam.hasGetter(property)) &#123; Object defaultValue = metaParam.getValue(property); if (defaultValue != null) &#123; return; &#125; &#125; metaParam.setValue(property, value); &#125; else &#123; throw new ExecutorException(\"No setter found for the keyProperty '\" + property + \"' in \" + metaParam.getOriginalObject().getClass().getName() + \".\"); &#125; &#125;&#125; 调用 1MapperHelper mapperHelper = new LeeMapperHelper();","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"},{"name":"id","slug":"id","permalink":"https://liyong.ac.cn/tags/id/"}]},{"title":"Mybatis：Others","slug":"Mybatis-Others","date":"2017-11-16T01:00:25.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/16/Mybatis-Others/","link":"","permalink":"https://liyong.ac.cn/2017/11/16/Mybatis-Others/","excerpt":"数据源事务","text":"数据源事务 怎么根据接口生成实现类使用java自带的反射生成 Configuration123public class Configuration &#123; protected final MapperRegistry mapperRegistry = new MapperRegistry(this);&#125; MapperRegistry1234public class MapperRegistry &#123; private final Configuration config; private final Map&lt;Class&lt;?&gt;, MapperProxyFactory&lt;?&gt;&gt; knownMappers = new HashMap&lt;Class&lt;?&gt;, MapperProxyFactory&lt;?&gt;&gt;();&#125; MapperProxyFactory1234567891011121314public class MapperProxyFactory&lt;T&gt; &#123; private final Class&lt;T&gt; mapperInterface; private final Map&lt;Method, MapperMethod&gt; methodCache = new ConcurrentHashMap&lt;Method, MapperMethod&gt;(); @SuppressWarnings(\"unchecked\") protected T newInstance(MapperProxy&lt;T&gt; mapperProxy) &#123; return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] &#123; mapperInterface &#125;, mapperProxy); &#125; public T newInstance(SqlSession sqlSession) &#123; final MapperProxy&lt;T&gt; mapperProxy = new MapperProxy&lt;T&gt;(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy); &#125; MapperProxy123456789101112131415161718192021public class MapperProxy&lt;T&gt; implements InvocationHandler, Serializable &#123; private static final long serialVersionUID = -6424540398559729838L; private final SqlSession sqlSession; private final Class&lt;T&gt; mapperInterface; private final Map&lt;Method, MapperMethod&gt; methodCache; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; final MapperMethod mapperMethod = cachedMapperMethod(method); return mapperMethod.execute(sqlSession, args); &#125; private MapperMethod cachedMapperMethod(Method method) &#123; MapperMethod mapperMethod = methodCache.get(method); if (mapperMethod == null) &#123; mapperMethod = new MapperMethod(mapperInterface, method, sqlSession.getConfiguration()); methodCache.put(method, mapperMethod); &#125; return mapperMethod; &#125;&#125; MapperMethod12345678910111213141516171819202122232425262728293031323334353637383940414243public class MapperMethod &#123; public Object execute(SqlSession sqlSession, Object[] args) &#123; Object result; switch (command.getType()) &#123; case INSERT: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.insert(command.getName(), param)); break; &#125; case UPDATE: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.update(command.getName(), param)); break; &#125; case DELETE: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.delete(command.getName(), param)); break; &#125; case SELECT: if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) &#123; executeWithResultHandler(sqlSession, args); result = null; &#125; else if (method.returnsMany()) &#123; result = executeForMany(sqlSession, args); &#125; else if (method.returnsMap()) &#123; result = executeForMap(sqlSession, args); &#125; else if (method.returnsCursor()) &#123; result = executeForCursor(sqlSession, args); &#125; else &#123; Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param); &#125; break; case FLUSH: result = sqlSession.flushStatements(); break; default: throw new BindingException(\"Unknown execution method for: \" + command.getName()); &#125; return result; &#125; 12 怎么同时加载xml和接口入口123String resource = \"mybatis-config.xml\";InputStream inputStream = Resources.getResourceAsStream(resource);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); SqlSessionFactoryBuilder123456789101112public SqlSessionFactory build(Reader reader) &#123; return build(reader, null, null); &#125; public SqlSessionFactory build(Reader reader, String environment, Properties properties) &#123; XMLConfigBuilder parser = new XMLConfigBuilder(reader, environment, properties); return build(parser.parse()); &#125;public SqlSessionFactory build(Configuration config) &#123; return new DefaultSqlSessionFactory(config); &#125; XMLConfigBuilder1234567891011121314151617181920212223242526272829303132333435363738394041424344public XMLConfigBuilder(InputStream inputStream, String environment, Properties props) &#123; this(new XPathParser(inputStream, true, props, new XMLMapperEntityResolver()), environment, props); &#125; private XMLConfigBuilder(XPathParser parser, String environment, Properties props) &#123; super(new Configuration()); ErrorContext.instance().resource(\"SQL Mapper Configuration\"); this.configuration.setVariables(props); this.parsed = false; this.environment = environment; this.parser = parser; &#125; public Configuration parse() &#123; if (parsed) &#123; throw new BuilderException(\"Each XMLConfigBuilder can only be used once.\"); &#125; parsed = true; parseConfiguration(parser.evalNode(\"/configuration\")); return configuration; &#125; private void parseConfiguration(XNode root) &#123; try &#123; //issue #117 read properties first propertiesElement(root.evalNode(\"properties\")); Properties settings = settingsAsProperties(root.evalNode(\"settings\")); loadCustomVfs(settings); typeAliasesElement(root.evalNode(\"typeAliases\")); pluginElement(root.evalNode(\"plugins\")); objectFactoryElement(root.evalNode(\"objectFactory\")); objectWrapperFactoryElement(root.evalNode(\"objectWrapperFactory\")); reflectorFactoryElement(root.evalNode(\"reflectorFactory\")); settingsElement(settings); // read it after objectFactory and objectWrapperFactory issue #631 environmentsElement(root.evalNode(\"environments\")); databaseIdProviderElement(root.evalNode(\"databaseIdProvider\")); typeHandlerElement(root.evalNode(\"typeHandlers\")); mapperElement(root.evalNode(\"mappers\")); &#125; catch (Exception e) &#123; throw new BuilderException(\"Error parsing SQL Mapper Configuration. Cause: \" + e, e); &#125; &#125; XMLMapperBuilder通过xml方式加载的时候会同时加载相关接口 1234567891011121314private void bindMapperForNamespace() &#123; String namespace = builderAssistant.getCurrentNamespace(); boundType = Resources.classForName(namespace); if (boundType != null) &#123; if (!configuration.hasMapper(boundType)) &#123; // Spring may not know the real resource name so we set a flag // to prevent loading again this resource from the mapper interface // look at MapperAnnotationBuilder#loadXmlResource configuration.addLoadedResource(\"namespace:\" + namespace); configuration.addMapper(boundType); &#125; &#125; &#125;&#125; MapperRegistry123456789101112131415161718public &lt;T&gt; void addMapper(Class&lt;T&gt; type) &#123; if (type.isInterface()) &#123; boolean loadCompleted = false; try &#123; knownMappers.put(type, new MapperProxyFactory&lt;T&gt;(type)); // It's important that the type is added before the parser is run // otherwise the binding may automatically be attempted by the // mapper parser. If the type is already known, it won't try. MapperAnnotationBuilder parser = new MapperAnnotationBuilder(config, type); parser.parse(); loadCompleted = true; &#125; finally &#123; if (!loadCompleted) &#123; knownMappers.remove(type); &#125; &#125; &#125;&#125; 通过这段代码知道加载mapper的时候会加载相同包下对应的xml123456789101112131415161718private void loadXmlResource() &#123; // Spring may not know the real resource name so we check a flag // to prevent loading again a resource twice // this flag is set at XMLMapperBuilder#bindMapperForNamespace if (!configuration.isResourceLoaded(\"namespace:\" + type.getName())) &#123; String xmlResource = type.getName().replace('.', '/') + \".xml\"; InputStream inputStream = null; try &#123; inputStream = Resources.getResourceAsStream(type.getClassLoader(), xmlResource); &#125; catch (IOException e) &#123; // ignore, resource is not required &#125; if (inputStream != null) &#123; XMLMapperBuilder xmlParser = new XMLMapperBuilder(inputStream, assistant.getConfiguration(), xmlResource, configuration.getSqlFragments(), type.getName()); xmlParser.parse(); &#125; &#125;&#125; 12 12 MapperAnnotationBuilder：Mapper 1:1CacheNamespace 相当于 mapper 下面的cacheCacheNamespaceRef 相当于 mapper 下面的 当mybatis的xml和注解同时存在的时候首先解析xml mybatisid生成策略 mybatis缓存 ParameterMapping 参数映射 默认的LanguageDrive是XMLLanguageDriverSqlSource 是没有传入参数的sql对象BoundSql 是传入参数的sql对象，是真正执行的对象Build模式一般应用于设置多个属性，而工厂模式应用于以下生成 RawSqlSource 和 StaticSqlSource 差不多 1、构建sqlSource主键参数的解析动态sql缓存 foreach executor, statementHandler,parameterHandler，resultHandler","categories":[],"tags":[]},{"title":"Mybatis:框架整理设计","slug":"Mybatis-框架整理设计","date":"2017-11-15T02:20:14.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/15/Mybatis-框架整理设计/","link":"","permalink":"https://liyong.ac.cn/2017/11/15/Mybatis-框架整理设计/","excerpt":"总体流程加载配置并初始化触发条件：加载配置文件配置来源于两个地方，一处是配置文件，一处是Java代码的注解，将SQL的配置信息加载成为一个个MappedStatement对象（包括了传入参数映射配置、执行的SQL语句、结果映射配置）存储到内存中 接收调用请求触发条件：调用Mybatis提供的API传入参数：SQL的ID和传入参数对象处理过程：将请求传递给下层的请求处理层进行处理 处理操作请求** 触发条件：API接口层传递请求过来**传入参数：SQL的ID和传入参数对象处理过程： 根据SQL的ID查找对应的MappedStatement对象 据传入参数对象解析MappedStatement对象，得到最终要执行的SQL和执行传入参数 获取数据库连接，根据得到的最终SQL语句和执行传入参数到数据库执行，并得到执行结果 根据MappedStatement对象中的结果映射配置对得到的执行结果进行转换处理，并得到最终的处理结果 释放连接资源 返回处理结果将最终的处理结果返回","text":"总体流程加载配置并初始化触发条件：加载配置文件配置来源于两个地方，一处是配置文件，一处是Java代码的注解，将SQL的配置信息加载成为一个个MappedStatement对象（包括了传入参数映射配置、执行的SQL语句、结果映射配置）存储到内存中 接收调用请求触发条件：调用Mybatis提供的API传入参数：SQL的ID和传入参数对象处理过程：将请求传递给下层的请求处理层进行处理 处理操作请求** 触发条件：API接口层传递请求过来**传入参数：SQL的ID和传入参数对象处理过程： 根据SQL的ID查找对应的MappedStatement对象 据传入参数对象解析MappedStatement对象，得到最终要执行的SQL和执行传入参数 获取数据库连接，根据得到的最终SQL语句和执行传入参数到数据库执行，并得到执行结果 根据MappedStatement对象中的结果映射配置对得到的执行结果进行转换处理，并得到最终的处理结果 释放连接资源 返回处理结果将最终的处理结果返回 Mybatis的几个主要部件 SqlSession：作为主要顶层API,表示和数据库交互的会话，完成必要的增删改查 Executor:Mybatis执行器，是核心的调度器，负责SQL语句的生成和查询缓存的维护 StatementHandler:封装了JDBC Statement的操作，如设置参数等 1234567891011121314151617public interface StatementHandler &#123; Statement prepare(Connection connection, Integer transactionTimeout) throws SQLException; void parameterize(Statement statement) throws SQLException; void batch(Statement statement) throws SQLException; int update(Statement statement) throws SQLException; &lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) throws SQLException; &lt;E&gt; Cursor&lt;E&gt; queryCursor(Statement statement) throws SQLException; BoundSql getBoundSql(); ParameterHandler getParameterHandler();&#125; ParameterHandler:负责将用户传递的参数转换成JDBC Stament所需要的参数 12345public interface ParameterHandler &#123; Object getParameterObject(); void setParameters(PreparedStatement ps) throws SQLException;&#125; ResultSetHandler:负责将JDBC返回的ResultSet结果集对象转换成List类型的集合 123456public interface ResultSetHandler &#123; &lt;E&gt; List&lt;E&gt; handleResultSets(Statement stmt) throws SQLException; &lt;E&gt; Cursor&lt;E&gt; handleCursorResultSets(Statement stmt) throws SQLException; void handleOutputParameters(CallableStatement cs) throws SQLException;&#125; TypeHandler：负责Java数据类型和jdbc数据类型之间的映射和转换 Sqlsource:负责根据用户传递的parameterObject,动态生成SQL语句，将信息封装到BoundSql对象中 BoundSql：表示动态的SQL语句一级相应的参数信息 Configuration：维护所有的配置信息","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"}]},{"title":"Mybatis:从JDBC到Mybatis","slug":"Mybatis-从JDBC到Mybatis","date":"2017-11-14T11:30:03.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/14/Mybatis-从JDBC到Mybatis/","link":"","permalink":"https://liyong.ac.cn/2017/11/14/Mybatis-从JDBC到Mybatis/","excerpt":"JDBC实现查询分析 加载JDBC驱动 获取数据库连接 创建Statement对象 设置SQL语句的传入参数 执行SQL语句 处理返回结果 释放资源","text":"JDBC实现查询分析 加载JDBC驱动 获取数据库连接 创建Statement对象 设置SQL语句的传入参数 执行SQL语句 处理返回结果 释放资源 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465Connection conn = null;Statement stmt = null;ResultSet rs = null;String url = \"jdbc:mysql://localhost:3306/test?\" + \"user=root&amp;password=&amp;useUnicode=true&amp;characterEncoding=UTF8\";String sql;try &#123; // 加载JDBC驱动 Class.forName(\"com.mysql.jdbc.Driver\");// System.out.println(\"成功加载MySQL驱动程序\"); // 获取数据库连接 conn = DriverManager.getConnection(url); stmt = conn.createStatement(); try &#123; sql = \"drop table student\"; stmt.executeUpdate(sql); &#125; catch (Exception e) &#123; &#125; sql = \"create table student(NO char(20),name varchar(20),primary key(NO))\"; stmt.executeUpdate(sql);// System.out.println(\"创建数据表成功能\"); sql = \"insert into student(NO,name) values('2012001','测试1')\"; stmt.executeUpdate(sql); sql = \"select * from student where NO= ? \"; // 创建Statement对象（） PreparedStatement prepareStatement = conn.prepareStatement(sql); // 设置传入参数 prepareStatement.setString(1, \"2012001\"); // 执行SQL语句 rs = prepareStatement.executeQuery(); System.out.println(\"学号\\t姓名\"); // 处理查询结果 while (rs.next()) &#123; System.out.println(rs.getString(1) + \"\\t\" + rs.getString(2));// 入如果返回的是int类型可以用getInt() &#125;&#125; catch (SQLException e) &#123; System.out.println(\"MySQL操作错误\"); e.printStackTrace();&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; // 关闭结果集 if (rs != null) &#123; rs.close(); rs = null; &#125; // 关闭执行 if (stmt != null) &#123; stmt.close(); stmt = null; &#125; // 关闭连接 if (conn != null) &#123; conn.close(); conn = null; &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;&#125; 123456&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.43&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; JDBC演变到Mybatis过程连接获取和释放问题：数据库连接的频繁开启和关闭造成了资源的浪费解决：通过资源池解决 问题：连接池多种多样，有可能是DBCP连接池，也有可能采用容器本身的JNDI数据库连接池解决：可以通过DataSource进行隔离解耦，统一从DataSource里面获取数据库连接，DataSource具体由DBCP实现还是由容器的JNDI实现，由用户通过配置实现。 SQL统一存储问题：SQL语句散落在各个地方，有以下缺点 可读性差 不利于取出SQL在数据库上执行解决：SQL统一放置到配置文件或者数据库中 传入参数映射和动态SQL问题：查询条件是不固定的，怎么根据传入的条件的参数不同，动态生成对应的SQL呢？解决：通过Freemarker或者OGNL实现 结果映射和结果缓存问题：怎么把结果映射封装起来？解决：返回的结果可能是不做任何处理，或者是一个JavaBean或者List或者Map。所以只要知道返回什么类型就可以找到对应的类型处理前返回对应的结果。对于同一sql多次执行进行缓存，sql+查询条件作为key,返回值作为value. 解决重复SQL语句问题问题：许多sql有相同的片段，如果改动的时候就要改许多地方？解决:通过sql片段模块化，然后各个sql语句引用重复的模块。","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"},{"name":"jdbc","slug":"jdbc","permalink":"https://liyong.ac.cn/tags/jdbc/"}]},{"title":"mybatis：入门","slug":"Mybatis-入门","date":"2017-11-13T09:07:19.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/13/Mybatis-入门/","link":"","permalink":"https://liyong.ac.cn/2017/11/13/Mybatis-入门/","excerpt":"记录作者认为中比较重要的片段。精简再精简","text":"记录作者认为中比较重要的片段。精简再精简 XML 映射配置文件configurationproperties1234567891011&lt;properties resource=\"org/mybatis/example/config.properties\"&gt; &lt;property name=\"username\" value=\"dev_user\"/&gt; &lt;property name=\"password\" value=\"F2Fa3!33TYyg\"/&gt;&lt;/properties&gt;&lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;driver&#125;\"/&gt; &lt;property name=\"url\" value=\"$&#123;url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;username&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;password&#125;\"/&gt;&lt;/dataSource&gt; If a property exists in more than one of these places, MyBatis loads them in the following order: Properties specified in the body of the properties element are read first, Properties loaded from the classpath resource or url attributes of the properties element are read second Properties passed as a method parameter are read last.placeholderSince the MyBatis 3.4.2, your can specify a default value into placeholder as follow:123456789&lt;properties resource=\"org/mybatis/example/config.properties\"&gt; &lt;!-- ... --&gt; &lt;property name=\"org.apache.ibatis.parsing.PropertyParser.enable-default-value\" value=\"true\"/&gt; &lt;!-- Enable this feature --&gt;&lt;/properties&gt;&lt;dataSource type=\"POOLED\"&gt; &lt;!-- ... --&gt; &lt;property name=\"username\" value=\"$&#123;username:ut_user&#125;\"/&gt; &lt;!-- If 'username' property not present, username become 'ut_user' --&gt;&lt;/dataSource&gt; settingscacheEnabledlazyLoadingEnabledmultipleResultSetsEnableduseColumnLabeluseGeneratedKeysautoMappingBehavior PARTIALautoMappingUnknownColumnBehavior NONE?defaultExecutorType SIMPLEdefaultStatementTimeoutdefaultFetchSize?safeRowBoundsEnabledsafeResultHandlerEnabledmapUnderscoreToCamelCaselocalCacheScope SESSIONjdbcTypeForNull OTHERlazyLoadTriggerMethods equals,clone,hashCode,toStringdefaultScriptingLanguage org.apache.ibatis.scripting.xmltags.XMLLanguageDriverdefaultEnumTypeHandler org.apache.ibatis.type.EnumTypeHandlercallSettersOnNullsreturnInstanceForEmptyRowlogPrefixlogImplproxyFactory JAVASSISTvfsImpluseActualParamNameconfigurationFactory 123456789101112131415161718&lt;settings&gt; &lt;setting name=\"cacheEnabled\" value=\"true\"/&gt; &lt;setting name=\"lazyLoadingEnabled\" value=\"true\"/&gt; &lt;setting name=\"multipleResultSetsEnabled\" value=\"true\"/&gt; &lt;setting name=\"useColumnLabel\" value=\"true\"/&gt; &lt;setting name=\"useGeneratedKeys\" value=\"false\"/&gt; &lt;setting name=\"autoMappingBehavior\" value=\"PARTIAL\"/&gt; &lt;setting name=\"autoMappingUnknownColumnBehavior\" value=\"WARNING\"/&gt; &lt;setting name=\"defaultExecutorType\" value=\"SIMPLE\"/&gt; &lt;setting name=\"defaultStatementTimeout\" value=\"25\"/&gt; &lt;setting name=\"defaultFetchSize\" value=\"100\"/&gt; &lt;setting name=\"safeRowBoundsEnabled\" value=\"false\"/&gt; &lt;setting name=\"mapUnderscoreToCamelCase\" value=\"false\"/&gt; &lt;setting name=\"localCacheScope\" value=\"SESSION\"/&gt; &lt;setting name=\"jdbcTypeForNull\" value=\"OTHER\"/&gt; &lt;setting name=\"lazyLoadTriggerMethods\" value=\"equals,clone,hashCode,toString\"/&gt;&lt;/settings&gt; typeAliases12345678910111213&lt;typeAliases&gt; &lt;typeAlias alias=\"Author\" type=\"domain.blog.Author\"/&gt; &lt;typeAlias alias=\"Blog\" type=\"domain.blog.Blog\"/&gt;&lt;/typeAliases&gt;&lt;typeAliases&gt; &lt;package name=\"domain.blog\"/&gt;&lt;/typeAliases&gt;@Alias(\"author\")public class Author &#123; ...&#125; typeHandlers12345678&lt;!-- mybatis-config.xml --&gt;&lt;typeHandlers&gt; &lt;typeHandler handler=\"org.mybatis.example.ExampleTypeHandler\"/&gt;&lt;/typeHandlers&gt;&lt;typeHandlers&gt; &lt;package name=\"org.mybatis.example\"/&gt;&lt;/typeHandlers&gt; objectFactorypluginsMyBatis allows you to intercept calls to at certain points within the execution of a mapped statement. By default, MyBatis allows plug-ins to intercept method calls of: Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query)123456789101112131415161718192021@Intercepts(&#123;@Signature( type= Executor.class, method = \"update\", args = &#123;MappedStatement.class,Object.class&#125;)&#125;)public class ExamplePlugin implements Interceptor &#123; public Object intercept(Invocation invocation) throws Throwable &#123; return invocation.proceed(); &#125; public Object plugin(Object target) &#123; return Plugin.wrap(target, this); &#125; public void setProperties(Properties properties) &#123; &#125;&#125;&lt;!-- mybatis-config.xml --&gt;&lt;plugins&gt; &lt;plugin interceptor=\"org.mybatis.example.ExamplePlugin\"&gt; &lt;property name=\"someProperty\" value=\"100\"/&gt; &lt;/plugin&gt;&lt;/plugins&gt; environmentsenvironment12345678910111213&lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\"&gt; &lt;property name=\"...\" value=\"...\"/&gt; &lt;/transactionManager&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;driver&#125;\"/&gt; &lt;property name=\"url\" value=\"$&#123;url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;username&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;password&#125;\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt; transactionManagerThere are two TransactionManager types (i.e. type=”[JDBC|MANAGED]”) that are included with MyBatis: dataSourceThere are three build-in dataSource types (i.e. type=”[UNPOOLED|POOLED|JNDI]”): databaseIdProvider12345&lt;databaseIdProvider type=\"DB_VENDOR\"&gt; &lt;property name=\"SQL Server\" value=\"sqlserver\"/&gt; &lt;property name=\"DB2\" value=\"db2\"/&gt; &lt;property name=\"Oracle\" value=\"oracle\" /&gt;&lt;/databaseIdProvider&gt; mappers1234567891011121314&lt;!-- Using classpath relative resources --&gt;&lt;mappers&gt; &lt;mapper resource=\"org/mybatis/builder/AuthorMapper.xml\"/&gt;&lt;/mappers&gt;&lt;!-- Using mapper interface classes --&gt;&lt;mappers&gt; &lt;mapper class=\"org.mybatis.builder.AuthorMapper\"/&gt;&lt;/mappers&gt;&lt;!-- Register all interfaces in a package as mappers --&gt;&lt;mappers&gt; &lt;package name=\"org.mybatis.builder\"/&gt;&lt;/mappers&gt; select123456789101112131415&lt;select id=\"selectPerson\" //he fully qualified class name or alias for the parameter that will be passed into this statement. This attribute is optional because MyBatis can calculate the TypeHandler to use out of the actual parameter passed to the statement. Default is unset. parameterType=\"int\" resultType=\"hashmap\" resultMap=\"personResultMap\" // Setting this to true will cause the local and 2nd level caches to be flushed whenever this statement is called. Default: false for select statements. flushCache=\"false\" //Setting this to true will cause the results of this statement to be cached in 2nd level cache. Default: true for select statements useCache=\"true\" timeout=\"10000\" //This is a driver hint that will attempt to cause the driver to return results in batches of rows numbering in size equal to this setting. Default is unset (driver dependent). fetchSize=\"256\" statementType=\"PREPARED\" resultSetType=\"FORWARD_ONLY\"&gt; insert, update and delete1234567891011121314151617181920212223&lt;insert id=\"insertAuthor\" parameterType=\"domain.blog.Author\" flushCache=\"true\" statementType=\"PREPARED\" keyProperty=\"\" keyColumn=\"\" useGeneratedKeys=\"\" timeout=\"20\"&gt;&lt;update id=\"updateAuthor\" parameterType=\"domain.blog.Author\" flushCache=\"true\" statementType=\"PREPARED\" timeout=\"20\"&gt;&lt;delete id=\"deleteAuthor\" parameterType=\"domain.blog.Author\" flushCache=\"true\" statementType=\"PREPARED\" timeout=\"20\"&gt; statementTypeAny one of STATEMENT, PREPARED or CALLABLE. This causes MyBatis to use Statement, PreparedStatement or CallableStatement respectively. Default: PREPARED. useGeneratedKeys(insert and update only) This tells MyBatis to use the JDBC getGeneratedKeys method to retrieve keys generated internally by the database (e.g. auto increment fields in RDBMS like MySQL or SQL Server). Default: false keyProperty(insert and update only) Identifies a property into which MyBatis will set the key value returned by getGeneratedKeys, or by a selectKey child element of the insert statement. Default: unset. Can be a comma separated list of property names if multiple generated columns are expected. keyColumn(insert and update only) Sets the name of the column in the table with a generated key. This is only required in certain databases (like PostgreSQL) when the key column is not the first column in the table. Can be a comma separated list of columns names if multiple generated columns are expected. databaseIdIn case there is a configured databaseIdProvider, MyBatis will load all statements with no databaseId attribute or with a databaseId that matches the current one. If case the same statement if found with and without the databaseId the latter will be discarded. Parameters1#&#123;property,javaType=int,jdbcType=NUMERIC&#125; NOTE The JDBC Type is required by JDBC for all nullable columns, if null is passed as a value. You can investigate this yourself by reading the JavaDocs for the PreparedStatement.setNull() method. customize handlingTo further customize type handling, you can also specify a specific TypeHandler class (or alias), for example: 1#&#123;age,javaType=int,jdbcType=NUMERIC,typeHandler=MyTypeHandler&#125; numericScaleFor numeric types there’s also a numericScale for determining how many decimal places are relevant. 1#&#123;height,javaType=double,jdbcType=NUMERIC,numericScale=2&#125; String Substitution1ORDER BY $&#123;columnName&#125; Result MapsAdvanced Result Maps12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;!-- Very Complex Statement --&gt;&lt;select id=\"selectBlogDetails\" resultMap=\"detailedBlogResultMap\"&gt; select B.id as blog_id, B.title as blog_title, B.author_id as blog_author_id, A.id as author_id, A.username as author_username, A.password as author_password, A.email as author_email, A.bio as author_bio, A.favourite_section as author_favourite_section, P.id as post_id, P.blog_id as post_blog_id, P.author_id as post_author_id, P.created_on as post_created_on, P.section as post_section, P.subject as post_subject, P.draft as draft, P.body as post_body, C.id as comment_id, C.post_id as comment_post_id, C.name as comment_name, C.comment as comment_text, T.id as tag_id, T.name as tag_name from Blog B left outer join Author A on B.author_id = A.id left outer join Post P on B.id = P.blog_id left outer join Comment C on P.id = C.post_id left outer join Post_Tag PT on PT.post_id = P.id left outer join Tag T on PT.tag_id = T.id where B.id = #&#123;id&#125;&lt;/select&gt;&lt;!-- Very Complex Result Map --&gt;&lt;resultMap id=\"detailedBlogResultMap\" type=\"Blog\"&gt; &lt;constructor&gt; &lt;idArg column=\"blog_id\" javaType=\"int\"/&gt; &lt;/constructor&gt; &lt;result property=\"title\" column=\"blog_title\"/&gt; &lt;association property=\"author\" javaType=\"Author\"&gt; &lt;id property=\"id\" column=\"author_id\"/&gt; &lt;result property=\"username\" column=\"author_username\"/&gt; &lt;result property=\"password\" column=\"author_password\"/&gt; &lt;result property=\"email\" column=\"author_email\"/&gt; &lt;result property=\"bio\" column=\"author_bio\"/&gt; &lt;result property=\"favouriteSection\" column=\"author_favourite_section\"/&gt; &lt;/association&gt; &lt;collection property=\"posts\" ofType=\"Post\"&gt; &lt;id property=\"id\" column=\"post_id\"/&gt; &lt;result property=\"subject\" column=\"post_subject\"/&gt; &lt;association property=\"author\" javaType=\"Author\"/&gt; &lt;collection property=\"comments\" ofType=\"Comment\"&gt; &lt;id property=\"id\" column=\"comment_id\"/&gt; &lt;/collection&gt; &lt;collection property=\"tags\" ofType=\"Tag\" &gt; &lt;id property=\"id\" column=\"tag_id\"/&gt; &lt;/collection&gt; &lt;discriminator javaType=\"int\" column=\"draft\"&gt; &lt;case value=\"1\" resultType=\"DraftPost\"/&gt; &lt;/discriminator&gt; &lt;/collection&gt;&lt;/resultMap&gt; resultMapsub-elementsconstructor、id、result、association、collection、discriminator Attributeid、type、autoMapping id &amp; resultproperty、column、javaType、jdbcType、typeHandler association1234&lt;association property=\"author\" javaType=\"Author\"&gt; &lt;id property=\"id\" column=\"author_id\"/&gt; &lt;result property=\"username\" column=\"author_username\"/&gt;&lt;/association&gt; Nested Select for Association1234567891011&lt;resultMap id=\"blogResult\" type=\"Blog\"&gt; &lt;association property=\"author\" column=\"author_id\" javaType=\"Author\" select=\"selectAuthor\"/&gt;&lt;/resultMap&gt;&lt;select id=\"selectBlog\" resultMap=\"blogResult\"&gt; SELECT * FROM BLOG WHERE ID = #&#123;id&#125;&lt;/select&gt;&lt;select id=\"selectAuthor\" resultType=\"Author\"&gt; SELECT * FROM AUTHOR WHERE ID = #&#123;id&#125;&lt;/select&gt; While this approach is simple, it will not perform well for large data sets or lists. This problem is known as the “N+1 Selects Problem”. In a nutshell, the N+1 selects problem is caused like this: You execute a single SQL statement to retrieve a list of records (the “+1”).For each record returned, you execute a select statement to load details for each (the “N”). columnThe column name from the database, or the aliased column label that holds the value that will be passed to the nested statement as an input parameter. This is the same string that would normally be passed to resultSet.getString(columnName). Note: To deal with composite keys, you can specify multiple column names to pass to the nested select statement by using the syntax column=”{prop1=col1,prop2=col2}”. This will cause prop1 and prop2 to be set against the parameter object for the target nested select statement. selectThe ID of another mapped statement that will load the complex type required by this property mapping. The values retrieved from columns specified in the column attribute will be passed to the target select statement as parameters. A detailed example follows this table. Note: To deal with composite keys, you can specify multiple column names to pass to the nested select statement by using the syntax column=”{prop1=col1,prop2=col2}”. This will cause prop1 and prop2 to be set against the parameter object for the target nested select statement. fetchType如果场景不是懒加载（lazy），最好不要用这种方式Optional. Valid values are lazy and eager. If present, it supersedes the global configuration parameter lazyLoadingEnabled for this mapping. Nested Results for Association12345678910111213141516171819202122232425262728293031323334353637383940&lt;select id=\"selectBlog\" resultMap=\"blogResult\"&gt; select B.id as blog_id, B.title as blog_title, B.author_id as blog_author_id, A.id as author_id, A.username as author_username, A.password as author_password, A.email as author_email, A.bio as author_bio from Blog B left outer join Author A on B.author_id = A.id where B.id = #&#123;id&#125;&lt;/select&gt;&lt;resultMap id=\"blogResult\" type=\"Blog\"&gt; &lt;id property=\"id\" column=\"blog_id\" /&gt; &lt;result property=\"title\" column=\"blog_title\"/&gt; &lt;association property=\"author\" resultMap=\"authorResult\" /&gt;&lt;/resultMap&gt;&lt;resultMap id=\"authorResult\" type=\"Author\"&gt; &lt;id property=\"id\" column=\"author_id\"/&gt; &lt;result property=\"username\" column=\"author_username\"/&gt; &lt;result property=\"password\" column=\"author_password\"/&gt; &lt;result property=\"email\" column=\"author_email\"/&gt; &lt;result property=\"bio\" column=\"author_bio\"/&gt;&lt;/resultMap&gt;&lt;resultMap id=\"blogResult\" type=\"Blog\"&gt; &lt;id property=\"id\" column=\"blog_id\" /&gt; &lt;result property=\"title\" column=\"blog_title\"/&gt; &lt;association property=\"author\" javaType=\"Author\"&gt; &lt;id property=\"id\" column=\"author_id\"/&gt; &lt;result property=\"username\" column=\"author_username\"/&gt; &lt;result property=\"password\" column=\"author_password\"/&gt; &lt;result property=\"email\" column=\"author_email\"/&gt; &lt;result property=\"bio\" column=\"author_bio\"/&gt; &lt;/association&gt;&lt;/resultMap&gt; resultMapThis is the ID of a ResultMap that can map the nested results of this association into an appropriate object graph. columnPrefixWhen joining multiple tables, you would have to use column alias to avoid duplicated column names in the ResultSet. Specifying columnPrefix allows you to map such columns to an external resultMap. Please see the example explained later in this section. notNullColumnBy default a child object is created only if at least one of the columns mapped to the child’s properties is non null. autoMappingIf present, MyBatis will enable or disable automapping when mapping the result to this property. Multiple ResultSets for Associationxml12345678910111213141516171819//getBlogsAndAuthorsSELECT * FROM BLOG WHERE ID = #&#123;id&#125;SELECT * FROM AUTHOR WHERE ID = #&#123;id&#125;&lt;select id=&quot;selectBlog&quot; resultSets=&quot;blogs,authors&quot; resultMap=&quot;blogResult&quot; statementType=&quot;CALLABLE&quot;&gt; &#123;call getBlogsAndAuthors(#&#123;id,jdbcType=INTEGER,mode=IN&#125;)&#125;&lt;/select&gt;&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;id&quot; /&gt; &lt;result property=&quot;title&quot; column=&quot;title&quot;/&gt; &lt;association property=&quot;author&quot; javaType=&quot;Author&quot; resultSet=&quot;authors&quot; column=&quot;author_id&quot; foreignColumn=&quot;id&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;id&quot;/&gt; &lt;result property=&quot;username&quot; column=&quot;username&quot;/&gt; &lt;result property=&quot;password&quot; column=&quot;password&quot;/&gt; &lt;result property=&quot;email&quot; column=&quot;email&quot;/&gt; &lt;result property=&quot;bio&quot; column=&quot;bio&quot;/&gt; &lt;/association&gt;&lt;/resultMap&gt; columnWhen using multiple resultset this attribute specifies the columns (separated by commas) that will be correlated with the foreignColumn to identify the parent and the child of a relationship. foreignColumnIdentifies the name of the columns that contains the foreign keys which values will be matched against the values of the columns specified in the column attibute of the parent type. resultSetIdentifies the name of the result set where this complex type will be loaded from. collection1234567&lt;collection property=\"posts\" ofType=\"domain.blog.Post\"&gt; &lt;id property=\"id\" column=\"post_id\"/&gt; &lt;result property=\"subject\" column=\"post_subject\"/&gt; &lt;result property=\"body\" column=\"post_body\"/&gt;&lt;/collection&gt;private List&lt;Post&gt; posts; Nested Select for Collection1234567891011&lt;resultMap id=\"blogResult\" type=\"Blog\"&gt; &lt;collection property=\"posts\" javaType=\"ArrayList\" column=\"id\" ofType=\"Post\" select=\"selectPostsForBlog\"/&gt;&lt;/resultMap&gt;&lt;select id=\"selectBlog\" resultMap=\"blogResult\"&gt; SELECT * FROM BLOG WHERE ID = #&#123;id&#125;&lt;/select&gt;&lt;select id=\"selectPostsForBlog\" resultType=\"Post\"&gt; SELECT * FROM POST WHERE BLOG_ID = #&#123;id&#125;&lt;/select&gt; Nested Results for Collection12345678910111213141516171819202122&lt;select id=\"selectBlog\" resultMap=\"blogResult\"&gt; select B.id as blog_id, B.title as blog_title, B.author_id as blog_author_id, P.id as post_id, P.subject as post_subject, P.body as post_body, from Blog B left outer join Post P on B.id = P.blog_id where B.id = #&#123;id&#125;&lt;/select&gt;&lt;resultMap id=\"blogResult\" type=\"Blog\"&gt; &lt;id property=\"id\" column=\"blog_id\" /&gt; &lt;result property=\"title\" column=\"blog_title\"/&gt; &lt;collection property=\"posts\" ofType=\"Post\"&gt; &lt;id property=\"id\" column=\"post_id\"/&gt; &lt;result property=\"subject\" column=\"post_subject\"/&gt; &lt;result property=\"body\" column=\"post_body\"/&gt; &lt;/collection&gt;&lt;/resultMap&gt; Multiple ResultSets for Collection1234567891011121314151617//getBlogsAndPostsSELECT * FROM BLOG WHERE ID = #&#123;id&#125;SELECT * FROM POST WHERE BLOG_ID = #&#123;id&#125;&lt;select id=\"selectBlog\" resultSets=\"blogs,posts\" resultMap=\"blogResult\"&gt; &#123;call getBlogsAndPosts(#&#123;id,jdbcType=INTEGER,mode=IN&#125;)&#125;&lt;/select&gt;&lt;resultMap id=\"blogResult\" type=\"Blog\"&gt; &lt;id property=\"id\" column=\"id\" /&gt; &lt;result property=\"title\" column=\"title\"/&gt; &lt;collection property=\"posts\" ofType=\"Post\" resultSet=\"posts\" column=\"id\" foreignColumn=\"blog_id\"&gt; &lt;id property=\"id\" column=\"id\"/&gt; &lt;result property=\"subject\" column=\"subject\"/&gt; &lt;result property=\"body\" column=\"body\"/&gt; &lt;/collection&gt;&lt;/resultMap&gt; discriminator略 Auto-mappingUsually database columns are named using uppercase letters and underscores between words and java properties often follow the camelcase naming covention. To enable the auto-mapping between them set the setting mapUnderscoreToCamelCase to true. Auto-mapping works even when there is an specific result map. When this happens, for each result map, all columns that are present in the ResultSet that have not a manual mapping will be auto-mapped, then manual mappings will be processed. In the following sample id and userName columns will be auto-mapped and hashed_password column will be mapped. 123456789101112&lt;select id=\"selectUsers\" resultMap=\"userResultMap\"&gt; select user_id as \"id\", user_name as \"userName\", hashed_password from some_table where id = #&#123;id&#125;&lt;/select&gt;&lt;resultMap id=\"userResultMap\" type=\"User\"&gt; &lt;result property=\"password\" column=\"hashed_password\"/&gt;&lt;/resultMap&gt; There are three auto-mapping levels: NONE - disables auto-mapping. Only manually mapped properties will be set. PARTIAL - will auto-map results except those that have nested result mappings defined inside (joins). FULL - auto-maps everything. Regardless of the auto-mapping level configured you can enable or disable the automapping for an specific ResultMap by adding the attribute autoMapping to it: 123&lt;resultMap id=\"userResultMap\" type=\"User\" autoMapping=\"false\"&gt; &lt;result property=\"password\" column=\"hashed_password\"/&gt;&lt;/resultMap&gt; cache12345678910&lt;cache eviction=\"FIFO\" flushInterval=\"60000\" size=\"512\" readOnly=\"true\"/&gt;&lt;select ... flushCache=\"false\" useCache=\"true\"/&gt;&lt;insert ... flushCache=\"true\"/&gt;&lt;update ... flushCache=\"true\"/&gt;&lt;delete ... flushCache=\"true\"/&gt;","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"}]},{"title":"reflect","slug":"aa_category/se/reflect/reflect","date":"2017-11-08T02:08:30.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/11/08/aa_category/se/reflect/reflect/","link":"","permalink":"https://liyong.ac.cn/2017/11/08/aa_category/se/reflect/reflect/","excerpt":"java.lang.reflect.Type：java语言中所有类型的公共父接口 java.lang.reflect.ParameterizedType 表示一种参数化的类型，比如Collection java.lang.reflect.GenericArrayType 表示一种元素类型是参数化类型或者类型变量的数组类型 java.lang.reflectTypeVariable 是各种类型变量的公共父接口 java.lang.reflect.WildcardType 代表一种通配符类型表达式，比如?, ? extends Number, ? super Integer【wildcard是一个单词：就是“通配符”】","text":"java.lang.reflect.Type：java语言中所有类型的公共父接口 java.lang.reflect.ParameterizedType 表示一种参数化的类型，比如Collection java.lang.reflect.GenericArrayType 表示一种元素类型是参数化类型或者类型变量的数组类型 java.lang.reflectTypeVariable 是各种类型变量的公共父接口 java.lang.reflect.WildcardType 代表一种通配符类型表达式，比如?, ? extends Number, ? super Integer【wildcard是一个单词：就是“通配符”】 Type所有类型指代的有 原始类型 (raw types)【对应Class】 参数化类型 (parameterizedtypes)【对应ParameterizedType】 数组类型 (array types)【对应GenericArrayType】 类型变量 (type variables)【对应TypeVariable】 基本数据类型(primitivetypes)【仍然对应Class】 ParameterizedTypeParameterizedType represents a parameterized type such as Collection.【注意】无论&lt;&gt;中有几层&lt;&gt;嵌套，这个方法仅仅脱去最外层的&lt;&gt;之后剩下的内容就作为这个方法的返回值。 GenericArrayTypeType getGenericComponentType();【注意】无论从左向右有几个[]并列，这个方法仅仅脱去最右边的[]之后剩下的内容就作为这个方法的返回值 1234567public static &lt;E&gt; E methodV( String[] p1, E[] p2, ArrayList[] p3, E[][] p4) &#123; return null;&#125; TypeVariable123public static &lt;E extends Map &amp; Cloneable &amp; Serializable&gt; E methodVI(E e) &#123; return null; &#125; WildcardType123public static void methodVI(List&lt;? extends String&gt; list) &#123; &#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"reflect","slug":"reflect","permalink":"https://liyong.ac.cn/tags/reflect/"}]},{"title":"Mybatis:源码分析之总体概述","slug":"Mybatis-源码分析之总体概述","date":"2017-10-31T08:16:46.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/10/31/Mybatis-源码分析之总体概述/","link":"","permalink":"https://liyong.ac.cn/2017/10/31/Mybatis-源码分析之总体概述/","excerpt":"","text":"源码解读-mybatis3.44org.apache.ibatis.cacheCache1234567String getId();void putObject(Object key, Object value);Object getObject(Object key);Object removeObject(Object key);void clear();int getSize();ReadWriteLock getReadWriteLock(); org.apache.ibatis.cache.decorators装饰者模式 org.apache.ibatis.builder.annotation注解包 org.apache.ibatis.bindingorg.apache.ibatis.builderorg.apache.ibatis.cursorCursor123boolean isOpen();boolean isConsumed();int getCurrentIndex(); org.apache.ibatis.datasourceDataSourceFactory12void setProperties(Properties props); DataSource getDataSource(); JndiDataSourceFactoryjndi:Java Naming and Directory Interface org.apache.ibatis.exceptions异常包 org.apache.ibatis.executorExecutor1234567891011121314ResultHandler NO_RESULT_HANDLER = null;int update(MappedStatement ms, Object parameter) throws SQLException;&lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey cacheKey, BoundSql boundSql) throws SQLException;List&lt;BatchResult&gt; flushStatements() throws SQLException;void commit(boolean required) throws SQLException;void rollback(boolean required) throws SQLException;CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql);boolean isCached(MappedStatement ms, CacheKey key);void clearLocalCache();void deferLoad(MappedStatement ms, MetaObject resultObject, String property, CacheKey key, Class&lt;?&gt; targetType);Transaction getTransaction();void close(boolean forceRollback);boolean isClosed();void setExecutorWrapper(Executor executor); StatementHandler1234567891011121314Statement prepare(Connection connection, Integer transactionTimeout) throws SQLException;void parameterize(Statement statement) throws SQLException;void batch(Statement statement) throws SQLException;int update(Statement statement) throws SQLException;&lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) throws SQLException;&lt;E&gt; Cursor&lt;E&gt; queryCursor(Statement statement) throws SQLException;BoundSql getBoundSql();ParameterHandler getParameterHandler(); ResultSetHandler123&lt;E&gt; List&lt;E&gt; handleResultSets(Statement stmt) throws SQLException;&lt;E&gt; Cursor&lt;E&gt; handleCursorResultSets(Statement stmt) throws SQLException;void handleOutputParameters(CallableStatement cs) throws SQLException; ParameterHandler1234Object getParameterObject();void setParameters(PreparedStatement ps) throws SQLException; ProxyFactory12void setProperties(Properties properties); Object createProxy(Object target, ResultLoaderMap lazyLoader, Configuration configuration, ObjectFactory objectFactory, List&lt;Class&lt;?&gt;&gt; constructorArgTypes, List&lt;Object&gt; constructorArgs); org.apache.ibatis.ioClassLoaderWrapperA class to wrap access to multiple class loaders making them work as one ResourcesA class to simplify access to resources through the classloader. VFSProvides a very simple API for accessing resources within an application server. org.apache.ibatis.javassistNONE将编译时不存在的类动态创建并加载动态生成二进制的字节码 org.apache.ibatis.jdbcorg.apache.ibatis.lang表明使用哪个版本jdk的特性 org.apache.ibatis.loggingorg.apache.ibatis.mappingFetchType1LAZY, EAGER, DEFAULT SqlCommandType1UNKNOWN, INSERT, UPDATE, DELETE, SELECT, FLUSH ParameterMode1IN, OUT, INOUT StatementType1STATEMENT, PREPARED, CALLABLE ResultFlag1ID, CONSTRUCTOR ResultSetType1FORWARD_ONLY,SCROLL_INSENSITIVE,SCROLL_SENSITIVE SqlSource1BoundSql getBoundSql(Object parameterObject); ParameterMappingDatabaseIdProvider VendorDatabaseIdProvider Vendor DatabaseId provider org.apache.ibatis.ognlNONEognl:Object-Graph Navigation Language org.apache.ibatis.parsingorg.apache.ibatis.pluginorg.apache.ibatis.reflectionParamNameResolver根据方法和配置获取方法的参数 SystemMetaObjectTypeParameterResolver类型解析 ReflectorFactory通过反射生成对象的工厂 MetaClass类的原数据 org.apache.ibatis.reflection.factory使用反射构建对象 ObjectFactory12void setProperties(Properties properties);&lt;T&gt; T create(Class&lt;T&gt; type); org.apache.ibatis.reflection.invoker使用反射调用方法 org.apache.ibatis.reflection.property使用操作属性 PropertyCopier 利用属性进行复制操作 org.apache.ibatis.reflection.wrapperObjectWrapper对象的原数据 org.apache.ibatis.scriptingLanguageDriver12 ParameterHandler createParameterHandler(MappedStatement mappedStatement, Object parameterObject, BoundSql boundSql);SqlSource createSqlSource(Configuration configuration, XNode script, Class&lt;?&gt; parameterType); SqlNode1boolean apply(DynamicContext context); org.apache.ibatis.sessionAutoMappingBehavior1NONE,PARTIAL,FULL ExecutorType1SIMPLE, REUSE, BATCH LocalCacheScope1SESSION,STATEMENT TransactionIsolationLevel12345678910//NONE(Connection.TRANSACTION_NONE),//读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。READ_COMMITTED(Connection.TRANSACTION_READ_COMMITTED),// 读未提交，顾名思义，就是一个事务可以读取另一个未提交事务的数据。READ_UNCOMMITTED(Connection.TRANSACTION_READ_UNCOMMITTED),//重复读，就是在开始读取数据（事务开启）时，不再允许修改操REPEATABLE_READ(Connection.TRANSACTION_REPEATABLE_READ),//SERIALIZABLE(Connection.TRANSACTION_SERIALIZABLE); ResultContext1234T getResultObject();int getResultCount();boolean isStopped();void stop(); ResultHandler1void handleResult(ResultContext&lt;? extends T&gt; resultContext); SqlSession123456789101112&lt;T&gt; T selectOne(String statement, Object parameter);&lt;K, V&gt; Map&lt;K, V&gt; selectMap(String statement, Object parameter, String mapKey);int insert(String statement);int update(String statement, Object parameter);int delete(String statement, Object parameter);void commit(boolean force);void rollback(boolean force); List&lt;BatchResult&gt; flushStatements();void clearCache();Configuration getConfiguration();&lt;T&gt; T getMapper(Class&lt;T&gt; type);Connection getConnection(); SqlSessionFactory12SqlSession openSession(boolean autoCommit);Configuration getConfiguration(); org.apache.ibatis.transactionTransaction12345Connection getConnection() throws SQLException;void commit() throws SQLException;void rollback() throws SQLException;void close() throws SQLException;Integer getTimeout() throws SQLException; org.apache.ibatis.typeTypeHandler123void setParameter(PreparedStatement ps, int i, T parameter, JdbcType jdbcType) throws SQLException; T getResult(ResultSet rs, String columnName) throws SQLException; TypeAliasRegistry和contain的概念差不多","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://liyong.ac.cn/tags/mybatis/"}]},{"title":"《spring技术内幕：深入解析spring架构与计原理（第2版）》读书笔记","slug":"《spring技术内幕：深入解析spring架构与计原理（第2版）》读书笔记","date":"2017-10-23T01:46:35.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/10/23/《spring技术内幕：深入解析spring架构与计原理（第2版）》读书笔记/","link":"","permalink":"https://liyong.ac.cn/2017/10/23/《spring技术内幕：深入解析spring架构与计原理（第2版）》读书笔记/","excerpt":"","text":"第1章 spring的设计理念和整体架构 / 11.1 spring的各个子项目 / 21.2 spring的设计目标 / 51.3 spring的整体架构 / 71.4 spring的应用场景 / 10第一部分 spring核心实现篇第2章 spring framework的核心：ioc容器的实现 / 162.1 spring ioc容器概述 / 172.1.1 ioc容器和依赖反转模式 / 172.1.2 spring ioc的应用场景 / 182.2 ioc容器系列的设计与实现：beanfactory和applicationcontext / 192.2.1 spring的ioc容器系列 / 192.2.2 spring ioc容器的设计 / 212.3 ic容器的初始化过程 / 282.3.1 beandefinition的resource定位 / 292.3.2 beandefinition的载入和解析 / 372.3.3 beandefinition在ioc容器中的注册 / 52.2.4 ioc容器的依赖注入 / 542.5 容器其他相关特性的设计与实现 / 752.5.1 applicationcontext和bean的初始化及销毁 / 752.5.2 lazy-init属性和预实例化 / 812.5.3 factorybean的实现 / 822.5.4 beanpostprocessor的实现 / 852.5.5 autowiring（自动依赖装配）的实现 / 882.5.6 bean的依赖检查 / 902.5.7 bean对ioc容器的感知 / 912.6 小结 / 92 第3章 spring aop的实现 / 943.1 spring aop概述 / 953.1.1 aop概念回顾 / 953.1.2 advice通知 / 983.1.3 pointcut切点 / 1023.1.4 advisor通知器 / 1053.2 spring aop的设计与实现 / 1063.2.1 jvm的动态代理特性 / 1063.2.2 spring aop的设计分析 / 1083.2.3 spring aop的应用场景 / 1083.3 建立aopproxy代理对象 / 1093.3.1 设计原理 / 1093.3.2 配置proxyfactorybean / 1103.3.3 proxyfactorybean生成aopproxy代理对象 / 1113.3.4 jdk生成aopproxy代理对象 / 1163.3.5 cglib生成aopproxy代理对象 / 1173.4 spring aop拦截器调用的实现 / 1193.4.1 设计原理 / 1193.4.2 jdkdynamicaopproxy的invoke拦截 / 1203.4.3 cglib2aopproxy的intercept拦截 / 1213.4.4 目标对象方法的调用 / 1223.4.5 aop拦截器链的调用 / 1233.4.6 配置通知器 / 1243.4.7 advice通知的实现 / 1293.4.8 proxyfactory实现aop / 1363.5 spring aop的高级特性 / 1383.6 小结 / 140 第二部分 spring组件实现篇第4章 spring mvc与web环境 / 1454.1 spring mvc概述 / 1464.2 web环境中的spring mvc / 1484.3 上下文在web容器中的启动 / 1494.3.1 ioc容器启动的基本过程 / 1494.3.2 web容器中的上下文设计 / 1514.3.3 contextloader的设计与实现 / 1544.4 spring mvc的设计与实现 / 1584.4.1 spring mvc的应用场景 / 1584.4.2 spring mvc设计概览 / 1584.4.3 dispatcherservlet的启动和初始化 / 1604.4.4 mvc处理http分发请求 / 1664.5 spring mvc视图的呈现 / 1784.5.1 dispatcherservlet视图呈现的设计 / 1784.5.2 jsp视图的实现 / 1824.5.3 excelview的实现 / 1854.5.4 pdf视图的实现 / 1874.6 小结 / 189 第5章 数据库操作组件的实现 / 1915.1 spring jdbc的设计与实现 / 1925.1.1 应用场景 / 1925.1.2 设计概要 / 1925.2 spring jdbc中模板类的设计与实现 / 1935.2.1 设计原理 / 1935.2.2 jdbctemplate的基本使用 / 1935.2.3 jdbctemplate的execute实现 / 1945.2.4 jdbctemplate的query实现 / 1965.2.5 使用数据库connection / 1975.3 spring jdbc中rdbms操作对象的实现 / 1995.3.1 sqlquery的实现 / 2005.3.2 sqlupdate的实现 / 2045.3.3 sqlfunction / 2065.4 spring orm的设计与实现 / 2085.4.1 应用场景 / 2085.4.2 设计概要 / 2085.5 spring驱动hibernate的设计与实现 / 2095.5.1 设计原理 / 2105.5.2 hibernate的sessionfactory / 2105.5.3 hibernatetemplate的实现 / 2155.5.4 session的管理 / 2195.6 spring驱动ibatis的设计与实现 / 2225.6.1 设计原理 / 2225.6.2 创建sqlmapclient / 2225.6.3 sqlmapclienttemplate的实现 / 2245.7 小结 / 227 第6章 spring事务处理的实现 / 2286.1 spring与事务处理 / 2296.2 spring事务处理的设计概览 / 2296.3 spring事务处理的应用场景 / 2306.4 spring声明式事务处理 / 2316.4.1 设计原理与基本过程 / 2316.4.2 实现分析 / 2316.5 spring事务处理的设计与实现 / 2416.5.1 spring事务处理的编程式使用 / 2416.5.2 事务的创建 / 2426.5.3 事务的挂起 / 2496.5.4 事务的提交 / 2516.5.5 事务的回滚 / 2536.6 spring事务处理器的设计与实现 / 2556.6.1 spring事务处理的应用场景 / 2556.6.2 datasourcetransactionmanager的实现 / 2566.6.3 hibernatetransactionmanager的实现 / 2596.7 小结 / 265 第7章 spring远端调用的实现 / 2677.1 spring远端调用的应用场景 / 2687.2 spring远端调用的设计概览 / 2687.3 spring远端调用的实现 / 2717.3.1 spring http调用器的实现 / 2717.3.2 spring hession/burlap的实现原理 / 2827.3.3 spring rmi的实现 / 2957.4 小结 / 302 第三部分 spring应用实现篇第8章 安全框架acegi的设计与实现 / 3078.1 spring acegi安全框架概述 / 3088.1.1 概述 / 3088.1.2 设计原理与基本实现过程 / 3088.1.3 acegi的bean配置 / 3098.2 配置spring acegi / 3108.3 acegi的web过滤器实现 / 3138.4 acegi验证器的实现 / 3158.4.1 authenticationmanager的authenticate / 3158.4.2 daoauthenticationprovider的实现 / 3188.4.3 读取数据库用户信息 / 3208.4.4 完成用户信息的对比验证 / 3238.5 acegi授权器的实现 / 3248.5.1 与web环境的接口filtersecurityinterceptor / 3248.5.2 授权器的实现 / 3278.5.3 投票器的实现 / 3298.6 小结 / 330第9章 spring dm模块的设计与实现 / 3329.1 spring dm模块的应用场景 / 3339.2 spring dm的应用过程 / 3349.3 spring dm设计与实现 / 3389.4 小结 / 348 第10章 spring flex的设计与实现 / 35010.1 spring flex模块的应用场景 / 35110.2 spring flex的应用过程 / 35310.3 spring flex的设计与实现 / 35510.4 小结 / 362附录a spring项目的源代码环境 / 363附录b 构建spring项目的发布包 / 378附录c 使用spring ide / 381附录d spring pet clinic应用实例 / 385· · · · · · (收起)","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"}]},{"title":"《Spring源码深度解析》读书笔记","slug":"《Spring源码深度解析》读书笔记","date":"2017-10-23T01:30:12.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/10/23/《Spring源码深度解析》读书笔记/","link":"","permalink":"https://liyong.ac.cn/2017/10/23/《Spring源码深度解析》读书笔记/","excerpt":"","text":"第一部分 核心实现##第1章 Spring整体架构和环境搭建 2 1.1 Spring的整体架构 21.2 环境搭建 41.2.1 安装GitHub 41.2.2 安装Gradle 51.2.3 下载Spring 6第2章 容器的基本实现 102.1 容器基本用法 102.2 功能分析 112.3 工程搭建 122.4 Spring的结构组成 132.4.1 beans包的层级结构 132.4.2 核心类介绍 132.5 容器的基础XmlBeanFactory 172.5.1 配置文件封装 182.5.2 加载Bean 212.6 获取XML的验证模式 242.6.1 DTD与XSD区别 242.6.2 验证模式的读取 262.7 获取Document 282.7.1 EntityResolver用法 292.8 解析及注册BeanDefinitions 312.8.1 profile属性的使用 322.8.2 解析并注册BeanDefinition 33 第3章 默认标签的解析 353.1 bean标签的解析及注册 353.1.1 解析BeanDefinition 373.1.2 AbstractBeanDefinition属性 553.1.3 解析默认标签中的自定义标签元素 583.1.4 注册解析的BeanDefinition 603.1.5 通知监听器解析及注册完成 633.2 alias标签的解析 633.3 import标签的解析 653.4 嵌入式beans标签的解析 67 第4章 自定义标签的解析 684.1 自定义标签使用 694.2 自定义标签解析 714.2.1 获取标签的命名空间 724.2.2 提取自定义标签处理器 724.2.3 标签解析 74 第5章 bean的加载 785.1 FactoryBean的使用 835.2 缓存中获取单例bean 855.3 从bean的实例中获取对象 865.4 获取单例 905.5 准备创建bean 925.5.1 处理ovverride属性 935.5.2 实例化的前置处理 945.6 循环依赖 965.6.1 什么是循环依赖 965.6.2 Spring如何解决循环依赖 965.7 创建bean 1005.7.1 创建bean的实例 1035.7.2 记录创建bean的ObjectFactory 1125.7.3 属性注入 1155.7.4 初始化bean 1245.7.5 注册DisposableBean 128 第6章 容器的功能扩展 1296.1 设置配置路径 1306.2 扩展功能 1306.3 环境准备 1326.4 加载BeanFactory 1336.4.1 定制BeanFactory 1356.4.2 加载BeanDefinition 1366.5 功能扩展 1376.5.1 增加SPEL语言的支持 1386.5.2 增加属性注册编辑器 1396.5.3 添加ApplicationContext AwareProcessor处理器 1446.5.4 设置忽略依赖 1466.5.5 注册依赖 1466.6 BeanFactory的后处理 1466.6.1 激活注册的BeanFactory PostProcessor 1476.6.2 注册BeanPostProcessor 1536.6.3 初始化消息资源 1566.6.4 初始化ApplicationEvent Multicaster 1596.6.5 注册监听器 1616.7 初始化非延迟加载单例 1626.8 finishRefresh 165 第7章 AOP 1677.1 动态AOP使用示例 1677.2 动态AOP自定义标签 1697.2.1 注册AnnotationAwareAspectJ AutoProxyCreator 1707.3 创建AOP代理 1737.3.1 获取增强器 1767.3.2 寻找匹配的增强器 1867.3.3 创建代理 1877.4 静态AOP使用示例 2017.5 创建AOP静态代理 2037.5.1 Instrumentation使用 2037.5.2 自定义标签 2077.5.3 织入 209 第二部分 企业应用第8章 数据库连接JDBC 2148.1 Spring连接数据库程序实现(JDBC) 2158.2 save/update功能的实现 2178.2.1 基础方法execute 2198.2.2 Update中的回调函数 2238.3 query功能的实现 2258.4 queryForObject 229 第9章 整合MyBatis 2319.1 MyBatis独立使用 2319.2 Spring整合MyBatis 2359.3 源码分析 2379.3.1 sqlSessionFactory创建 2379.3.2 MapperFactoryBean的创建 2419.3.3 MapperScannerConfigurer 244 第10章 事务 25410.1 JDBC方式下的事务使用 示例 25410.2 事务自定义标签 25710.2.1 注册InfrastructureAdvisor AutoProxyCreator 25710.2.2 获取对应class/method的增强器 26110.3 事务增强器 26910.3.1 创建事务 27110.3.2 回滚处理 28110.3.3 事务提交 287 第11章 SpringMVC 29111.1 SpringMVC快速体验 29111.2 ContextLoaderListener 29511.2.1 ServletContextListener的使用 29511.2.2 Spring中的ContextLoader Listener 29611.3 DispatcherServlet 30011.3.1 servlet的使用 30111.3.2 DispatcherServlet的初始化 30211.3.3 WebApplicationContext的初始化 30411.4 DispatcherServlet的逻辑处理 32011.4.1 MultipartContent类型的request处理 32611.4.2 根据request信息寻找对应的Handler 32711.4.3 没找到对应的Handler的错误处理 33111.4.4 根据当前Handler寻找对应的HandlerAdapter 33111.4.5 缓存处理 33211.4.6 HandlerInterceptor的处理 33311.4.7 逻辑处理 33411.4.8 异常视图的处理 33411.4.9 根据视图跳转页面 335 第12章 远程服务 34012.1 RMI 34012.1.1 使用示例 34012.1.2 服务端实现 34212.1.3 客户端实现 35012.2 HttpInvoker 35512.2.1 使用示例 35612.2.2 服务端实现 35712.2.3 客户端实现 361 第13章 Spring消息 36713.1 JMS的独立使用 36713.2 Spring整合ActiveMQ 36913.3 源码分析 37113.3.1 JmsTemplate 37213.3.2 监听器容器 376","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"}]},{"title":"《Spring实战（第4版）》读书笔记","slug":"《Spring实战（第4版）》读书笔记","date":"2017-10-23T01:15:43.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/10/23/《Spring实战（第4版）》读书笔记/","link":"","permalink":"https://liyong.ac.cn/2017/10/23/《Spring实战（第4版）》读书笔记/","excerpt":"第1部分 Spring的核心第1章 Spring之旅 31.1 简化Java开发 41.1.1 激发POJO的潜能 51.1.2 依赖注入 51.1.3 应用切面 111.1.4 使用模板消除样板式代码 161.2 容纳你的Bean 181.2.1 使用应用上下文 191.2.2 bean的生命周期 201.3 俯瞰Spring风景线 211.3.1 Spring模块 221.3.2 Spring Portfolio 241.4 Spring的新功能 271.4.1 Spring 3.1新特性 271.4.2 Spring 3.2新特性 281.4.3 Spring 4.0新特性 30","text":"第1部分 Spring的核心第1章 Spring之旅 31.1 简化Java开发 41.1.1 激发POJO的潜能 51.1.2 依赖注入 51.1.3 应用切面 111.1.4 使用模板消除样板式代码 161.2 容纳你的Bean 181.2.1 使用应用上下文 191.2.2 bean的生命周期 201.3 俯瞰Spring风景线 211.3.1 Spring模块 221.3.2 Spring Portfolio 241.4 Spring的新功能 271.4.1 Spring 3.1新特性 271.4.2 Spring 3.2新特性 281.4.3 Spring 4.0新特性 30 第2章 装配Bean 332.1 Spring配置的可选方案 342.2 自动化装配bean 352.2.1 创建可被发现的bean 352.2.2 为组件扫描的bean命名 382.2.3 设置组件扫描的基础包 392.2.4 通过为bean添加注解实现自动装配 402.2.5 验证自动装配 422.3 通过Java代码装配bean 442.3.1 创建配置类 442.3.2 声明简单的bean 452.3.3 借助JavaConfig实现注入 462.4 通过XML装配bean 482.4.1 创建XML配置规范 482.4.2 声明一个简单的[bean] 492.4.3 借助构造器注入初始化bean 502.4.4 设置属性 562.5 导入和混合配置 612.5.1 在JavaConfig中引用XML配置 612.5.2 在XML配置中引用JavaConfig 63第3章 高级装配 673.1 环境与profile 673.1.1 配置profile bean 693.1.2 激活profile 733.2 条件化的bean 753.3 处理自动装配的歧义性 783.3.1 标示首选的bean 793.3.2 限定自动装配的bean 803.4 bean的作用域 843.4.1 使用会话和请求作用域 863.4.2 在XML中声明作用域代理 883.5 运行时值注入 883.5.1 注入外部的值 893.5.2 使用Spring表达式语言进行装配 93##第4章 面向切面的Spring 101 4.1 什么是面向切面编程？ 1024.1.1 定义AOP术语 1034.1.2 Spring对AOP的支持 1054.2 通过切点来选择连接点 1074.2.1 编写切点 1084.2.2 在切点中选择bean 1094.3 使用注解创建切面 1094.3.1 定义切面 1104.3.2 创建环绕通知 1144.3.3 处理通知中的参数 1154.3.4 通过注解引入新功能 1184.4 在XML中声明切面 1204.4.1 声明前置和后置通知 1224.4.2 声明环绕通知 1244.4.3 为通知传递参数 1254.4.4 通过切面引入新的功能 1274.5 注入AspectJ切面 128#第２部分 Web中的Spring 第5章 构建Spring Web应用程序 1355.1 Spring MVC起步 1365.1.1 跟踪Spring MVC的请求 1365.1.2 搭建Spring MVC 1385.1.3 Spittr应用简介 1425.2 编写基本的控制器 1435.2.1 测试控制器 1455.2.2 定义类级别的请求处理 1465.2.3 传递模型数据到视图中 1475.3 接受请求的输入 1535.3.1 处理查询参数 1535.3.2 通过路径参数接受输入 1555.4 处理表单 1575.4.1 编写处理表单的控制器 1605.4.2 校验表单 1635.5 小结 166第6章 渲染Web视图 1676.1 理解视图解析 1676.2 创建JSP视图 1706.2.1 配置适用于JSP的视图解析器 1706.2.2 使用Spring的JSP库 1726.3 使用Apache Tiles视图定义布局 1846.3.1 配置Tiles视图解析器 1856.4 使用Thymeleaf 1906.4.1 配置Thymeleaf视图解析器 1906.4.2 定义Thymeleaf模板 1926.5 小结 196第7章 Spring MVC的高级技术 1977.1 Spring MVC配置的替代方案 1987.1.1 自定义DispatcherServlet配置 1987.1.2 添加其他的Servlet和Filter 1997.1.3 在web.xml中声明DispatcherServlet 2017.2 处理multipart形式的数据 2047.2.1 配置multipart解析器 2057.2.2 处理multipart请求 2087.3 处理异常 2127.3.1 将异常映射为HTTP状态码 2137.3.2 编写异常处理的方法 2147.4 为控制器添加通知 2167.5 跨重定向请求传递数据 2177.5.1 通过URL模板进行重定向 2187.5.2 使用flash属性 2197.6 小结 221第8章 使用Spring WebFlow 2238.1 在Spring中配置Web Flow 2248.1.1 装配流程执行器 2248.1.2 配置流程注册表 2248.1.3 处理流程请求 2258.2 流程的组件 2268.2.1 状态 2268.2.2 转移 2308.2.3 流程数据 2318.3 组合起来：披萨流程 2328.3.1 定义基本流程 2338.3.2 收集顾客信息 2368.3.2 构建订单 2428.3.2 支付 2448.4 保护Web流程 2468.5 小结 246第9章 保护Web应用 2499.1 Spring Security简介 2509.1.1 理解Spring Security的模块 2509.1.2 过滤Web请求 2519.1.3 编写简单的安全性配置 2529.2 选择查询用户详细信息的服务 2559.2.1使用基于内存的用户存储 2559.2.2 基于数据库表进行认证 2579.2.3 基于LDAP进行认证 2599.2.4 配置自定义的用户服务 2639.3 拦截请求 2659.3.1 使用Spring表达式进行安全保护 2679.3.2 强制通道的安全性 2699.3.3 防止跨站请求伪造 2709.4 认证用户 2719.4.1 添加自定义的登录页 2729.4.2 启用HTTP Basic认证 2749.4.3 启用Remember-me功能 2749.4.4 退出 2759.5 保护视图 2769.5.1 使用Spring Security的JSP标签库 2769.5.2 使用Thymeleaf的SpringSecurity方言 2809.6 小结 281第3部分 后端中的Spring第10章 通过Spring和JDBC征服数据库 28510.1 Spring的数据访问哲学 28610.1.1 了解Spring的数据访问异常体系 28710.1.2 数据访问模板化 28910.2 配置数据源 29110.2.1 使用JNDI数据源 29210.2.2 使用数据源连接池 29210.2.3 基于JDBC驱动的数据源 29410.2.4 使用嵌入式的数据源 29510.2.5 使用profile选择数据源 29610.3 在Spring中使用JDBC 29810.3.1 应对失控的JDBC代码 29910.3.2 使用JDBC模板 30210.4 小结 307第11章 使用对象-关系映射持久化数据 30911.1 在Spring中集成Hibernate 31011.1.1 声明Hibernate的Session工厂 31111.1.2 构建不依赖于Spring的Hibernate代码 31311.2 Spring与Java持久化API 31511.2.1 配置实体管理器工厂 31511.2.2 编写基于JPA的Repository 32011.3 借助Spring Data实现自动化的JPA Repository 32211.3.1 定义查询方法 32511.3.2 声明自定义查询 32811.3.3 混合自定义的功能 32911.4 小结 330第12章 使用NoSQL数据库 33312.1 使用MongoDB持久化文档数据 33412.1.1 启用MongoDB 33512.1.2 为模型添加注解，实现MongoDB持久化 33812.1.3 使用MongoTemplate访问MongoDB 34112.1.4 编写MongoDBRepository 34212.2 使用Neo4j操作图数据 34712.2.1 配置Spring DataNeo4j 34712.2.2 使用注解标注图实体 35012.2.3 使用Neo4jTemplate 35312.2.4 创建自动化的Neo4j Repository 35412.3 使用Redis操作key-value数据 35912.3.1 连接到Redis 35912.3.2 使用RedisTemplate 36012.3.3 使用key和value的序列化器 36412.4 小结 365第13章 缓存数据 36713.1 启用对缓存的支持 36813.1.1 配置缓存管理器 36913.2 为方法添加注解以支持缓存 37313.2.1 填充缓存 37413.2.2 移除缓存条目 37813.3 使用XML声明缓存 37913.4 小结 383 第14章 保护方法应用 38514.1 使用注解保护方法 38614.1.1 使用@Secured注解限制方法调用 38614.1.2 在Spring Security中使用JSR-250的@RolesAllowed注解 38714.2 使用表达式实现方法级别的安全性 38814.2.1 表述方法访问规则 38914.2.2 过滤方法的输入和输出 39114.3 小结 395 第4部分 Spring集成第15章 使用远程服务 39915.1 Spring远程调用概览 40015.2 使用RMI 40215.2.1 导出RMI服务 40315.2.2 装配RMI服务 40515.3 使用Hessian和Burlap发布远程服务 40715.3.1 使用Hessian和Burlap导出bean的功能 40815.3.2 访问Hessian/Burlap服务 41115.4 使用Spring的HttpInvoker 41315.4.1 将bean导出为HTTP服务 41315.4.2 通过HTTP访问服务 41415.5 发布和使用Web服务 41615.5.1 创建基于Spring的JAX-WS端点 41615.5.2 在客户端代理JAX-WS服务 41915.6 小结 421第16章 使用Spring MVC创建REST API 42316.1 了解REST 42416.1.1 REST的基础知识 42416.1.2 Spring是如何支持REST的 42516.2 创建第一个REST端点 42616.2.1 协商资源表述 42816.2.2 使用HTTP信息转换器 43316.3 提供资源之外的其他内容 43816.3.1 发送错误信息到客户端 43816.3.2 在响应中设置头部信息 44316.4 编写REST客户端 44516.4.1 了解RestTemplate的操作 44616.4.2 GET资源 44716.4.3 检索资源 44816.4.4 抽取响应的元数据 44916.4.5 PUT资源 45016.4.6 DELETE资源 45116.4.7 POST资源数据 45216.4.8 在POST请求中获取响应对象 45216.4.9 在POST请求后获取资源位置 45316.4.10 交换资源 45416.5 小结 456第17章 Spring消息 45717.1 异步消息简介 45817.1.1 发送消息 45917.1.2 评估异步消息的优点 46117.2 使用JMS发送消息 46317.2.1 在Spring中搭建消息代理 46317.2.2 使用Spring的JMS模板 46517.2.3 创建消息驱动的POJO 47417.2.4 使用基于消息的RPC 47717.3 使用AMQP实现消息功能 47917.3.1 AMQP简介 48017.3.2 配置Spring支持AMQP消息 48117.3.3 使用RabbitTemplate发送消息 48417.3.4 接收AMQP消息 48617.4 小结 489 第18章 使用WebSocket和STOMP实现消息功能 49118.1 使用Spring的低层级WebSocket API 49218.2 应对不支持WebSocket的场景 49718.3 使用STOMP消息 50018.3.1 启用STOMP消息功能 50118.3.2 处理来自客户端的STOMP消息 50418.3.3 发送消息到客户端 50718.4 为目标用户发送消息 51118.4.1 在控制器中处理用户的消息 51218.4.2 为指定用户发送消息 51418.5 处理消息异常 51518.6 小结 516第19章 使用Spring发送Email 51719.1 配置Spring发送邮件 51819.1.1 配置邮件发送器 51819.1.2 装配和使用邮件发送器 52019.2 构建丰富内容的Email消息 52119.2.1 添加附件 52119.2.2 发送富文本内容的Email 52219.3 使用模板生成Email 52419.3.1 使用Velocity构建Email消息 52419.3.2 使用Thymeleaf构建Email消息 52619.4 小结 528 第20章 使用JMX管理Spring Bean 52920.1 将Spring bean导出为MBean 53020.1.1 通过名称暴露方法 53320.1.2 使用接口定义MBean的操作和属性 53520.1.3 使用注解驱动的MBean 53620.1.4 处理MBean冲突 53820.2 远程MBean 53920.2.1 暴露远程MBean 53920.2.2 访问远程MBean 54020.2.3 代理MBean 54220.3 处理通知 54320.3.1 监听通知 544第21章 借助Spring Boot简化Spring开发 54721.1 Spring Boot简介 54821.1.1 添加Starter依赖 54821.1.2 自动配置 55221.1.3 Spring Boot CLI 55221.1.4 Actuator 55321.2 使用Spring Boot构建应用 55321.2.1 处理请求 55621.2.2 创建视图 55821.2.3 添加静态内容 56021.2.4 持久化数据 56121.2.5 尝试运行 56321.3 组合使用Groovy与SpringBoot CLI 56621.3.1 编写Groovy控制器 56621.3.2 使用Groovy Repository实现数据持久化 56921.3.3 运行Spring Boot CLI 57021.4 通过Actuator获取了解应用内部状况 57121.5 小结 574","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"}]},{"title":"Javadoc","slug":"Javadoc","date":"2017-10-21T00:43:21.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/10/21/Javadoc/","link":"","permalink":"https://liyong.ac.cn/2017/10/21/Javadoc/","excerpt":"@link引用当前类 1Alias for &#123;@link #basePackages&#125;. 1","text":"@link引用当前类 1Alias for &#123;@link #basePackages&#125;. 1 @code1&#123;@code @ComponentScan(basePackages = \"org.my.pkg\")&#125;","categories":[{"name":"javadoc","slug":"javadoc","permalink":"https://liyong.ac.cn/categories/javadoc/"}],"tags":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"}]},{"title":"spring","slug":"spring","date":"2017-10-18T01:25:41.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/10/18/spring/","link":"","permalink":"https://liyong.ac.cn/2017/10/18/spring/","excerpt":"spring 系列知识 spring spring4all 翟永超 纯洁的微笑 liaokailin的专栏 ## bean ## container ## Processor ## Factory ## Manager ## Wrapper 继承且持有 装饰模式 ## Registry ## Context ## Resolver ## Listener 观察者模式 ## Converter Adapter适配器模式","text":"spring 系列知识 spring spring4all 翟永超 纯洁的微笑 liaokailin的专栏 ## bean ## container ## Processor ## Factory ## Manager ## Wrapper 继承且持有 装饰模式 ## Registry ## Context ## Resolver ## Listener 观察者模式 ## Converter Adapter适配器模式 springboot 梁桂钊 catoop的专栏 简书Spring Boot专题 springcloud","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"}]},{"title":"微服务设计读书笔记","slug":"微服务设计读书笔记","date":"2017-10-10T08:51:08.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/10/10/微服务设计读书笔记/","link":"","permalink":"https://liyong.ac.cn/2017/10/10/微服务设计读书笔记/","excerpt":"第1章 微服务 11.1 什么是微服务 21.1.1 很小，专注于做好一件事 21.1.2 自治性 3 1.2 主要好处 31.2.1 技术异构性 31.2.2 弹性 41.2.3 扩展 51.2.4 简化部署 51.2.5 与组织结构相匹配 61.2.6 可组合性 61.2.7 对可替代性的优化 6 1.3 面向服务的架构 71.4 其他分解技术 71.4.1 共享库 81.4.2 模块 8 1.5 没有银弹 9","text":"第1章 微服务 11.1 什么是微服务 21.1.1 很小，专注于做好一件事 21.1.2 自治性 3 1.2 主要好处 31.2.1 技术异构性 31.2.2 弹性 41.2.3 扩展 51.2.4 简化部署 51.2.5 与组织结构相匹配 61.2.6 可组合性 61.2.7 对可替代性的优化 6 1.3 面向服务的架构 71.4 其他分解技术 71.4.1 共享库 81.4.2 模块 8 1.5 没有银弹 9 第2章 演化式架构师 112.1 不准确的比较 112.2 架构师的演化视角 122.3 分区 142.4 一个原则性的方法 152.4.1 战略目标 152.4.2 原则 152.4.3 实践 162.4.4 将原则和实践相结合 162.4.5 真实世界的例子 16 2.5 要求的标准 172.5.1 监控 182.5.2 接口 182.5.3 架构安全性 18 2.6 代码治理 182.6.1 范例 192.6.2 裁剪服务代码模板 19 2.7 技术债务 202.8 例外管理 212.9 集中治理和领导 212.10 建设团队 22第3章 如何建模服务 243.1 MusicCorp简介 243.2 什么样的服务是好服务 253.2.1 松耦合 253.2.2 高内聚 25 3.3 限界上下文 263.3.1 共享的隐藏模型 263.3.2 模块和服务 273.3.3 过早划分 28 3.4 业务功能 283.5 逐步划分上下文 293.6 关于业务概念的沟通 303.7 技术边界 30第4章 集成 324.1 寻找理想的集成技术 324.1.1 避免破坏性修改 324.1.2 保证API的技术无关性 324.1.3 使你的服务易于消费方使用 334.1.4 隐藏内部实现细节 33 4.2 为用户创建接口 334.3 共享数据库 334.4 同步与异步 354.5 编排与协同 354.6 远程过程调用 384.6.1 技术的耦合 384.6.2 本地调用和远程调用并不相同 394.6.3 脆弱性 394.6.4 RPC很糟糕吗 40 4.7 REST 414.7.1 REST和HTTP 414.7.2 超媒体作为程序状态的引擎 424.7.3 JSON、XML还是其他 444.7.4 留心过多的约定 444.7.5 基于HTTP的REST的缺点 45 4.8 实现基于事件的异步协作方式 464.8.1 技术选择 464.8.2 异步架构的复杂性 47 4.9 服务即状态机 484.10 响应式扩展 484.11 微服务世界中的DRY和代码重用的危险 494.12 按引用访问 504.13 版本管理 514.13.1 尽可能推迟 514.13.2 及早发现破坏性修改 524.13.3 使用语义化的版本管理 534.13.4 不同的接口共存 534.13.5 同时使用多个版本的服务 54 4.14 用户界面 554.14.1 走向数字化 564.14.2 约束 564.14.3 API组合 574.14.4 UI片段的组合 574.14.5 为前端服务的后端 594.14.6 一种混合方式 60 4.15 与第三方软件集成 614.15.1 缺乏控制 614.15.2 定制化 624.15.3 意大利面式的集成 624.15.4 在自己可控的平台进行定制化 624.15.5 绞杀者模式 64 第5章 分解单块系统 665.1 关键是接缝 665.2 分解MusicCorp 675.3 分解单块系统的原因 685.3.1 改变的速度 685.3.2 团队结构 685.3.3 安全 685.3.4 技术 68 5.4 杂乱的依赖 695.5 数据库 695.6 找到问题的关键 695.7 例子：打破外键关系 705.8 例子：共享静态数据 715.9 例子：共享数据 725.10 例子：共享表 73 5.11 重构数据库 74《数据库重构》 5.12 事务边界 755.12.1 再试一次 765.12.2 终止整个操作 775.12.3 分布式事务 775.12.4 应该怎么办呢 78 5.13 报告 785.14 报告数据库 785.15 通过服务调用来获取数据 805.16 数据导出 815.17 事件数据导出 825.18 数据导出的备份 835.19 走向实时 845.20 修改的代价 845.21 理解根本原因 84第6章 部署 866.1 持续集成简介 866.2 把持续集成映射到微服务 876.3 构建流水线和持续交付 906.4 平台特定的构建物 916.5 操作系统构建物 926.6 定制化镜像 936.6.1 将镜像作为构建物 946.6.2 不可变服务器 95 6.7 环境 956.8 服务配置 966.9 服务与主机之间的映射 976.9.1 单主机多服务 976.9.2 应用程序容器 996.9.3 每个主机一个服务 1006.9.4 平台即服务 101 6.10 自动化 1016.11 从物理机到虚拟机 1026.11.1 传统的虚拟化技术 1036.11.2 Vagrant 1046.11.3 Linux容器 1046.11.4 Docker 106 6.12 一个部署接口 107参数化的命令行调用是出发任何部署的最合理方式 第7章 测试 1107.1 测试类型 110 7.2 测试范围 111测试金字塔7.2.1 单元测试 112TDD(Test Drivern Design)7.2.2 服务测试 1137.2.3 端到端测试 1147.2.4 权衡 1147.2.5 比例 115 7.3 实现服务测试 1157.3.1 mock还是打桩 1157.3.2 智能的打桩服务 116 7.4 微妙的端到端测试 1177.5 端到端测试的缺点 1187.6 脆弱的测试 1187.6.1 谁来写这些测试 1197.6.2 测试多长时间 1197.6.3 大量的堆积 1207.6.4 元版本 120 7.7 测试场景，而不是故事 1217.8 拯救消费者驱动的测试 1217.8.1 Pact 1237.8.2 关于沟通 124 7.9 还应该使用端到端测试吗 1247.10 部署后再测试 1257.10.1 区分部署和上线 1257.10.2 金丝雀发布 1267.10.3 平均修复时间胜过平均故障间隔时间 127 7.11 跨功能的测试 128第8章 监控 1318.1 单一服务，单一服务器 1328.2 单一服务，多个服务器 1328.3 多个服务，多个服务器 1338.4 日志，日志，更多的日志 1348.5 多个服务的指标跟踪 1358.6 服务指标 1358.7 综合监控 1368.8 关联标识 1378.9 级联 1398.10 标准化 1398.11 考虑受众 1408.12 未来 1408.13 小结 141第9章 安全 1439.1 身份验证和授权 1439.1.1 常见的单点登录实现 1449.1.2 单点登录网关 1459.1.3 细粒度的授权 1469.2 服务间的身份验证和授权 1469.2.1 在边界内允许一切 1469.2.2 HTTP(S) 基本身份验证 1479.2.3 使用SAML或OpenID Connect 1489.2.4 客户端证书 1489.2.5 HTTP之上的HMAC 1499.2.6 API密钥 1499.2.7 代理问题 1509.3 静态数据的安全 1529.3.1 使用众所周知的加密算法 1529.3.2 一切皆与密钥相关 1539.3.3 选择你的目标 1539.3.4 按需解密 1539.3.5 加密备份 1539.4 深度防御 1549.4.1 防火墙 1549.4.2 日志 1549.4.3 入侵检测（和预防）系统 1549.4.4 网络隔离 1559.4.5 操作系统 1559.5 一个示例 1569.6 保持节俭 1589.7 人的因素 1589.8 黄金法则 1589.9 内建安全 1599.10 外部验证 1599.11 小结 159第10章 康威定律和系统设计 16110.1 证据 16110.1.1 松耦合组织和紧耦合组织 16210.1.2 Windows Vista 16210.2 Netflix和Amazon 16210.3 我们可以做什么 16310.4 适应沟通途径 16310.5 服务所有权 16410.6 共享服务的原因 16410.6.1 难以分割 16410.6.2 特性团队 16410.6.3 交付瓶颈 16510.7 内部开源 16610.7.1 守护者的角色 16610.7.2 成熟 16610.7.3 工具 16710.8 限界上下文和团队结构 16710.9 孤儿服务 16710.10 案例研究：RealEstate.com.au 16810.11 反向的康威定律 16910.12 人 17010.13 小结 170第11章 规模化微服务 17111.1 故障无处不在 17111.2 多少是太多 17211.3 功能降级 17311.4 架构性安全措施 17411.5 反脆弱的组织 17511.5.1 超时 17611.5.2 断路器 17611.5.3 舱壁 17811.5.4 隔离 17911.6 幂等 17911.7 扩展 18011.7.1 更强大的主机 18111.7.2 拆分负载 18111.7.3 分散风险 18111.7.4 负载均衡 18211.7.5 基于worker的系统 18411.7.6 重新设计 18411.8 扩展数据库 18511.8.1 服务的可用性和数据的持久性 18511.8.2 扩展读取 18511.8.3 扩展写操作 18611.8.4 共享数据库基础设施 18711.8.5 CQRS 18711.9 缓存 18811.9.1 客户端、 代理和服务器端缓存 18811.9.2 HTTP缓存 18911.9.3 为写使用缓存 19011.9.4 为弹性使用缓存 19011.9.5 隐藏源服务 19111.9.6 保持简单 19111.9.7 缓存中毒：一个警示 19211.10 自动伸缩 19211.11 CAP定理 19311.11.1 牺牲一致性 19411.11.2 牺牲可用性 19511.11.3 牺牲分区容忍性 19511.11.4 AP还是CP 19611.11.5 这不是全部或全不 19611.11.6 真实世界 19711.12 服务发现 19711.13 动态服务注册 19911.13.1 Zookeeper 19911.13.2 Consul 20011.13.4 构造你自己的系统 20111.13.5 别忘了人 20111.14 文档服务 20111.14.1 Swagger 20211.14.2 HAL 和HAL浏览器 20211.15 自描述系统 20311.16 小结 203第12章 总结 20412.1 微服务的原则 20412.1.1 围绕业务概念建模 20512.1.2 接受自动化文化 20512.1.3 隐藏内部实现细节 20512.1.4 让一切都去中心化 20612.1.5 可独立部署 20612.1.6 隔离失败 20612.1.7 高度可观察 20712.2 什么时候你不应该使用微服务 20712.3 临别赠言 208关于作者 209关于封面 209","categories":[{"name":"ms","slug":"ms","permalink":"https://liyong.ac.cn/categories/ms/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"https://liyong.ac.cn/tags/笔记/"}]},{"title":"react资源","slug":"react资源","date":"2017-09-26T01:15:32.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/09/26/react资源/","link":"","permalink":"https://liyong.ac.cn/2017/09/26/react资源/","excerpt":"业务开发 react redux redux-saga react-router immutable.js styled-component","text":"业务开发 react redux redux-saga react-router immutable.js styled-component 代码规范检查 eslint flow 单元测试工具 jest chai enzyme 学习材料http://www.ruanyifeng.com/blog/2016/09/react-technology-stack.html http://huziketang.com/books/react/ 推荐UI框架antd","categories":[],"tags":[]},{"title":"死磕自己","slug":"死磕自己","date":"2017-09-22T02:16:58.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/09/22/死磕自己/","link":"","permalink":"https://liyong.ac.cn/2017/09/22/死磕自己/","excerpt":"JVMGC类加载机制内存","text":"JVMGC类加载机制内存 SE数据结构LinkedList，ArrayListHashMap，TreeMap，ConcurrentHashMap、LinkedHashMap Threadthread.join 底层是调用的object的wait方法,线程结束的时候会调用notifyAll（hotspot的源码中） 并发包NIO包分布式CURDspring源码分析12345678910111213141516171819202122232425262728293031323334public abstract class Base&lt;T&gt; &#123; public void save(T vo) &#123; // 子类前置处理 subBeforeSave(vo); // 父类的前置链处理 beforeSave(vo); // 子类处理 subSave(vo); // 父类的后置链处理 afterSave(vo); // 子类后置处理 subAfterSave(vo); &#125;; public abstract void subBeforeSave(T vo); /** * 责任链 */ public void beforeSave(T vo) &#123; &#125;; /** * 责任链 */ public void afterSave(T vo) &#123; &#125;; public abstract void subSave(T vo); public abstract void subAfterSave(T vo);&#125; 所有代码 数据 、指令，返回地址 线程计数器 本地方法栈虚拟机栈局部变量表 32 ，所以long double需要占用两行操作数栈，做计算运到的动态连接出口，正常，异常 方法去堆 看源码-猜想-验证 重构","categories":[{"name":"死磕","slug":"死磕","permalink":"https://liyong.ac.cn/categories/死磕/"}],"tags":[]},{"title":"git","slug":"git","date":"2017-09-11T12:26:49.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/09/11/git/","link":"","permalink":"https://liyong.ac.cn/2017/09/11/git/","excerpt":"远程仓库检出仓库：$ git clone git://github.com/jquery/jquery.git查看远程仓库：$ git remote -v拉取远程仓库：$ git pull [remoteName] [localBranchName]推送远程仓库：$ git push [remoteName] [localBranchName]","text":"远程仓库检出仓库：$ git clone git://github.com/jquery/jquery.git查看远程仓库：$ git remote -v拉取远程仓库：$ git pull [remoteName] [localBranchName]推送远程仓库：$ git push [remoteName] [localBranchName] 分支(branch)查看本地分支：$ git branch查看远程分支：$ git branch -r创建本地分支：$ git branch [name] —-注意新分支创建后不会自动切换为当前分支切换分支：$ git checkout [name]创建新分支并立即切换到新分支：$ git checkout -b [name]删除分支：$ git branch -d [name] —- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项合并分支：$ git merge [name] —-将名称为[name]的分支与当前分支合并创建远程分支(本地分支push到远程)：$ git push origin [name]删除远程分支：$ git push origin :heads/[name]$ git push origin test:master // 提交本地test分支作为远程的master分支$ git push origin test:test // 提交本地test分支作为远程的test分支,如果没有远程分支，自动创建 版本(tag)操作相关命令查看版本：$ git tag创建版本：$ git tag [name]删除版本：$ git tag -d [name]查看远程版本：$ git tag -r创建远程版本(本地版本push到远程)：$ git push origin [name]删除远程版本：$ git push origin :refs/tags/[name] 忽略一些文件、文件夹不提交在仓库根目录下创建名称为“.gitignore”的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如 1234/target/.project.settings/.classpath git删除文件1234rm add2.txtgit rm add2.txtgit commit -m \"rm test\"git push web 冲突回滚先显示提交的log 123456789101112131415161718$ git log -3commit 4dc08bb8996a6ee02fAuthor: Mark &lt;xxx@xx.com&gt;Date: Wed Sep 7 08:08:53 2016 +0800 xxxxxcommit 9cac9ba76574da2167Author: xxx&lt;xx@qq.com&gt;Date: Tue Sep 6 22:18:59 2016 +0800 improved the requstcommit e377f60e28c8b84158Author: xxx&lt;xxx@qq.com&gt;Date: Tue Sep 6 14:42:44 2016 +0800 changed the password from empty to max123 回滚到指定的版本 1git reset --hard e377f60e28c8b84158 强制提交 1git push -f origin master 换行符GNU/Linux和Mac OS使用换行(LF)或新行作为行结束字符，而Windows使用换行和回车(LFCR)组合来表示行结束字符。 为了避免这些行结尾的差异的不必要提交，我们必须配置Git客户端写入与Git仓库使用相同的行结束符。 对于Windows系统，可以将Git客户端配置为将行结束符转换为CRLF格式，同时退出，并在提交操作时将其转换回LF格式。以下可根据您的需要来设置。 1$ git config --global core.autocrlf true 取消本地变更git checkout – …” to discard changes in working directory)","categories":[{"name":"git","slug":"git","permalink":"https://liyong.ac.cn/categories/git/"}],"tags":[]},{"title":"docker基础入门","slug":"docker基础入门","date":"2017-09-11T12:26:49.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/09/11/docker基础入门/","link":"","permalink":"https://liyong.ac.cn/2017/09/11/docker基础入门/","excerpt":"资料http://www.runoob.com/ 常用命令12yum install dockerdocker version","text":"资料http://www.runoob.com/ 常用命令12yum install dockerdocker version 2 核心概念与安装配置centos安装12345yum install dockersystemctl start docker.service systemctl enable docker.service 3 使用docker镜像Dockerfile123456789101112131415161718192021222324FROM centos:7.2.1511#作者信息MAINTAINER yzzhouyalei \"yzzhouyalei@foxmail.com\"#创建目录RUN mkdir -p /z/java/jdk#把当前目录下的jdk文件夹添加到镜像ADD jdk /z/java/jdk#创建tomcat目录RUN mkdir -p /z/java/tomcat#把当前目录下的tomcat文件夹添加到镜像ADD tomcat /z/java/tomcat#添加环境变量ENV JAVA_HOME /z/java/jdkENV CATALINA_HOME /z/java/tomcatENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/bin#暴露8080端口EXPOSE 8080#启动时运行tomcatCMD [\"/z/java/tomcat/bin/catalina.sh\",\"run\"] FROMRUNMAINTAINERCMDENTRYPOINTUSEREXPOSEENVADDVOLUMEWORKDIRONBUILD","categories":[{"name":"docker","slug":"docker","permalink":"https://liyong.ac.cn/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://liyong.ac.cn/tags/docker/"}]},{"title":"高效团队开发工具与方法读后感","slug":"高效团队开发工具与方法读书笔记","date":"2017-09-07T08:12:25.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/09/07/高效团队开发工具与方法读书笔记/","link":"","permalink":"https://liyong.ac.cn/2017/09/07/高效团队开发工具与方法读书笔记/","excerpt":"1.什么是团队开发2.团队开发中发生的问题（痛点）重要的邮件太多，无法确定处理没有能用于验证的环境用别名目录管理分支重新制作数据库比较困难不运行系统就无法察觉问题覆盖了其他组员修改的代码无法自行地进行代码重构不知道bug的修正日期，也不能追踪退化没有灵活使用分支和标签在测试环境、正式环境无法运行发布太复杂，以至于需要发布手册","text":"1.什么是团队开发2.团队开发中发生的问题（痛点）重要的邮件太多，无法确定处理没有能用于验证的环境用别名目录管理分支重新制作数据库比较困难不运行系统就无法察觉问题覆盖了其他组员修改的代码无法自行地进行代码重构不知道bug的修正日期，也不能追踪退化没有灵活使用分支和标签在测试环境、正式环境无法运行发布太复杂，以至于需要发布手册 3.版本管理管理的对象代码需求资料、设计资料等文档数据库模式、数据配置文件 使用Git顺利地推进并行开发分支的用法标签的用法 分支的策略git-flowgithub-flow 数据库模式和数据的管理版本管理的必要条件无论什么环境都能用相同的步骤能够反复执行多次文本文件 数据库迁移的功能管理SQL执行顺序和需要执行哪些SQL管理模式定义编辑的冲突提供回滚的机制支持数据的加载 数据迁移工具Migration(Ruby on Rails)south(Django)Migration Plugin(CakePHP)Evolution(Play Framework) java调用的数据迁移工具FlywayLiquibasedbdeploy 4.缺陷管理回答“那个bug是什么时候修正的”的问题 回答“为什么要这样修改”的问题 OSS产品TracRedmineBugzillaMantis 商用产品JIRAYouTRACKPivotal TrackerBacklogGitHub 缺陷管理系统与版本管理系统的关联从提交链接到问题票从问题票到链接 5.CI(持续集成)jenkins能干什么checkout代码自动build并执行测试统计结果并制作报表通知 统计覆盖率覆盖率统计工具CoberturaJacocoScctSimpleCovRcov Jenkins插件的配置配置插件 Publish Cobertura coverage Report Jenkins中静态分析CheckStylePMDFindBugs 6.部署的自动化(持续交付)引导（Bootstrapping）KickstartKickstart是一种无人职守安装方式。KickStart的工作原理是通过记录典型的安装过程中所需人工干预填写的各种参数，并生成一个名为ks.cfg的文件 VagrantVagrant是一个基于Ruby的工具，用于创建和部署虚拟化开发环境。它 使用Oracle的开源VirtualBox虚拟化系统，使用 Chef创建自动化虚拟环境 自动化配置工具chef只要准备好名为Cookbooks的服务器构建手册的配置文件，就能按照文件所记述的规则为服务其安装软件包并配置中间件 serverspec对服务器的构成进行单元测试的测试框架 7.回归测试以检查退化为目的的测试为回归测试自动化测试工具：Fit、Selenium、Cucumber、Watir、Canoo WebTest Selenium关联Jenkins和Selenium 利用Jenkins的分布式构建实现测试的并行执行 Selenium的组件Selenium IDESelenium Remote Control（Selenium RCSelenium WebDriver JMeter","categories":[{"name":"方法论","slug":"方法论","permalink":"https://liyong.ac.cn/categories/方法论/"}],"tags":[{"name":"方法论","slug":"方法论","permalink":"https://liyong.ac.cn/tags/方法论/"}]},{"title":"Android","slug":"Android","date":"2017-08-20T02:17:07.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/08/20/Android/","link":"","permalink":"https://liyong.ac.cn/2017/08/20/Android/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"spring概述","slug":"spring概述","date":"2017-07-12T11:08:01.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/07/12/spring概述/","link":"","permalink":"https://liyong.ac.cn/2017/07/12/spring概述/","excerpt":"","text":"Loggingspring依赖的日志框架为commons-logging commons-logging在启动的时候检查类路径下有没有对应的日志框架，如果有则用，如果没有直接用jdk自带的日志 ApplicationContextCircular dependencies For example: Class A requires an instance of class B through constructor injection, and class B requires an instance of class A through constructor injection. If you configure beans for classes A and B to be injected into each other, the Spring IoC container detects this circular reference at runtime, and throws a BeanCurrentlyInCreationException 解决循环依赖的最好办法是通过属性注入，不用通过构造器注入 单例对象引用非单例对象scopeThe prototype scopespring不负责完整的生命周期，只负责创建的回调函数，不负责销毁的回调函数To get the Spring container to release resources held by prototype-scoped beans, try using a custom bean post-processor, which holds a reference to beans that need to be cleaned up.这个作用域设计的目的就是替换new,没有其他的功能 Request scope基本和prototype一样，除了spring负责清理创建的对象","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/tags/spring/"}]},{"title":"OAuth2soohttp","slug":"OAuth2soohttp","date":"2017-07-12T11:04:34.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/07/12/OAuth2soohttp/","link":"","permalink":"https://liyong.ac.cn/2017/07/12/OAuth2soohttp/","excerpt":"默认GET http://localhost:9999/client HTTP/1.1requestGET http://localhost:9999/client HTTP/1.1Host: localhost:9999Connection: keep-aliveUpgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8Accept-Encoding: gzip, deflate, sdch, brAccept-Language: zh-CN,zh;q=0.8,en;q=0.6Cookie: _ga=GA1.1.1706053280.1487319818 responseHTTP/1.1 302Set-Cookie: JSESSIONID=E6B077995912BA756067F58D89AEB3EF; Path=/; HttpOnlyX-Content-Type-Options: nosniffX-XSS-Protection: 1; mode=blockCache-Control: no-cache, no-store, max-age=0, must-revalidatePragma: no-cacheExpires: 0X-Frame-Options: DENYLocation: http://localhost:9999/loginContent-Length: 0Date: Sun, 06 Aug 2017 01:18:11 GMT","text":"默认GET http://localhost:9999/client HTTP/1.1requestGET http://localhost:9999/client HTTP/1.1Host: localhost:9999Connection: keep-aliveUpgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8Accept-Encoding: gzip, deflate, sdch, brAccept-Language: zh-CN,zh;q=0.8,en;q=0.6Cookie: _ga=GA1.1.1706053280.1487319818 responseHTTP/1.1 302Set-Cookie: JSESSIONID=E6B077995912BA756067F58D89AEB3EF; Path=/; HttpOnlyX-Content-Type-Options: nosniffX-XSS-Protection: 1; mode=blockCache-Control: no-cache, no-store, max-age=0, must-revalidatePragma: no-cacheExpires: 0X-Frame-Options: DENYLocation: http://localhost:9999/loginContent-Length: 0Date: Sun, 06 Aug 2017 01:18:11 GMT GET http://localhost:9999/login HTTP/1.1requestGET http://localhost:9999/login HTTP/1.1Host: localhost:9999Connection: keep-aliveUpgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8Accept-Encoding: gzip, deflate, sdch, brAccept-Language: zh-CN,zh;q=0.8,en;q=0.6Cookie: _ga=GA1.1.1706053280.1487319818; JSESSIONID=E6B077995912BA756067F58D89AEB3EF responseHTTP/1.1 302X-Content-Type-Options: nosniffX-XSS-Protection: 1; mode=blockCache-Control: no-cache, no-store, max-age=0, must-revalidatePragma: no-cacheExpires: 0X-Frame-Options: DENYLocation: http://localhost:8080/oauth/authorize?client_id=acme&amp;redirect_uri=http://localhost:9999/login&amp;response_type=code&amp;state=y2YkbaContent-Length: 0Date: Sun, 06 Aug 2017 01:18:11 GMT http://localhost:8080/oauth/authorizerequestGET http://localhost:8080/oauth/authorize?client_id=acme&amp;redirect_uri=http://localhost:9999/login&amp;response_type=code&amp;state=y2Ykba HTTP/1.1Host: localhost:8080Connection: keep-aliveUpgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8Accept-Encoding: gzip, deflate, sdch, brAccept-Language: zh-CN,zh;q=0.8,en;q=0.6Cookie: _ga=GA1.1.1706053280.1487319818; JSESSIONID=E6B077995912BA756067F58D89AEB3EF responseHTTP/1.1 401WWW-Authenticate: Basic realm=”Spring”X-Content-Type-Options: nosniffX-XSS-Protection: 1; mode=blockCache-Control: no-cache, no-store, max-age=0, must-revalidatePragma: no-cacheExpires: 0X-Frame-Options: DENYStrict-Transport-Security: max-age=31536000 ; includeSubDomainsContent-Type: text/html;charset=ISO-8859-1Content-Language: zh-CNContent-Length: 344Date: Sun, 06 Aug 2017 01:18:12 GMT Whitelabel Error PageThis application has no explicit mapping for /error, so you are seeing this as a fallback.Sun Aug 06 09:18:12 CST 2017There was an unexpected error (type=Unauthorized, status=401).Full authentication is required to access this resource ### request GET http://localhost:8080/oauth/authorize?client_id=acme&redirect_uri=http://localhost:9999/login&response_type=code&state=y2Ykba HTTP/1.1 Host: localhost:8080 Connection: keep-alive Authorization: Basic dXNlcjpwYXNzd29yZA== Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Encoding: gzip, deflate, sdch, br Accept-Language: zh-CN,zh;q=0.8,en;q=0.6 Cookie: _ga=GA1.1.1706053280.1487319818; JSESSIONID=E6B077995912BA756067F58D89AEB3EF ### response HTTP/1.1 302 X-Application-Context: application Cache-Control: no-store X-Content-Type-Options: nosniff X-XSS-Protection: 1; mode=block X-Frame-Options: DENY Strict-Transport-Security: max-age=31536000 ; includeSubDomains Location: http://localhost:9999/login?code=i3aw0O&state=y2Ykba Content-Language: zh-CN Content-Length: 0 Date: Sun, 06 Aug 2017 01:18:14 GMT ## GET http://localhost:9999/login?code=i3aw0O&state=y2Ykba HTTP/1.1 ### request GET http://localhost:9999/login?code=i3aw0O&state=y2Ykba HTTP/1.1 Host: localhost:9999 Connection: keep-alive Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Encoding: gzip, deflate, sdch, br Accept-Language: zh-CN,zh;q=0.8,en;q=0.6 Cookie: _ga=GA1.1.1706053280.1487319818; JSESSIONID=E6B077995912BA756067F58D89AEB3EF ### response HTTP/1.1 302 Set-Cookie: JSESSIONID=21C792C7AB814AF6A4365A124BB97913; Path=/; HttpOnly X-Content-Type-Options: nosniff X-XSS-Protection: 1; mode=block Cache-Control: no-cache, no-store, max-age=0, must-revalidate Pragma: no-cache Expires: 0 X-Frame-Options: DENY Location: http://localhost:9999/client Content-Length: 0 Date: Sun, 06 Aug 2017 01:18:14 GMT ## GET http://localhost:9999/client HTTP/1.1 ### request GET http://localhost:9999/client HTTP/1.1 Host: localhost:9999 Connection: keep-alive Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Encoding: gzip, deflate, sdch, br Accept-Language: zh-CN,zh;q=0.8,en;q=0.6 Cookie: _ga=GA1.1.1706053280.1487319818; JSESSIONID=21C792C7AB814AF6A4365A124BB97913 ### response HTTP/1.1 200 X-Application-Context: application:9999 X-Content-Type-Options: nosniff X-XSS-Protection: 1; mode=block Cache-Control: no-cache, no-store, max-age=0, must-revalidate Pragma: no-cache Expires: 0 X-Frame-Options: DENY Content-Type: text/html;charset=UTF-8 Content-Length: 10 Date: Sun, 06 Aug 2017 01:18:14 GMT Hello user 两次 client 分析第一次比第二次多了 CookieJSESSIONID=21C792C7AB814AF6A4365A124BB97913 两次客户都登录，第二次比第一次多了code=i3aw0O&amp;state=y2Ykba 两次授权，第二次多了Authorization: Basic dXNlcjpwYXNzd29yZA==","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"OAuth2","slug":"OAuth2","permalink":"https://liyong.ac.cn/tags/OAuth2/"},{"name":"sso","slug":"sso","permalink":"https://liyong.ac.cn/tags/sso/"}]},{"title":"OAuth2的SSO分析","slug":"OAuth2的SSO分析","date":"2017-07-12T11:04:34.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/07/12/OAuth2的SSO分析/","link":"","permalink":"https://liyong.ac.cn/2017/07/12/OAuth2的SSO分析/","excerpt":"@EnableAuthorizationServer1、@EnableAuthorizationServer：声明一个认证服务器，当用此注解后，应用启动后将自动生成几个Endpoint：（注：其实实现一个认证服务器就是这么简单，加一个注解就搞定，当然真正用到生产环境还是要进行一些配置和复写工作的。）/oauth/authorize：验证/oauth/token：获取token/oauth/confirm_access：用户授权/oauth/error：认证失败/oauth/check_token：资源服务器用来校验token/oauth/token_key：如果jwt模式则可以用此来从认证服务器获取公钥 AuthorizationEndpoint:进行授权的服务，Default URL: /oauth/authorizeTokenEndpoint：获取token的服务，Default URL: /oauth/token Resource Server:OAuth2AuthenticationProcessingFilter：给带有访问令牌的请求加载认证 从哪来到哪里去","text":"@EnableAuthorizationServer1、@EnableAuthorizationServer：声明一个认证服务器，当用此注解后，应用启动后将自动生成几个Endpoint：（注：其实实现一个认证服务器就是这么简单，加一个注解就搞定，当然真正用到生产环境还是要进行一些配置和复写工作的。）/oauth/authorize：验证/oauth/token：获取token/oauth/confirm_access：用户授权/oauth/error：认证失败/oauth/check_token：资源服务器用来校验token/oauth/token_key：如果jwt模式则可以用此来从认证服务器获取公钥 AuthorizationEndpoint:进行授权的服务，Default URL: /oauth/authorizeTokenEndpoint：获取token的服务，Default URL: /oauth/token Resource Server:OAuth2AuthenticationProcessingFilter：给带有访问令牌的请求加载认证 从哪来到哪里去 第一步访问http://localhost:9999/client第二步http://localhost:9999/loginLocationhttp://localhost:8080/oauth/authorize?client_id=acme&amp;redirect_uri=http://localhost:9999/login&amp;response_type=code&amp;state=XatSCf 第三步http://localhost:8080/oauth/authorize?client_id=acme&amp;redirect_uri=http://localhost:9999/login&amp;response_type=code&amp;state=XatSCf 第四步http://localhost:9999/login?code=D8G8Dq&amp;state=XatSCf 第五步访问http://localhost:9999/client @EnableOAuth2Client@EnableResourceServerResourceServerConfigurerAdapterResourceServer 扮演什么角色 HttpSecurity.authorizeRequests ExpressionInterceptUrlRegistry ExpressionInterceptUrlRegistry.antMatchers RequestMatcher RemoteTokenServices 配置项security.user.password: passwordsecurity.sessions: if-requiredsecurity.oauth2.client.clientId: acmesecurity.oauth2.client.clientSecret: acmesecretsecurity.oauth2.client.authorized-grant-types: ### authorization_code,refresh_token,password授权码模式（authorization code）简化模式（implicit）密码模式（resource owner password credentials）客户端模式（client credentials） security.oauth2.client.scope: openidserverClientDetailsServiceConfigurer: a configurer that defines the client details service. Client details can be initialized, or you can just refer to an existing store.AuthorizationServerSecurityConfigurer : defines the security constraints on the token endpoint.AuthorizationServerEndpointsConfigurer: defines the authorization and token endpoints and the token services. Configuring Client DetailsJdbcClientDetailsService security.basic.authorize-mode要使用权限控制模式.security.basic.enabled是否开启基本的鉴权，默认为truesecurity.basic.path需要鉴权的path，多个的话以逗号分隔，默认为[/**]security.basic.realmHTTP basic realm 的名字，默认为Springsecurity.enable-csrf是否开启cross-site request forgery校验，默认为false.security.filter-orderSecurity filter chain的order，默认为0security.headers.cache是否开启http头部的cache控制，默认为false.security.headers.content-type是否开启X-Content-Type-Options头部，默认为false.security.headers.frame是否开启X-Frame-Options头部，默认为false.security.headers.hsts指定HTTP Strict Transport Security (HSTS)模式(none, domain, all).security.headers.xss是否开启cross-site scripting (XSS) 保护，默认为false.security.ignored指定不鉴权的路径，多个的话以逗号分隔.security.oauth2.client.access-token-uri指定获取access token的URI.security.oauth2.client.access-token-validity-seconds指定access token失效时长.security.oauth2.client.additional-information.[key]设定要添加的额外信息.security.oauth2.client.authentication-scheme指定传输不记名令牌(bearer token)的方式(form, header, none,query)，默认为headersecurity.oauth2.client.authorities指定授予客户端的权限.security.oauth2.client.authorized-grant-types指定客户端允许的grant types.security.oauth2.client.auto-approve-scopes对客户端自动授权的scope.security.oauth2.client.client-authentication-scheme传输authentication credentials的方式(form, header, none, query)，默认为header方式security.oauth2.client.client-id指定OAuth2 client ID.security.oauth2.client.client-secret指定OAuth2 client secret. 默认是一个随机的secret.security.oauth2.client.grant-type指定获取资源的access token的授权类型.security.oauth2.client.id指定应用的client ID.security.oauth2.client.pre-established-redirect-uri服务端pre-established的跳转URI.security.oauth2.client.refresh-token-validity-seconds指定refresh token的有效期.security.oauth2.client.registered-redirect-uri指定客户端跳转URI，多个以逗号分隔.security.oauth2.client.resource-ids指定客户端相关的资源id，多个以逗号分隔.security.oauth2.client.scopeclient的scopesecurity.oauth2.client.token-name指定token的名称security.oauth2.client.use-current-uri是否优先使用请求中URI，再使用pre-established的跳转URI. 默认为truesecurity.oauth2.client.user-authorization-uri用户跳转去获取access token的URI.security.oauth2.resource.id指定resource的唯一标识.security.oauth2.resource.jwt.key-uriJWT token的URI. 当key为公钥时，或者value不指定时指定.security.oauth2.resource.jwt.key-valueJWT token验证的value. 可以是对称加密或者PEMencoded RSA公钥. 可以使用URI作为value.security.oauth2.resource.prefer-token-info是否使用token info，默认为truesecurity.oauth2.resource.service-id指定service ID，默认为resource.security.oauth2.resource.token-info-uritoken解码的URI.security.oauth2.resource.token-type指定当使用userInfoUri时，发送的token类型.security.oauth2.resource.user-info-uri指定user info的URIsecurity.oauth2.sso.filter-order如果没有显示提供WebSecurityConfigurerAdapter时指定的Filter order.security.oauth2.sso.login-path跳转到SSO的登录路径默认为/login.security.require-ssl是否对所有请求开启SSL，默认为false.security.sessions指定Session的创建策略(always, never, if_required, stateless).security.user.name指定默认的用户名，默认为user.security.user.password默认的用户密码.security.user.role默认用户的授权角色.","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"OAuth2","slug":"OAuth2","permalink":"https://liyong.ac.cn/tags/OAuth2/"},{"name":"sso","slug":"sso","permalink":"https://liyong.ac.cn/tags/sso/"}]},{"title":"springboot","slug":"springboot","date":"2017-07-12T11:04:34.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/07/12/springboot/","link":"","permalink":"https://liyong.ac.cn/2017/07/12/springboot/","excerpt":"MVC1ContentNegotiatingViewResolver spring.mvc.message-codes-resolver.format=PREFIX_ERROR_CODEspring.mvc.static-path-pattern=/resources/** Mustachespring-boot-maven-plugin相关连接 Test12@RunWith(SpringJUnit4ClassRunner.class)@SpringBootTest(classes = Application.class) 源码123456789101112131415BackgroundPreinitializerBasicErrorControllerSpringFactoriesLoaderConfigurableWebApplicationContextApplicationContextInitializerApplicationListenerEventPublishingRunListenerSpringBootBannerApplicationContextAwareAnnotationConfigEmbeddedWebApplicationContext YAML1234 YamlPropertiesFactoryBean YamlMapFactoryBean YamlPropertySourceLoader YAML shortcomingsYAML files can’t be loaded via the @PropertySource annotation. So in the case that you need to load values that way, you need to use a properties file. springApplication可以读取不同种类的源文件： 类- java类由AnnotatedBeanDefinitionReader加载。Resource - xml资源文件由XmlBeanDefinitionReader读取, 或者groovy脚本由GroovyBeanDefinitionReader读取Package - java包文件由ClassPathBeanDefinitionScanner扫描读取。CharSequence - 字符序列可以是类名、资源文件、包名，根据不同方式加载。如果一个字符序列不可以解析程序到类，也不可以解析到资源文件，那么就认为它是一个包。 listener最终会被初始化为ParentContextCloserApplicationListener，FileEncodingApplicationListener，AnsiOutputApplicationListener，ConfigFileApplicationListener，DelegatingApplicationListener，LiquibaseServiceLocatorApplicationListener，ClasspathLoggingApplicationListener，LoggingApplicationListener这几个类的对象组成的list。 ConfigFileApplicationListener‘spring.config.name’‘spring.config.location’ 例如，使用java -jar运行应用时，你可以通过如下命令启用debug属性： $ java -jar myproject-0.0.1-SNAPSHOT.jar –debug 配置优先级顺序 Devtools global settings properties on your home directory (~/.spring-boot-1. 1. devtools.properties when devtools is active). @TestPropertySource annotations on your tests. @SpringBootTest#properties annotation attribute on your tests. Command line arguments. Properties from SPRING_APPLICATION_JSON (inline JSON embedded in an environment variable or system property) ServletConfig init parameters. ServletContext init parameters. JNDI attributes from java:comp/env. Java System properties (System.getProperties()). OS environment variables. A RandomValuePropertySource that only has properties in random.*. Profile-specific application properties outside of your packaged jar (application-{profile}.properties and YAML variants) Profile-specific application properties packaged inside your jar (application-{profile}.properties and YAML variants) Application properties outside of your packaged jar (application.properties and YAML variants). Application properties packaged inside your jar (application.properties and YAML variants). @PropertySource annotations on your @Configuration classes. Default properties (specified using SpringApplication.setDefaultProperties). 配置文件属性文件是最常见的管理配置属性的方式。Spring Boot 提供的 SpringApplication 类会搜索并加载 application.properties 文件来获取配置属性值。SpringApplication 类会在下面位置搜索该文件。 当前目录的“/config”子目录。当前目录。classpath 中的“/config”包。classpath actuatorhttp://localhost:8881/healthhttp://localhost:8881/autoconfighttp://localhost:8881/beanshttp://localhost:8881/configpropshttp://localhost:8881/mappingshttp://localhost:8881/metricshttp://localhost:8881/dumphttp://localhost:8881/trace","text":"MVC1ContentNegotiatingViewResolver spring.mvc.message-codes-resolver.format=PREFIX_ERROR_CODEspring.mvc.static-path-pattern=/resources/** Mustachespring-boot-maven-plugin相关连接 Test12@RunWith(SpringJUnit4ClassRunner.class)@SpringBootTest(classes = Application.class) 源码123456789101112131415BackgroundPreinitializerBasicErrorControllerSpringFactoriesLoaderConfigurableWebApplicationContextApplicationContextInitializerApplicationListenerEventPublishingRunListenerSpringBootBannerApplicationContextAwareAnnotationConfigEmbeddedWebApplicationContext YAML1234 YamlPropertiesFactoryBean YamlMapFactoryBean YamlPropertySourceLoader YAML shortcomingsYAML files can’t be loaded via the @PropertySource annotation. So in the case that you need to load values that way, you need to use a properties file. springApplication可以读取不同种类的源文件： 类- java类由AnnotatedBeanDefinitionReader加载。Resource - xml资源文件由XmlBeanDefinitionReader读取, 或者groovy脚本由GroovyBeanDefinitionReader读取Package - java包文件由ClassPathBeanDefinitionScanner扫描读取。CharSequence - 字符序列可以是类名、资源文件、包名，根据不同方式加载。如果一个字符序列不可以解析程序到类，也不可以解析到资源文件，那么就认为它是一个包。 listener最终会被初始化为ParentContextCloserApplicationListener，FileEncodingApplicationListener，AnsiOutputApplicationListener，ConfigFileApplicationListener，DelegatingApplicationListener，LiquibaseServiceLocatorApplicationListener，ClasspathLoggingApplicationListener，LoggingApplicationListener这几个类的对象组成的list。 ConfigFileApplicationListener‘spring.config.name’‘spring.config.location’ 例如，使用java -jar运行应用时，你可以通过如下命令启用debug属性： $ java -jar myproject-0.0.1-SNAPSHOT.jar –debug 配置优先级顺序 Devtools global settings properties on your home directory (~/.spring-boot-1. 1. devtools.properties when devtools is active). @TestPropertySource annotations on your tests. @SpringBootTest#properties annotation attribute on your tests. Command line arguments. Properties from SPRING_APPLICATION_JSON (inline JSON embedded in an environment variable or system property) ServletConfig init parameters. ServletContext init parameters. JNDI attributes from java:comp/env. Java System properties (System.getProperties()). OS environment variables. A RandomValuePropertySource that only has properties in random.*. Profile-specific application properties outside of your packaged jar (application-{profile}.properties and YAML variants) Profile-specific application properties packaged inside your jar (application-{profile}.properties and YAML variants) Application properties outside of your packaged jar (application.properties and YAML variants). Application properties packaged inside your jar (application.properties and YAML variants). @PropertySource annotations on your @Configuration classes. Default properties (specified using SpringApplication.setDefaultProperties). 配置文件属性文件是最常见的管理配置属性的方式。Spring Boot 提供的 SpringApplication 类会搜索并加载 application.properties 文件来获取配置属性值。SpringApplication 类会在下面位置搜索该文件。 当前目录的“/config”子目录。当前目录。classpath 中的“/config”包。classpath actuatorhttp://localhost:8881/healthhttp://localhost:8881/autoconfighttp://localhost:8881/beanshttp://localhost:8881/configpropshttp://localhost:8881/mappingshttp://localhost:8881/metricshttp://localhost:8881/dumphttp://localhost:8881/trace Spring Boot应用启动器1）spring-boot-starter这是Spring Boot的核心启动器，包含了自动配置、日志和YAML。 2）spring-boot-starter-actuator帮助监控和管理应用。 3）spring-boot-starter-amqp通过spring-rabbit来支持AMQP协议（Advanced Message Queuing Protocol）。 4）spring-boot-starter-aop支持面向方面的编程即AOP，包括spring-aop和AspectJ。 5）spring-boot-starter-artemis通过Apache Artemis支持JMS的API（Java Message Service API）。 6）spring-boot-starter-batch支持Spring Batch，包括HSQLDB数据库。 7）spring-boot-starter-cache支持Spring的Cache抽象。 8）spring-boot-starter-cloud-connectors支持Spring Cloud Connectors，简化了在像Cloud Foundry或Heroku这样的云平台上连接服务。 9）spring-boot-starter-data-elasticsearch支持ElasticSearch搜索和分析引擎，包括spring-data-elasticsearch。 10）spring-boot-starter-data-gemfire支持GemFire分布式数据存储，包括spring-data-gemfire。 11）spring-boot-starter-data-jpa支持JPA（Java Persistence API），包括spring-data-jpa、spring-orm、Hibernate。 12）spring-boot-starter-data-mongodb支持MongoDB数据，包括spring-data-mongodb。 13）spring-boot-starter-data-rest通过spring-data-rest-webmvc，支持通过REST暴露Spring Data数据仓库。 14）spring-boot-starter-data-solr支持Apache Solr搜索平台，包括spring-data-solr。 15）spring-boot-starter-freemarker支持FreeMarker模板引擎。 16）spring-boot-starter-groovy-templates支持Groovy模板引擎。 17）spring-boot-starter-hateoas通过spring-hateoas支持基于HATEOAS的RESTful Web服务。 18）spring-boot-starter-hornetq通过HornetQ支持JMS。 19）spring-boot-starter-integration支持通用的spring-integration模块。 20）spring-boot-starter-jdbc支持JDBC数据库。 21）spring-boot-starter-jersey支持Jersey RESTful Web服务框架。 22）spring-boot-starter-jta-atomikos通过Atomikos支持JTA分布式事务处理。 23）spring-boot-starter-jta-bitronix通过Bitronix支持JTA分布式事务处理。 24）spring-boot-starter-mail支持javax.mail模块。 25）spring-boot-starter-mobile支持spring-mobile。 26）spring-boot-starter-mustache支持Mustache模板引擎。 27）spring-boot-starter-redis支持Redis键值存储数据库，包括spring-redis。 28）spring-boot-starter-security支持spring-security。 29）spring-boot-starter-social-facebook支持spring-social-facebook 30）spring-boot-starter-social-linkedin支持pring-social-linkedin 31）spring-boot-starter-social-twitter支持pring-social-twitter 32）spring-boot-starter-test支持常规的测试依赖，包括JUnit、Hamcrest、Mockito以及spring-test模块。 33）spring-boot-starter-thymeleaf支持Thymeleaf模板引擎，包括与Spring的集成。 34）spring-boot-starter-velocity支持Velocity模板引擎。 35）spring-boot-starter-webS支持全栈式Web开发，包括Tomcat和spring-webmvc。 36）spring-boot-starter-websocket支持WebSocket开发。 37）spring-boot-starter-ws支持Spring Web Services。 Spring Boot应用启动器面向生产环境的还有2种，具体如下： 1）spring-boot-starter-actuator增加了面向产品上线相关的功能，比如测量和监控。 2）spring-boot-starter-remote-shell增加了远程ssh shell的支持。 最后，Spring Boot应用启动器还有一些替换技术的启动器，具体如下： 1）spring-boot-starter-jetty引入了Jetty HTTP引擎（用于替换Tomcat）。 2）spring-boot-starter-log4j支持Log4J日志框架。 3）spring-boot-starter-logging引入了Spring Boot默认的日志框架Logback。 4）spring-boot-starter-tomcat引入了Spring Boot默认的HTTP引擎Tomcat。 5）spring-boot-starter-undertow引入了Undertow HTTP引擎（用于替换Tomcat）。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; WebMvcConfigurerAdapterspring mvc 配置 redis配置Redis数据库索引（默认为0）spring.redis.database=0Redis服务器地址spring.redis.host=localhostRedis服务器连接端口spring.redis.port=6379Redis服务器连接密码（默认为空）spring.redis.password=连接池最大连接数（使用负值表示没有限制）spring.redis.pool.max-active=8连接池最大阻塞等待时间（使用负值表示没有限制）spring.redis.pool.max-wait=-1连接池中的最大空闲连接spring.redis.pool.max-idle=8连接池中的最小空闲连接spring.redis.pool.min-idle=0连接超时时间（毫秒）spring.redis.timeout=0","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"boot","slug":"boot","permalink":"https://liyong.ac.cn/tags/boot/"}]},{"title":"maven","slug":"maven","date":"2017-07-12T01:44:48.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/07/12/maven/","link":"","permalink":"https://liyong.ac.cn/2017/07/12/maven/","excerpt":"","text":"概述groupId、artifactId、version、packaging Maven - 仓库在 Maven 的术语中，仓库是一个位置（place），例如目录，可以存储所有的工程 jar 文件、library jar 文件、插件或任何其他的工程指定的文件。Maven 仓库有三种类型：本地（local）中央（central）远程（remote） Maven 依赖搜索顺序当我们执行 Maven 构建命令时，Maven 开始按照以下顺序查找依赖的库：步骤 1 － 在本地仓库中搜索，如果找不到，执行步骤 2，如果找到了则执行其他操作。步骤 2 － 在中央仓库中搜索，如果找不到，并且有一个或多个远程仓库已经设置，则执行步骤 4，如果找到了则下载到本地仓库中已被将来引用。步骤 3 － 如果远程仓库没有被设置，Maven 将简单的停滞处理并抛出错误（无法找到依赖的文件）。步骤 4 － 在一个或多个远程仓库中搜索依赖的文件，如果找到则下载到本地仓库已备将来引用，否则 Maven 将停止处理并抛出错误（无法找到依赖的文件）。 Maven - 插件Maven 实际上是一个依赖插件执行的框架，每个任务实际上是由插件完成 外部依赖1234567&lt;dependency&gt; &lt;groupId&gt;ldapjdk&lt;/groupId&gt; &lt;artifactId&gt;ldapjdk&lt;/artifactId&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;systemPath&gt;$&#123;basedir&#125;\\src\\lib\\ldapjdk.jar&lt;/systemPath&gt; &lt;/dependency&gt; 常用Maven插件maven-antrun-pluginhttp://maven.apache.org/plugins/maven-antrun-plugin/maven-antrun-plugin能让用户在Maven项目中运行Ant任务。用户可以直接在该插件的配置以Ant的方式编写Target， 然后交给该插件的run目标去执行。在一些由Ant往Maven迁移的项目中，该插件尤其有用。此外当你发现需要编写一些自定义程度很高的任务，同时又觉 得Maven不够灵活时，也可以以Ant的方式实现之。maven-antrun-plugin的run目标通常与生命周期绑定运行 maven-archetype-pluginhttp://maven.apache.org/archetype/maven-archetype-plugin/Archtype指项目的骨架，Maven初学者最开始执行的Maven命令可能就是mvn archetype:generate，这实际上就是让maven-archetype-plugin生成一个很简单的项目骨架，帮助开发者快速上手。可能也有人看到一些文档写了mvn archetype:create， 但实际上create目标已经被弃用了，取而代之的是generate目标，该目标使用交互式的方式提示用户输入必要的信息以创建项目，体验更好。 maven-archetype-plugin还有一些其他目标帮助用户自己定义项目原型，例如你由一个产品需要交付给很多客户进行二次开发，你就可以为 他们提供一个Archtype，帮助他们快速上手。 maven-assembly-pluginhttp://maven.apache.org/plugins/maven-assembly-plugin/maven-assembly-plugin的用途是制作项目分发包，该分发包可能包含了项目的可执行文件、源代码、readme、平台脚本等等。 maven-assembly-plugin支持各种主流的格式如zip、tar.gz、jar和war等，具体打包哪些文件是高度可控的，例如用户可以 按文件级别的粒度、文件集级别的粒度、模块级别的粒度、以及依赖级别的粒度控制打包，此外，包含和排除配置也是支持的。maven-assembly- plugin要求用户使用一个名为assembly.xml的元数据文件来表述打包，它的single目标可以直接在命令行调用，也可以被绑定至生命周期。 maven-dependency-pluginhttp://maven.apache.org/plugins/maven-dependency-plugin/maven-dependency-plugin最大的用途是帮助分析项目依赖，dependency:list能够列出项目最终解析到的依赖列表，dependency:tree能进一步的描绘项目依赖树，dependency:analyze可以告诉你项目依赖潜在的问题，如果你有直接使用到的却未声明的依赖，该目标就会发出警告。maven-dependency-plugin还有很多目标帮助你操作依赖文件，例如dependency:copy-dependencies能将项目依赖从本地Maven仓库复制到某个特定的文件夹下面。 问题properties","categories":[{"name":"maven","slug":"maven","permalink":"https://liyong.ac.cn/categories/maven/"}],"tags":[{"name":"maven","slug":"maven","permalink":"https://liyong.ac.cn/tags/maven/"}]},{"title":"springcloud","slug":"springcloud","date":"2017-07-09T02:57:53.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/07/09/springcloud/","link":"","permalink":"https://liyong.ac.cn/2017/07/09/springcloud/","excerpt":"Srping Cloud Eureka服务端@EnableEurekaServereureka.client.register-with-eureka=falseeureka.client.fetch-registry=false","text":"Srping Cloud Eureka服务端@EnableEurekaServereureka.client.register-with-eureka=falseeureka.client.fetch-registry=false 12EurekaServerConfigBeancom.netflix.appinfo.InstanceInfo 客户端@EnableDiscoveryClienteureka.client.serviceUrl.defaultZone=http://localhost:1111/eureka/ Hystrix@EnableHystrix Feign@EnableFeignClients Ribbon1234567LoadBalancerAutoConfiguration 实现客户端负载均衡器的自动化配置类ILoadBalancer 客户端负载均衡需要的一系列抽象操作AbstractLoadBalancerRibbonClientConfigurationZoneAwareLoadBalancer@EurekaRibbonClientConfigurationDomainExtractingServerList 源码解读12345678910111213141516@ConditionalOnMissingBean@Order@Import@Inherited@RequiredArgsConstructor@EnableConfigurationProperties@PropertySource(\"classpath:/eureka/server.properties\")@ConfigurationProperties(\"eureka.dashboard\")@ConditionalOnBean@ConfigurationLoadBalancerInterceptor-&gt;RibbonLoadBalancerClientInterceptingClientHttpRequest@PostConstruct@SpringCloudApplication@EnableCircuitBreaker@Order SpringClientFactoryA factory that creates client, load balancer and client configuration instances. It creates a Spring ApplicationContext per client name, and extracts the beans that it needs from there. RibbonLoadBalancerContext extends LoadBalancerContext存储一些被负载均衡使用的内容和API Region Zone Jersery XStream spring-cloud-archaius RabbitProperties gatewayroutes 设计模式构建模式 Builder Zuul12345678910ZuulConfigurationZuulFilterInitializerSendErrorFilterZuulServlet@Autowired private Map&lt;String, ZuulFilter&gt; filters; 123456789ZuulFilter /** * The name of the Archaius property to disable this filter. by default it is zuul.[classname].[filtertype].disable * * @return */ public String disablePropertyName() &#123; return \"zuul.\" + this.getClass().getSimpleName() + \".\" + filterType() + \".disable\"; &#125; 核心过滤器spring-cloud-netflix-core docker-maven-plugin","categories":[{"name":"spring","slug":"spring","permalink":"https://liyong.ac.cn/categories/spring/"}],"tags":[{"name":"cloud","slug":"cloud","permalink":"https://liyong.ac.cn/tags/cloud/"}]},{"title":"乱码总结","slug":"aa_category/se/乱码总结","date":"2017-05-30T10:21:17.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/30/aa_category/se/乱码总结/","link":"","permalink":"https://liyong.ac.cn/2017/05/30/aa_category/se/乱码总结/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.io.UnsupportedEncodingException;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2016-10-25 * @update 修改时间：2016-10-25 */public class EncodeDecode &#123; public static void main(String[] args) throws UnsupportedEncodingException &#123; String defaultStr = new String(\"my english name is lee,my age is 0,我热爱我的工作编程,123\"); method1(defaultStr); method2(defaultStr); method3(defaultStr); &#125; public static void method1(String defaultStr) throws UnsupportedEncodingException &#123; byte[] encodeByUtf8 = defaultStr.getBytes(\"UTF-8\"); System.out.println(); System.out.println(\"===========method1 encodeByUtf8 \" + defaultStr + \"==============\"); cmmonMethod(encodeByUtf8); &#125; public static void method2(String defaultStr) throws UnsupportedEncodingException &#123; byte[] encodeByGbk = defaultStr.getBytes(\"GBK\"); System.out.println(); System.out.println(\"===========method2 encodeByGbk \" + defaultStr + \"==============\"); cmmonMethod(encodeByGbk); &#125; public static void method3(String defaultStr) throws UnsupportedEncodingException &#123; byte[] encodeiso88591 = defaultStr.getBytes(\"ISO8859-1\"); System.out.println(); System.out.println(\"===========method3 encodeiso88591 \" + defaultStr + \"==============\"); cmmonMethod(encodeiso88591); &#125; private static void cmmonMethod(byte[] bytes) throws UnsupportedEncodingException &#123; String utf8 = new String(bytes, \"UTF-8\"); System.out.println(\"utf8 :\" + utf8); String utf16 = new String(bytes, \"UTF-16\"); System.out.println(\"utf16 :\" + utf16); String gbk = new String(bytes, \"GBK\"); System.out.println(\"gbk :\" + gbk); String iso88591 = new String(bytes, \"ISO8859-1\"); System.out.println(\"iso88591 :\" + iso88591); &#125;&#125; 结果：","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"编码","slug":"编码","permalink":"https://liyong.ac.cn/tags/编码/"}]},{"title":"JavaWeb中涉及的编解码","slug":"JavaWeb中涉及的编解码","date":"2017-05-30T08:48:33.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/30/JavaWeb中涉及的编解码/","link":"","permalink":"https://liyong.ac.cn/2017/05/30/JavaWeb中涉及的编解码/","excerpt":"用户从浏览器发起一个HTTP请求，存在编码的地方是URL、Cookie、Paramiter。服务器端接收到HTTP请求后要解析HTTP协议，其中URL、Cookie和POST表单参数要解码，服务器端可能还需要读取硬盘数据（数据库、文件），这些数据都可能存在编码问题。当Servlet处理完所有请求的数据后，需要将这些数据再编码通过Socket发送到用户请求的浏览器里，再经过浏览器解码成为文本。这些过程用图表示如下：","text":"用户从浏览器发起一个HTTP请求，存在编码的地方是URL、Cookie、Paramiter。服务器端接收到HTTP请求后要解析HTTP协议，其中URL、Cookie和POST表单参数要解码，服务器端可能还需要读取硬盘数据（数据库、文件），这些数据都可能存在编码问题。当Servlet处理完所有请求的数据后，需要将这些数据再编码通过Socket发送到用户请求的浏览器里，再经过浏览器解码成为文本。这些过程用图表示如下： URL的编解码为了验证浏览器是怎么编码URL的，我们选择FireFox浏览器并通过HTTPFox插件观察请求的URL的实际内容：从结果上看，PathInfo是UTF-8编码，而QueryString是GBK编码。至于为什么有%，是由URL的编码规范FRC3986规定：浏览器编码URL将非ASCII字符按照某种编码格式编码成16进制数字后将每个16进制表示的字节前加上“%”。从上面的测试结果可知，浏览器对PathInfo和QueryString的编码是不一样的，不同的浏览器对PathInfo的编码也可能不一样。如Chrome会对请求“http://localhost:8080/中国?中国”转变为“http://localhost:8080/%E4%B8%AD%E5%9B%BD?%E4%B8%AD%E5%9B%BD”，这里PathInfo和QueryString的编码是一样的，都是UTF-8编码。 URL解析请求的 URL 是在 org.apache.coyote.HTTP11.InternalInputBuffer 的 parseRequestLine 方法中，这个方法把传过来的 URL 的 byte[] 设置到 org.apache.coyote.Request 的相应的属性中。这里的 URL 仍然是 byte 格式，转成 char 是在 org.apache.catalina.connector.CoyoteAdapter 的 convertURI 方法中完成的 1234567891011121314151617181920212223242526272829303132protected void convertURI(MessageBytes uri, Request request) throws Exception &#123; ByteChunk bc = uri.getByteChunk(); int length = bc.getLength(); CharChunk cc = uri.getCharChunk(); cc.allocate(length, -1); String enc = connector.getURIEncoding(); //获取URI解码集 if (enc != null) &#123; B2CConverter conv = request.getURIConverter(); try &#123; if (conv == null) &#123; conv = new B2CConverter(enc); request.setURIConverter(conv); &#125; &#125; catch (IOException e) &#123;...&#125; if (conv != null) &#123; try &#123; conv.convert(bc, cc, cc.getBuffer().length - cc.getEnd()); uri.setChars(cc.getBuffer(), cc.getStart(), cc.getLength()); return; &#125; catch (IOException e) &#123;...&#125; &#125; &#125; // Default encoding: fast conversion byte[] bbuf = bc.getBuffer(); char[] cbuf = cc.getBuffer(); int start = bc.getStart(); for (int i = 0; i &lt; length; i++) &#123; cbuf[i] = (char) (bbuf[i + start] &amp; 0xff); &#125; uri.setChars(cbuf, 0, length); &#125; 从上面的代码可知，对URI的解码操作是首先获取Connector的解码集，该配置在server.xml中 1&lt;Connector URIEncoding=\"UTF-8\" /&gt; 如果没有定义则会采用默认编码ISO-8859-1来解析. Query String对于Query String部分，我们知道无论我们是通过get方式还是POST方式提交，所有的参数都是保存在Parameters，然后我们通过request.getParameter，解码工作就是在第一次调用getParameter方法时进行的。在getParameter方法内部它调用org.apache.catalina.connector.Request 的 parseParameters 方法，这个方法将会对传递的参数进行解码。下面代码只是parseParameters方法的一部分 123456789101112131415//获取编码 String enc = getCharacterEncoding(); //获取ContentType 中定义的 Charset boolean useBodyEncodingForURI = connector.getUseBodyEncodingForURI(); if (enc != null) &#123; //如果设置编码不为空，则设置编码为enc parameters.setEncoding(enc); if (useBodyEncodingForURI) &#123; //如果设置了Chartset，则设置queryString的解码为ChartSet parameters.setQueryStringEncoding(enc); &#125; &#125; else &#123; //设置默认解码方式 parameters.setEncoding(org.apache.coyote.Constants.DEFAULT_CHARACTER_ENCODING); if (useBodyEncodingForURI) &#123; parameters.setQueryStringEncoding(org.apache.coyote.Constants.DEFAULT_CHARACTER_ENCODING); &#125; &#125; 从上面代码可以看出对query String的解码格式要么采用设置的ChartSet要么采用默认的解码格式ISO-8859-1。注意这个设置的ChartSet是在 http Header中定义的ContentType，同时如果我们需要改指定属性生效，还需要进行如下配置 1&lt;Connector URIEncoding=\"UTF-8\" useBodyEncodingForURI=\"true\"/&gt; POST表单的编解码POST表单参数传递方式与QueryString不同，它是通过HTTP的BODY传递到服务端的。当我们在页面上点击提交按钮时浏览器首先将根据页面的ContentType的Charset编码格式对表单填的参数进行编码，然后提交到服务器端。在服务器端同样也是用ContentType中的字符集进行解码。所以通过POST表单提交的参数一般不会出现问题，而且这个字符集编码可以通过HttpServletRequest.setCharacterEncoding设置，且一定要在第一次调用HttpServletRequest.getParameter之前设置。另外，针对multipart/form-data类型的参数，也就是上传的文件编码，同样也使用ContentType定义的字符集编码。值得注意的地方是，上传文件是用字节流的方式传输到服务器的本地临时目录，这个过程并没有涉及字符编码，而真正编码是在将文件内容添加到parameters中时，如果用这个不能编码将会用默认编码ISO-8859-1来编码。 HTTP BODY的编解码当用户请求的资源服务端已经成功获取后，这些内容将通过Response返回给客户端浏览器，这个过程先要经过编码再到浏览器进行解码，浏览器根据HTML的中的charset来解码。如果没有定义，那么浏览器将会使用默认的编码来解码。 JS中的编解码1、html文件本身中的js的编码和当前页面中的Content-Type保持一致。2、对于采用类型引入的js文件，浏览器就会以当前这个页面的默认字符集解析这个JS文件，如果外部的JS文件的编码格式与当前页面的编码格式一致，那么可以不设置这个charset。但是如果script.js文件的编码格式与当前页面的不一致，就必须要指定对应的字符集，要不然对于非ASCII字符就会出现乱码。 1&lt;script src=\"\" charset=\"\"&gt;&lt;/script&gt; encodeURI()函数它着眼于对整个URL进行utf-8编码，因此除了常见的符号以外，对其他一些在网址中有特殊含义的符号“; / ? : @ & = + $ , #”，也不进行编码。编码后，它输出符号的形式，并且在每个字节前加上%。 encodeURIComponent()函数用于对URL的组成部分进行个别编码，而不用于对整个URL进行编码。因此，“; / ? : @ & = + $ , #”，这些在encodeURI()中不被编码的符号，在encodeURIComponent()中统统会被编码。至于具体的编码方法，两者是一样。","categories":[{"name":"web","slug":"web","permalink":"https://liyong.ac.cn/categories/web/"}],"tags":[{"name":"编码","slug":"编码","permalink":"https://liyong.ac.cn/tags/编码/"}]},{"title":"常见编码格式","slug":"aa_category/se/常见编码格式","date":"2017-05-30T08:00:13.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/30/aa_category/se/常见编码格式/","link":"","permalink":"https://liyong.ac.cn/2017/05/30/aa_category/se/常见编码格式/","excerpt":"为什么编码？1、计算机中最小存储单位为一个字节，即8个bit，所以能表示的范围为0-2552、人类要表示的符号太多，无法用一个字节来完全表示要解决这个矛盾就要新的数据结构char，从char到byte就需要编码","text":"为什么编码？1、计算机中最小存储单位为一个字节，即8个bit，所以能表示的范围为0-2552、人类要表示的符号太多，无法用一个字节来完全表示要解决这个矛盾就要新的数据结构char，从char到byte就需要编码 ASCII编码ASCII码一共规定了128个字符的编码，比如空格”SPACE”是32（二进制00100000），大写的字母A是65（二进制01000001）。这128个符号（包括32个不能打印出来的控制符号），只占用了一个字节的后面7位，最前面的1位统一规定为0。 ISO-8859-1编码单字节编码，向下兼容ASCII，其编码范围是0x00-0xFF，0x00-0x7F之间完全和ASCII一致，0x80-0x9F之间是控制字符，0xA0-0xFF之间是文字符号。此字符集主要支持欧洲使用的语言 GB2312编码GB2312编码双字节编码，是第一个汉字编码国家标准 GBK编码GBK编码，是对GB2312编码的扩展，因此完全兼容GB2312-80标准。GBK编码依然采用双字节编码方案 Unicode编码Unicode背后的想法非常简单，然而却被普遍的误解了。Unicode就像一个电话本，标记着字符和数字之间的映射关系。Joel称之为「神奇数字」，因为它们可能是随机指定的，而且不会给出任何解释。官方术语是码位(Code Point)，总是用U+开头。理论上每种语言中的每种字符都被Unicode协会指定了一个神奇数字。例如希伯来文中的第一个字母א，是U+2135，字母A是U+0061。Unicode并不涉及字符是怎么在字节中表示的，它仅仅指定了字符对应的数字，仅此而已。关于Unicode的其它误解包括：Unicode支持的字符上限是65536个，Unicode字符必须占两个字节。告诉你这些的人应该去换换脑子了。记住，Unicode只是一个用来映射字符和数字的标准。它对支持字符的数量没有限制，也不要求字符必须占两个、三个或者其它任意数量的字节。 Unicode字符是怎样被编码成内存中的字节这是另外的话题，它是被UTF(Unicode Transformation Formats)定义的。 两个最流行的Unicode编码方案是UTF-8和UTF-16 UTF-16编码1、用固定的两个字节表示unicode2、通常说的unicode编码就是指UTF-16编码 UTF-8编码采用了变长技术，每个编码区域有不同的字码长度1、如果一个字节最高位为0，表示这是一个ASCII字符2、如果一个字节以11开头，连续的1的个数暗示这个字符的字节数，例如110xxxxx，代表双字节字符的首字节。3、如果以10开头，代表不是首字节，向前查找才能找到首字节","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"编码","slug":"编码","permalink":"https://liyong.ac.cn/tags/编码/"}]},{"title":"域名解析","slug":"域名解析","date":"2017-05-30T07:29:18.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/30/域名解析/","link":"","permalink":"https://liyong.ac.cn/2017/05/30/域名解析/","excerpt":"","text":"","categories":[{"name":"web","slug":"web","permalink":"https://liyong.ac.cn/categories/web/"}],"tags":[{"name":"域名解析","slug":"域名解析","permalink":"https://liyong.ac.cn/tags/域名解析/"}]},{"title":"垃圾收集算法","slug":"垃圾收集算法","date":"2017-05-30T05:25:13.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/30/垃圾收集算法/","link":"","permalink":"https://liyong.ac.cn/2017/05/30/垃圾收集算法/","excerpt":"垃圾收集器通常会假设大部分的对象的存活时间都非常短，只有少数对象的存活时间比较长。垃圾收集算法在JVM中主要是复制算法（新生代GC）和标记/整理算法（老年代GC）。","text":"垃圾收集器通常会假设大部分的对象的存活时间都非常短，只有少数对象的存活时间比较长。垃圾收集算法在JVM中主要是复制算法（新生代GC）和标记/整理算法（老年代GC）。 标记-清除（Mark-Sweep）算法算法过程： 先判定对象是否可回收，对其标记。 统一回收（简单地删除对垃圾对象的内存引用）。优点：简单直观容易实现和理解。缺点：效率不高，内存空间碎片化。 复制（Copying）算法将内存平均分成A、B两块，算法过程：1.新生对象被分配到A块中未使用的内存当中。当A块的内存用完了， 把A块的存活对象对象复制到B块。2.清理A块所有对象。3.新生对象被分配的B块中未使用的内存当中。当B块的内存用完了， 把B块的存活对象对象复制到A块。4.清理B块所有对象。5.goto 1。优点：简单高效。缺点：内存代价高，有效内存为占用内存的一半。 对复制算法进一步优化：使用Eden/S0/S1三个分区平均分成A/B块太浪费内存，采用Eden/S0/S1三个区更合理，空间比例为Eden:S0:S1==8:1:1，有效内存（即可分配新生对象的内存）是总内存的9/10。算法过程：1.Eden+S0可分配新生对象；2.对Eden+S0进行垃圾收集，存活对象复制到S1。清理Eden+S0。一次新生代GC结束。3.Eden+S1可分配新生对象；4.对Eden+S1进行垃圾收集，存活对象复制到S0。清理Eden+S1。二次新生代GC结束。5.goto 1。 标记-整理（Mark-Compact）算法过程：1.标记：标记可回收对象（垃圾对象）和存活对象。2.整理：将所有存活对象向一端移动，然后直接清理掉边界以外的内存。 分代收集策略由于不同的对象适合使用不同的垃圾收集算法，所以引入“代”这个概念。不同的代有不同的分区，一般分为新生代区和老年代区。新生代：适合采用复制算法进行垃圾收集，对象分布在Eden/S0/S1三个区。老年代：适合采用标记-紧凑算法进行垃圾收集。 Heap分区和分代概念Heap分区的目的 为了分代：不同代的对象放到不同的内存分区中，实现“代提升”，也方便实现对不同分代采用不同的垃圾收集算法。 垃圾收集算法需要：新生代GC使用到复制算法，该算法需要将对应的分区划分成三个分区：Eden/S0/S1。 术语Generation代 YongGeneration/NewGeneration：新生代，在Eden/S0/S1的存活的对象。 OldGeneration：老年代，在Tenured区存活的对象。 PermanentGeneration：永久代。Space 区 Eden：伊甸园区，是新生代的一个区。 Survivor：幸存区，属于新生代，为了复制算法的需要。一般分成大小相等的两个区（S0/S1或者From/To）。 Tenured：存放老年代的区域。 Permanent：终身区。 Eden/S0/S1 新生代S0/S1是大小相当的两个区域，共同组成Survivor区。空间比例：Eden:S0==8:1。设定方法：-XX:SurvivorRatio=8。新生对象在Eden/S0或者Eden/S1中分配，Eden区的对象量达到一个阈值后，发生一次新生代GC。 ##Old 老年代每个对象有“对象年龄计数器”。对象由Eden收集到Survivor区后，年龄+1。进行新生代GC后，年龄+1。依次，当年龄&gt;=15后进入老年代。最大年龄阈值设定：-XX:MaxTenuringThreshold。动态年龄：如果在Survivor中所有相同年龄对象占用了空间的一半多，大于等于上述年龄的对象直接进入老年代。大对象（比如大的数组）直接进入老年代。阈值设定：-XX:PretenureSizeThreshold。 Perm 永久代（PermanentGeneration）用于存放不变对象，如类、方法、字符串等。Java7把驻留字符串（intentd string）放到了老年代区。Java8中移除了Hotspot的永久代区 HotSpot的算法实现枚举根节点从可达性分析中从GC Roots节点找引用为例，可作为GC Roots的节点主要是全局性的引用与执行上下文中，如果要逐个检查引用，必然消耗时间。另外可达性分析对执行时间的敏感还体现在GC停顿上，因为这项分析工作必须在一个能确保一致性的快照中进行——这里的“一致性”的意思是指整个分析期间整个系统执行系统看起来就行被冻结在某个时间点，不可以出现分析过程中对象引用关系还在不断变化的情况，该点不满足的话分析结果的准确性就无法得到保证。这点是导致GC进行时必须暂停所有Java执行线程的其中一个重要原因。由于目前主流的Java虚拟机都是准确式GC，做一档执行系统停顿下来之后，并不需要一个不漏的检查执行上下文和全局的引用位置，虚拟机应当有办法得知哪些地方存放的是对象的引用。在HotSpot的实现中，是使用一组OopMap的数据结构来达到这个目的。 安全点在OopMap的协助下，HotSpot可以快速且准确的完成GC Roots的枚举，但可能导致引用关系变化的指令非常多，如果为每一条指令都生成OopMap，那将会需要大量的额外空间，这样GC的空间成本会变的很高。实际上，HotSpot也的确没有为每条指令生成OopMap，只是在特定的位置记录了这些信息，这些位置被称为安全点（SafePoint）。SafePoint的选定既不能太少，以致让GC等待时间太久，也不能设置的太频繁以至于增大运行时负荷。所以安全点的设置是以让程序“是否具有让程序长时间执行的特征”为标准选定的。“长时间执行”最明显的特征就是指令序列的复用，例如方法调用、循环跳转、异常跳转等，所以具有这些功能的指令才会产生SafePoint。对于SafePoint，另一个问题是如何在GC发生时让所有线程都跑到安全点在停顿下来。这里有两种方案：抢先式中断和主动式中断。抢先式中断不需要线程代码主动配合，当GC发生时，首先把所有线程中断，如果发现线程中断的地方不在安全点上，就恢复线程，让他跑到安全点上。现在几乎没有虚拟机实现采用抢先式中断来暂停线程来响应GC。而主动式中断的思想是当GC需要中断线程的时候，不直接对线程操作，仅仅简单的设置一个标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起，轮询标志的地方和安全点是重合的另外再加上创建对象需要分配的内存的地方。 安全区域使用安全点似乎已经完美解决了如何进入GC的问题，但实际情况却并不一定，安全点机制保证了程序执行时，在不太长的时间内就会进入到可进入的GC的安全点。但是程序如果不执行呢？所谓的程序不执行就是没有分配cpu时间，典型的例子就是线程处于sleep状态或者blocked状态，这时候线程无法响应jvm中断请求，走到安全的地方中断挂起，jvm显然不太可能等待线程重新分配cpu时间，对于这种情况，我们使用安全区域来解决。安全区域是指在一段代码片段之中，你用关系不会发生变化。在这个区域的任何地方开始GC都是安全的，我们可以把安全区域看做是扩展了的安全点。当线程执行到安全区域中的代码时，首先标识自己已经进入了安全区，那样当在这段时间里，JVM要发起GC时，就不用管标识自己为安全区域状态的线程了。当线程要离开安全区域时，他要检查系统是否完成了根节点枚举，如果完成了，那线程就继续执行，否则他就必须等待，直到收到可以安全离开安全区域的信号为止。 http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"memory","slug":"memory","permalink":"https://liyong.ac.cn/tags/memory/"},{"name":"gc","slug":"gc","permalink":"https://liyong.ac.cn/tags/gc/"}]},{"title":"对象已死吗","slug":"对象已死吗","date":"2017-05-30T02:44:01.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/30/对象已死吗/","link":"","permalink":"https://liyong.ac.cn/2017/05/30/对象已死吗/","excerpt":"引用计数算法给对象添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的。主流的java虚拟机里面没有选用这个算法来管理内存，主要原因是它很难解决对象之间相互循环引用的问题。","text":"引用计数算法给对象添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的。主流的java虚拟机里面没有选用这个算法来管理内存，主要原因是它很难解决对象之间相互循环引用的问题。 可达性分析算法这个算法的基本思路就是通过一系列的称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。在Java语言中，可作为GC Roots的对象包括下面几种：1）虚拟机栈（帧栈中的本地变量表）中引用的对象。2）方法区中类静态属性引用的对象。3）方法区中常量引用的对象。4）本地方法栈中JNI（即一般说的Native对象）引用的对象。 再谈引用在JDK1.2以前，Java中的引用的定义很传统：如果Reference类型的数据中存储的数值代表的是另外一块内存的起始地址，就称这块内存代表着一个引用。在JDK1.2之后，Java对引用的概念进行了扩充，将引用分为：1）强引用（Strong Reference）2）软引用（Soft Reference）3）弱引用（Weak Reference）4）虚引用（Phantom Reference）这四种引用强度依次逐渐减弱。1）强引用就是指在程序代码之中普遍存在的，类似“Object obj = new Object()”这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。2）软引用是用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK1.2之后，提供了SoftReference类来实现软引用。3）弱引用也是用来描述非必须对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK1.2之后，提供了WeakReference类来实现弱引用。4）虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在JDK1.2之后，提供了PhantomReference类来实现虚引用。 生存还是死亡即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，这时候他们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”。如果这个对象被判定为有必要执行finalize()方法，那么这个对象将会放置在一个叫做F-Queue的队列之中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。这里所谓的“执行”是指虚拟机会触发这个方法，但并不承诺会等待它运行结束，这样做的原因是，如果一个对象在finalize()方法中执行缓慢，或者发生了死循环（更极端情况），将很可能会导致F-Queue队列中其它对象永久处于等待，甚至导致整个内存回收系统崩溃。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己–只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this关键字）赋值给某个类变量或者对象的成员变量，那么在第二次标记时它将被移除出“即将回收”的集合，如果对象这时候还没有逃脱，那基本上它就真的被回收了。从下面代码中我们可以看到一个对象的finalize()被执行，但是它仍然可以存活，一次对象自我拯救的演示 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 此代码演示了两点： 1.对象可以在被GC时自我拯救。 2.这种自救的机会只有一次，因为一个对象的finalize()方法最多只会被系统自动调用一次 * * @author zzm */public class FinalizeEscapeGC &#123; public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive() &#123; System.out.println(\"yes, i am still alive :)\"); &#125; @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println(\"finalize mehtod executed!\"); FinalizeEscapeGC.SAVE_HOOK = this; &#125; public static void main(String[] args) throws Throwable &#123; SAVE_HOOK = new FinalizeEscapeGC(); // 对象第一次成功拯救自己 SAVE_HOOK = null; System.gc(); // 因为Finalizer方法优先级很低，暂停0.5秒，以等待它 Thread.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125; else &#123; System.out.println(\"no, i am dead :(\"); &#125; // 下面这段代码与上面的完全相同，但是这次自救却失败了 SAVE_HOOK = null; System.gc(); // 因为Finalizer方法优先级很低，暂停0.5秒，以等待它 Thread.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125; else &#123; System.out.println(\"no, i am dead :(\"); &#125; &#125;&#125; 运行结果： 123finalize mehtod executed!yes, i am still alive :)no, i am dead :( 执行结果一次成功，一次失败，这是因为任何一个对象的finalize()方法都只会被系统自动调用一次，如果对象面临下一次回收，它的finalize()方法不会被再次执行，因此第二段代码的自救行动失败了。特别说明：不建议使用finalize方法，因为它的运行代价高，不确定性大，无法保证各个对象的调用顺序。finalize()方法能做的所有工作，使用try-finally或者其它方式都可以做得更好、更及时。 回收方法区在方法区中进行垃圾收集的“性价比”一般比较低：在堆中，尤其是在新生代中，常规应用进行一次垃圾收集一般可以回收70%~95%的空间，而永久代的垃圾收集效率远低于此。永久代的垃圾收集主要回收两部分内容： 1）废弃的常量 2）无用的类回收废弃的常量与回收Java堆中的对象非常类似。以常量池中字面量的回收为例，假如一个字串“abc”已经进入了常量池中，但是当前系统没有任何一个String对象叫做“abc”的，换句话说,就是没有任何String对象引用常量池中的“abc”常量，也没有其它地方引用了这个字面量，如果这时发生内存回收，而且必要的话，这个“abc”常量就会被系统清理出常量池。常量池中的其它类（接口）、方法、字段的符号引用也与此类似。类需要同时满足下面3个条件才能算是“无用的类”：1）该类所有的实例都已经被回收了，也就是Java堆中不存在该类的任何实例。2）加载该类的ClassLoader已经被回收。3）该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。虚拟机可以对满足上述3各条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样，不使用了就必然会回收。是否对类进行回收，HotSpot虚拟机提供了: 1）-Xnoclassgc参数控制是否对类进行回收 2）-verbose:class以及-XX:+TraceClassLoading、-XX:+TraceClassUnLoading查看类加载和卸载信息。其中-verbose:class和-XX:+TraceClassLoading可以在Product版的虚拟机中使用，-XX:+TraceClassUnLoading参数需要FastDebug版的虚拟机支持。","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"memory","slug":"memory","permalink":"https://liyong.ac.cn/tags/memory/"},{"name":"gc","slug":"gc","permalink":"https://liyong.ac.cn/tags/gc/"}]},{"title":"HotSpot虚拟机对象探秘","slug":"HotSpot虚拟机对象探秘","date":"2017-05-29T08:01:26.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/29/HotSpot虚拟机对象探秘/","link":"","permalink":"https://liyong.ac.cn/2017/05/29/HotSpot虚拟机对象探秘/","excerpt":"对象的创建虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过的。如果没有，那必须先执行相应的类加载过程","text":"对象的创建虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过的。如果没有，那必须先执行相应的类加载过程 在类加载通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务具体便等同于一块确定大小的内存从Java堆中划分出来 内存分配完成之后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），如果使用TLAB的话，这一个工作也可以提前至TLAB分配时进行。除如何划分可用空间之外，还有另外一个需要考虑的问题是对象创建在虚拟机中是非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存。解决这个问题有两个方案，一种是对分配内存空间的动作进行同步——实际上虚拟机是采用CAS配上失败重试的方式保证更新操作的原子性；另外一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲，（TLAB ，Thread Local Allocation Buffer），哪个线程要分配内存，就在哪个线程的TLAB上分配，只有TLAB用完，分配新的TLAB时才需要同步锁定。虚拟机是否使用TLAB，可以通过-XX:+/-UseTLAB参数来设定。 接下来，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头（Object Header）之中 在上面工作都完成之后，在虚拟机的视角来看，一个新的对象已经产生了。但是在Java程序的视角看来，对象创建才刚刚开始——方法还没有执行，所有的字段都为零呢。所以一般来说（由字节码中是否跟随有invokespecial指令所决定），new指令之后会接着就是执行方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 对象的内存布局HotSpot虚拟机中，对象在内存中存储的布局可以分为三块区域：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。 对象头HotSpot虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等等，这部分数据的长度在32位和64位的虚拟机（暂不考虑开启压缩指针的场景）中分别为32个和64个Bits，官方称它为“Mark Word”。考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存储尽量多的信息，它会根据对象的状态复用自己的存储空间。例如在32位的HotSpot虚拟机中对象未被锁定的状态下，Mark Word的32个Bits空间中的25Bits用于存储对象哈希码（HashCode），4Bits用于存储对象分代年龄，2Bits用于存储锁标志位，1Bit固定为0，在其他状态（轻量级锁定、重量级锁定、GC标记、可偏向）下对象的存储内容如下表所示。 对象头的另外一部分是类型指针，即是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说查找对象的元数据信息并不一定要经过对象本身。另外，如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是从数组的元数据中无法确定数组的大小。 实例数据实例数据部分是对象真正存储的有效信息，也既是我们在程序代码里面所定义的各种类型的字段内容，无论是从父类继承下来的，还是在子类中定义的都需要记录袭来。这部分的存储顺序会受到虚拟机分配策略参数（FieldsAllocationStyle）和字段在Java源码中定义顺序的影响。HotSpot虚拟机默认的分配策略为longs/doubles、ints、shorts/chars、bytes/booleans、oops（Ordinary Object Pointers），从分配策略中可以看出，相同宽度的字段总是被分配到一起。在满足这个前提条件的情况下，在父类中定义的变量会出现在子类之前。如果CompactFields参数值为true（默认为true），那子类之中较窄的变量也可能会插入到父类变量的空隙之中。 对齐填充第三部分对齐填充并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于HotSpot VM的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说就是对象的大小必须是8字节的整数倍。对象头部分正好似8字节的倍数（1倍或者2倍），因此当对象实例数据部分没有对齐的话，就需要通过对齐填充来补全。 对象的访问定位建立对象是为了使用对象，我们的Java程序需要通过栈上的reference数据来操作堆上的具体对象。由于reference类型在Java虚拟机规范里面只规定了是一个指向对象的引用，并没有定义这个引用应该通过什么种方式去定位、访问到堆中的对象的具体位置，对象访问方式也是取决于虚拟机实现而定的。主流的访问方式有使用句柄和直接指针两种。 如果使用句柄访问的话，Java堆中将会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据的具体各自的地址信息。如果使用直接指针访问的话，Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，reference中存储的直接就是对象地址。 使用句柄来访问的最大好处就是reference中存储的是稳定句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要被修改。 使用直接指针来访问最大的好处就是速度更快，它节省了一次指针定位的时间开销，由于对象访问的在Java中非常频繁，因此这类开销积小成多也是一项非常可观的执行成本。就虚拟机HotSpot而言，它是使用第二种方式进行对象访问","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"memory","slug":"memory","permalink":"https://liyong.ac.cn/tags/memory/"}]},{"title":"运行时数据区域","slug":"运行时数据区域","date":"2017-05-29T03:19:49.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/29/运行时数据区域/","link":"","permalink":"https://liyong.ac.cn/2017/05/29/运行时数据区域/","excerpt":"","text":"程序计数器（Program Counter Register） 程序计数器是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined） Java虚拟机栈（Java Virtual Machine Stacks）Java虚拟机栈也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型），它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈动态扩展时，无法申请到足够的内存时会抛出OutOfMemoryError异常。 本地方法栈本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法服务，而本地方法栈则是为虚拟机使用到的Native方法服务 Java堆（Java Heap）Java堆是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。这一点在Java虚拟机规范中的描述是：所有的对象实例以及数组都要在堆上分配，但是随着JIT编译器的发展与逃逸分析技术的逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化发生，所有的对象都分配在堆上也渐渐变得不是那么“绝对”了。Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”（Garbage Collected Heap，幸好国内没翻译成“垃圾堆”）。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。如果从内存分配的角度看，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer，TLAB）。不过，无论如何划分，都与存放内容无关，无论哪个区域，存储的都仍然是对象实例，进一步划分的目的是为了更好地回收内存，或者更快地分配内存。根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 方法区（Method Area）是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。对于其他虚拟机（如BEA JRockit、IBM J9等）来说是不存在永久代的概念的。即使是HotSpot虚拟机本身，根据官方发布的路线图信息，现在也有放弃永久代并“搬家”至Native Memory来实现方法区的规划了。Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的根据Java虚拟机规范的规定，当方法区无法满足内存分配 需求时，将抛出OutOfMemoryError异常。 运行时常量池（Runtime Constant Pool）运行时常量池是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述等信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只能在编译期产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 既然运行时常量池是方法区的一部分，自然会受到方法区内存的限制，当常量池无法再申请到内存时会抛出OutOfMemoryError异常。","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"memory","slug":"memory","permalink":"https://liyong.ac.cn/tags/memory/"},{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/tags/se/"}]},{"title":"JVM中锁优化，偏向锁、自旋锁、锁消除、锁粗化","slug":"JVM中锁优化","date":"2017-05-29T01:18:45.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/29/JVM中锁优化/","link":"","permalink":"https://liyong.ac.cn/2017/05/29/JVM中锁优化/","excerpt":"锁消除锁削除是指虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行削除（主要判定依据来源于逃逸分析的数据支持，如果判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须进行）","text":"锁消除锁削除是指虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行削除（主要判定依据来源于逃逸分析的数据支持，如果判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须进行） 锁粗化(膨胀)如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。 如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁，将会把加锁同步的范围扩展（膨胀）到整个操作序列的外部（由多次加锁编程只加锁一次）。 偏向锁目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能。如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS操作都不做了。 偏向锁会偏向于第一个获得它的线程（Mark Word中的偏向线程ID信息），如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步。 假设当前虚拟机启用了偏向锁（启用参数-XX:+UseBiasedLocking，JDK 1.6的默认值），当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设为“01”，即偏向模式。同时使用CAS操作把获取到这个锁的线程的ID记录在对象的Mark Word之中，如果CAS操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作（例如Locking、Unlocking及对Mark Word的Update等）。当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束。 轻量级锁轻量级锁并不是用来代替重量级锁（传统锁机制，如互斥等）的，目的是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。HotSpot虚拟机的对象头（Object Header）分为两部分信息，第一部分用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄（Generational GC Age）等，这部分数据的长度在32位和64位的虚拟机中分别为32个和64个Bits，官方称它为“Mark Word”，它是实现轻量级锁和偏向锁的关键。另外一部分用于存储指向方法区对象类型数据的指针，如果是数组对象的话，还会有一个额外的部分用于存储数组长度。Mark Word被设计成一个非固定的数据结构以便在极小的空间内存储尽量多的信息，它会根据对象的状态复用自己的存储空间。例如在32位的HotSpot虚拟机中对象未被锁定的状态下，Mark Word的32个Bits空间中的25Bits用于存储对象哈希码（HashCode），4Bits用于存储对象分代年龄，2Bits用于存储锁标志位，1Bit固定为0，在其他状态（轻量级锁定、重量级锁定、GC标记、可偏向）下Mark Word的存储内容 加锁过程在代码进入同步块的时候，如果此同步对象没有被锁定（锁标志位为“01”状态），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝（官方把这份拷贝加了一个Displaced前缀，即Displaced Mark Word）,然后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针。如果这个更新动作成功，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位（Mark Word的最后两个Bits）将转变为“00”，即表示此对象处于轻量级锁定状态,如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程抢占了。如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁（互斥量）的指针，后面等待锁的线程也要进入阻塞状态。 解锁过程解锁过程也是通过CAS操作来进行的，如果对象的Mark Word仍然指向着线程的锁记录，那就用CAS操作把对象当前的Mark Word和线程中复制的Displaced Mark Word替换回来，如果替换成功，整个同步过程就完成了。如果替换失败，说明有其他线程尝试过获取该锁，那就要在释放锁的同时，唤醒被挂起的线程。 小结轻量级锁能提升程序同步性能的依据是“对于绝大部分的锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据。如果没有竞争，轻量级锁使用CAS操作避免了使用互斥量的开销，但如果存在锁竞争，除了互斥量的开销外，还额外发生了CAS操作，因此在有竞争的情况下，轻量级锁会比传统的重量级锁更慢。 自旋锁互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给系统的并发性能带来了很大的压力。而在很多应用上，共享数据的锁定状态只会持续很短的一段时间。若实体机上有多个处理器，能让两个以上的线程同时并行执行，我们就可以让后面请求锁的那个线程原地自旋（不放弃CPU时间），看看持有锁的线程是否很快就会释放锁。为了让线程等待，我们只须让线程执行一个忙循环（自旋），这项技术就是自旋锁。如果锁长时间被占用，则浪费处理器资源，因此自旋等待的时间必须要有一定的限度，如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式去挂起线程了（默认10次）。JDK1.6引入自适应的自旋锁：自旋时间不再固定，由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"Future模式","slug":"Future模式","date":"2017-05-14T10:14:48.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/14/Future模式/","link":"","permalink":"https://liyong.ac.cn/2017/05/14/Future模式/","excerpt":"1234567package concurrent.future;public interface Data&lt;T&gt; &#123; public T getResult();&#125;","text":"1234567package concurrent.future;public interface Data&lt;T&gt; &#123; public T getResult();&#125; 1234567891011121314151617181920212223242526package concurrent.future;public class RealData&lt;T&gt; implements Data&lt;T&gt; &#123; private T result; @Override public T getResult() &#123; return result; &#125; public RealData(T pram) &#123; super(); StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt;= 10; i++) &#123; sb.append(pram + \" \" + i + \" \"); try &#123; Thread.sleep(1 * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; result = (T) sb.toString(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233package concurrent.future;public class FutureData&lt;T&gt; implements Data&lt;T&gt; &#123; boolean isReady = false; private RealData&lt;T&gt; realData = null; @Override public synchronized T getResult() &#123; System.out.println(\"进入 FutureData getResult\"); while (!isReady) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; System.out.println(\"FutureData getResult 已经有数据\"); return realData.getResult(); &#125; public synchronized void setRealData(RealData&lt;T&gt; realData) &#123; if (isReady) &#123; return; &#125; this.realData = realData; this.isReady = true; notifyAll(); &#125;&#125; 123456789101112131415161718192021222324252627package concurrent.future;import org.apache.poi.ss.formula.functions.T;@SuppressWarnings(\"hiding\")public class Client&lt;T&gt; &#123; public Data&lt;T&gt; request(final T queryStr) &#123; final FutureData&lt;T&gt; futureData = new FutureData&lt;T&gt;(); new Thread() &#123; public void run() &#123; RealData&lt;T&gt; realData = new RealData&lt;T&gt;(queryStr); futureData.setRealData(realData); &#125; &#125;.start(); return futureData; &#125; public static void main(String[] args) &#123; Client&lt;String&gt; client = new Client&lt;String&gt;(); Data&lt;String&gt; data = client.request(\" lee is good\"); System.out.println(System.currentTimeMillis() + \" 请求完毕 \"); String result = data.getResult(); System.out.println(System.currentTimeMillis() + \" 结果 \" + result); &#125;&#125; JDK中的Future模式 12345678910111213141516171819202122232425262728293031323334353637383940414243package concurrent.future;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.FutureTask;public class RealData2 implements Callable&lt;String&gt; &#123; String pram; public RealData2(String pram) &#123; super(); this.pram = pram; &#125; @Override public String call() throws Exception &#123; // TODO Auto-generated method stub StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt;= 10; i++) &#123; sb.append(pram + \" \" + i + \" \"); try &#123; Thread.sleep(1 * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; return sb.toString(); &#125; public static void main(String[] args) throws InterruptedException, ExecutionException &#123; RealData2 data = new RealData2(\" lee is good\"); FutureTask&lt;String&gt; task = new FutureTask&lt;String&gt;(data); ExecutorService service = Executors.newSingleThreadExecutor(); service.submit(task); System.out.println(System.currentTimeMillis() + \" 请求完毕 \"); String result = task.get(); System.out.println(System.currentTimeMillis() + \" 结果 \" + result); service.shutdown(); &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"Se小众语法","slug":"Se小众语法","date":"2017-05-14T08:26:18.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/14/Se小众语法/","link":"","permalink":"https://liyong.ac.cn/2017/05/14/Se小众语法/","excerpt":"介绍不常用，可能用得着的语法","text":"介绍不常用，可能用得着的语法 在多层嵌套循环中continue到任意层1234567891011121314151617181920212223242526public static &lt;V&gt; void main(String[] args) &#123; lee: for (int i = 0; i &lt; 2; i++) &#123; for (int j = 0; j &lt; 2; j++) &#123; System.out.println(\"i = \" + i + \" j= \" + j); if (j == 0) &#123; System.out.println(\"continue lee\"); continue lee; &#125; System.out.println(\"再来一次 i = \" + i + \" j= \" + j); &#125; &#125; System.out.println(\"============\"); for (int i = 0; i &lt; 2; i++) &#123; for (int j = 0; j &lt; 2; j++) &#123; System.out.println(\"i = \" + i + \" j= \" + j); if (j == 0) &#123; System.out.println(\"continue \"); continue; &#125; System.out.println(\"再来一次 i = \" + i + \" j= \" + j); &#125; &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[]},{"title":"ConcurrentLinkedQueue","slug":"ConcurrentLinkedQueue","date":"2017-05-14T07:15:19.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/14/ConcurrentLinkedQueue/","link":"","permalink":"https://liyong.ac.cn/2017/05/14/ConcurrentLinkedQueue/","excerpt":"ConcurrentLinkedQueue由head节点和tail节点组成，每个节点（Node）由节点元素（item）和指向下一个节点的引用(next) 组成，节点与节点之间就是通过这个next关联起来，从而组成一张链表结构的队列。默认情况下head节点存储的元素为空，tail节点等于head节点","text":"ConcurrentLinkedQueue由head节点和tail节点组成，每个节点（Node）由节点元素（item）和指向下一个节点的引用(next) 组成，节点与节点之间就是通过这个next关联起来，从而组成一张链表结构的队列。默认情况下head节点存储的元素为空，tail节点等于head节点 入队列123456789101112131415161718192021222324252627282930313233343536/** * Inserts the specified element at the tail of this queue. * As the queue is unbounded, this method will never return &#123;@code false&#125;. * * @return &#123;@code true&#125; (as specified by &#123;@link Queue#offer&#125;) * @throws NullPointerException if the specified element is null */public boolean offer(E e) &#123; checkNotNull(e); final Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); for (Node&lt;E&gt; t = tail, p = t;;) &#123; Node&lt;E&gt; q = p.next; if (q == null) &#123; // p is last node if (p.casNext(null, newNode)) &#123; // Successful CAS is the linearization point // for e to become an element of this queue, // and for newNode to become \"live\". if (p != t) // hop two nodes at a time casTail(t, newNode); // Failure is OK. return true; &#125; // Lost CAS race to another thread; re-read next &#125; else if (p == q) // We have fallen off list. If tail is unchanged, it // will also be off-list, in which case we need to // jump to head, from which all live nodes are always // reachable. Else the new tail is a better bet. p = (t != (t = tail)) ? t : head; else // Check for tail updates after two hops. p = (p != t &amp;&amp; t != (t = tail)) ? t : q; &#125;&#125; 1p = (t != (t = tail)) ? t : head; 1、!=并不是原子操作2、先取得t的值，再执行t = tail并取得新的t的值，最后比较这两个值是否相同3、t!=t在单线程下并不会成立，但是在多线程下可能成立。 出队列123456789101112131415161718192021222324 public E poll() &#123; restartFromHead: for (;;) &#123; for (Node&lt;E&gt; h = head, p = h, q;;) &#123; E item = p.item; if (item != null &amp;&amp; p.casItem(item, null)) &#123; // Successful CAS is the linearization point // for item to be removed from this queue. if (p != h) // hop two nodes at a time updateHead(h, ((q = p.next) != null) ? q : p); return item; &#125; else if ((q = p.next) == null) &#123; updateHead(h, p); return null; &#125; else if (p == q) continue restartFromHead; else p = q; &#125; &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"线程安全的Map","slug":"aa_category/se/thread/线程安全的Map","date":"2017-05-14T04:59:08.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/14/aa_category/se/thread/线程安全的Map/","link":"","permalink":"https://liyong.ac.cn/2017/05/14/aa_category/se/thread/线程安全的Map/","excerpt":"","text":"SynchronizedMap 通过给传入的map加synchronized实现 1Collections.synchronizedMap(new HashMap&lt;String, Object&gt;()) ConcurrentHashMap","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"单例模式","slug":"aa_category/methodology/design-pattern/单例模式","date":"2017-05-14T04:35:50.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/14/aa_category/methodology/design-pattern/单例模式/","link":"","permalink":"https://liyong.ac.cn/2017/05/14/aa_category/methodology/design-pattern/单例模式/","excerpt":"12345678910111213141516171819202122232425package designpatterns.singleton;/** * * @author liyong * */public class StaticSingleton &#123; public static int staus = 1; public StaticSingleton() &#123; super(); System.out.println(\"StaticSingleton is create\"); &#125; private static class SingletonHolder &#123; private static StaticSingleton instance = new StaticSingleton(); &#125; public StaticSingleton getInstance() &#123; return SingletonHolder.instance; &#125;&#125;","text":"12345678910111213141516171819202122232425package designpatterns.singleton;/** * * @author liyong * */public class StaticSingleton &#123; public static int staus = 1; public StaticSingleton() &#123; super(); System.out.println(\"StaticSingleton is create\"); &#125; private static class SingletonHolder &#123; private static StaticSingleton instance = new StaticSingleton(); &#125; public StaticSingleton getInstance() &#123; return SingletonHolder.instance; &#125;&#125; 以上代码的优点1、getInstance() 方法没有锁，使得在高并发场景下性能卓越2、getInstance()方法只有在第一次调用的时候实例才会被创建","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://liyong.ac.cn/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://liyong.ac.cn/tags/设计模式/"}]},{"title":"线程池","slug":"线程池","date":"2017-05-07T08:39:54.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/07/线程池/","link":"","permalink":"https://liyong.ac.cn/2017/05/07/线程池/","excerpt":"为什么使用线程池？1、虽然线程是一个轻量级工具，但是创建和销毁线程依然需要花费时间，如果为小任务创建线程，很有可能创建线程的时间都大于执行任务的时间2、线程本身也占有内存空间，大量的线程会抢占宝贵的内存资源。","text":"为什么使用线程池？1、虽然线程是一个轻量级工具，但是创建和销毁线程依然需要花费时间，如果为小任务创建线程，很有可能创建线程的时间都大于执行任务的时间2、线程本身也占有内存空间，大量的线程会抢占宝贵的内存资源。 线程池工厂方法12345Executors.newSingleThreadExecutor();Executors.newFixedThreadPool(5);Executors.newCachedThreadPool();Executors.newSingleThreadScheduledExecutor();Executors.newScheduledThreadPool(5); 固定大小的线程池123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package concurrent;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.ThreadFactory;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-5-7 * @update 修改时间：2017-5-7 */public class ThreadPoolDemo1 &#123; public static class MyTask implements Runnable &#123; public void run() &#123; System.out.println( \"运行 Thread id :\" + Thread.currentThread().getId()+\" \"+System.currentTimeMillis()); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; ExecutorService es = Executors.newFixedThreadPool(5, new ThreadFactory() &#123; public Thread newThread(Runnable r) &#123; Thread thread = new Thread(r); System.out.println(\"创建 Thread id :\" + thread.getId()); return thread; &#125; &#125;); MyTask task = new MyTask(); for (int i = 0; i &lt; 10; i++) &#123; es.execute(task); &#125; es.shutdown(); &#125;&#125; 计划任务会在指定的时间，对任务进行调度，类似linux下面的crontab schedule给定的时间会对任务进行一次调度 scheduleAtFixedRate1、任务是按照一定频率调度2、以上一个任务开始执行时间为起点，之后的period时间调度下一个任务3、如果调度周期小于执行时间，不会多个任务堆叠在一起，上一个任务结束后，立即调用 scheduleWithFixedDelay上一个任务结束后再经过period时间 12345678910111213141516171819202122232425262728293031323334package concurrent;import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-5-7 * @update 修改时间：2017-5-7 */public class ScheduledExecutorServiceDemo &#123; public static void main(String[] args) &#123; ScheduledExecutorService ses = Executors.newScheduledThreadPool(5); ses.scheduleAtFixedRate(new Runnable() &#123; public void run() &#123; try &#123; Thread.sleep(3000); System.out.println(Thread.currentThread().getId()+\" \"+System.currentTimeMillis() / 1000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;, 1L, 2, TimeUnit.SECONDS); &#125;&#125; 核心实现1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) threadFactory:线程工厂 workQueue:任务队列，被提交尚未执行的任务1、直接提交队列1)该功能由SynchronousQueue提供2)SynchronousQueue没有容量，每一个插入操作都要等待一个删除操作，反之亦然。3)如果使用SynchronousQueue提交的任务不会被真实的保存，而总是将任务提交给线程执行，如果没有线程则创建一个线程，如果超过最大线程数，则执行拒绝策略2、有界的任务队列1)该功能由ArrayBlockingQueue提供2)如果线程数小于等于corePoolSize，则直接创建线程3)如果大于corePoolSize小于maximumPoolSize则把任务放入任务队列4)如果任务队列已满且线程数小于maximumPoolSize则创建新的线程完成任务5)若线程数大于maximumPoolSize则执行拒绝策略3、无界的任务队列1)该功能有LinkedBlockingQueue提供2)如果线程数小于等于corePoolSize，则直接创建线程3)如果线程数大于corePoolSize，有新的任务则直接加入任务队列，直至资源耗尽4、优先任务队列1)该功能由PriorityBlockingQueue提供2)控制任务的先后执行顺序，是一个特殊的无界队列 拒绝策略1、AbortPolicy:直接抛出异常，阻止系统正常工作2、DiscardPolicy：什么也不做3、DiscardOldestPolicy：丢弃最老的请求，并重新尝试提交任务4、CallerRunsPolicy：只要线程池未关闭，该策略直接在调用者线程中，运行当前被丢弃的任务。 123456789101112131415161718public static class CallerRunsPolicy implements RejectedExecutionHandler &#123; /** * Creates a &lt;tt&gt;CallerRunsPolicy&lt;/tt&gt;. */ public CallerRunsPolicy() &#123; &#125; /** * Executes task r in the caller's thread, unless the executor * has been shut down, in which case the task is discarded. * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125; &#125; &#125; 自定义ThreadFactory 请看固定大小的线程池的Demo 扩展线程池12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package concurrent;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-5-7 * @update 修改时间：2017-5-7 */public class ExtThreadPoolDemo1 &#123; public static class MyTask implements Runnable &#123; public String name; private MyTask(String name) &#123; super(); this.name = name; &#125; public void run() &#123; System.out.println(\"运行 Thread id :\" + Thread.currentThread().getId() + \" \" + System.currentTimeMillis()); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 5, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()) &#123; protected void beforeExecute(Thread t, Runnable r) &#123; System.out.println(\"准备执行 \" + ((MyTask) r).name); &#125; protected void afterExecute(Runnable r, Throwable t) &#123; System.out.println(\"执行完成\" + ((MyTask) r).name); &#125; protected void terminated() &#123; System.out.println(\"线程池结束\"); &#125; &#125;; for (int i = 0; i &lt; 10; i++) &#123; MyTask task = new MyTask(\"task \"+i); executor.execute(task); &#125; executor.shutdown(); &#125;&#125; 线程池数量Ncpu = CPU数量Ucpu=目标 CPU 的使用率，0&lt;=Ucpu&lt;=1W/C=等待时间与计算时间的比率最优的池的大小等于： 1Nthreads=Ncpu*Ucpu*(1+W/C) 获取CPU数量 1Runtime.getRuntime().availableProcessors(); 在线程池中寻找堆栈信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package concurrent;import java.util.concurrent.BlockingQueue;import java.util.concurrent.Future;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;/** * @description 类描述：让在调度任务之前，先保存一下提交任务线程的堆栈信息 * @author 作者：LIYONG * @create 创建时间：2017-5-13 * @update 修改时间：2017-5-13 */public class TraceThreadPoolExecutor extends ThreadPoolExecutor &#123; public TraceThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue); // TODO Auto-generated constructor stub &#125; /** * &#123;@inheritDoc&#125; * * @see java.util.concurrent.ThreadPoolExecutor#execute(java.lang.Runnable) */ public void execute(Runnable command) &#123; super.execute(wrap(command, clentTrace(), Thread.currentThread().getName())); &#125; public Future&lt;?&gt; submit(Runnable task) &#123; return super.submit(wrap(task, clentTrace(), Thread.currentThread().getName())); &#125; private Exception clentTrace() &#123; return new Exception(\"Client stack trace\"); &#125; private Runnable wrap(final Runnable task, final Exception clientStack, String clientThreadName) &#123; return new Runnable() &#123; public void run() &#123; try &#123; task.run(); &#125; catch (Exception e) &#123; clientStack.printStackTrace(); e.printStackTrace(); &#125; &#125; &#125;; &#125;&#125;package concurrent;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorService;import java.util.concurrent.SynchronousQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-5-13 * @update 修改时间：2017-5-13 */public class DivTask implements Runnable &#123; int a, b; private DivTask(int a, int b) &#123; super(); this.a = a; this.b = b; &#125; public void run() &#123; double re = a / b; System.out.println(re); &#125; public static void main(String[] args) throws InterruptedException, ExecutionException &#123; ExecutorService pool = new TraceThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); for (int i = 0; i &lt; 5; i++) &#123; DivTask task = new DivTask(100, i); // pool.execute(task); pool.submit(task); // future.get(); &#125; System.out.println(\"******************************************\"); ExecutorService threadPoolExecutor = new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); for (int i = 0; i &lt; 5; i++) &#123; DivTask task = new DivTask(100, i); // pool.execute(task); threadPoolExecutor.submit(task); // future.get(); &#125; &#125;&#125; 运行结果 1234567891011121314151617181920212223java.lang.Exception: Client stack trace at concurrent.TraceThreadPoolExecutor.clentTrace(TraceThreadPoolExecutor.java:42) at concurrent.TraceThreadPoolExecutor.submit(TraceThreadPoolExecutor.java:37) at concurrent.DivTask.main(DivTask.java:39)java.lang.ArithmeticException: / by zero at concurrent.DivTask.run(DivTask.java:29) at concurrent.TraceThreadPoolExecutor$1.run(TraceThreadPoolExecutor.java:52)100.0 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at concurrent.TraceThreadPoolExecutor$1.run(TraceThreadPoolExecutor.java:52) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:662)25.033.050.0******************************************100.033.025.050.0 分而治之 Fork/Join 框架1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package concurrent;import java.util.ArrayList;import java.util.concurrent.ExecutionException;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;import java.util.concurrent.RecursiveTask;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-5-13 * @update 修改时间：2017-5-13 */public class CountTask extends RecursiveTask&lt;Long&gt; &#123; private static final int threshold = 1000; private long start; private long end; public CountTask(long start, long end) &#123; super(); this.start = start; this.end = end; &#125; /** * */ private static final long serialVersionUID = 1L; @Override protected Long compute() &#123; long sum = 0; boolean canCpmpute = (end - start) &lt; threshold; if (canCpmpute) &#123; for (long i = start; i &lt;= end; i++) &#123; sum += i; &#125; &#125; else &#123; long step = (start + end) / 100; ArrayList&lt;CountTask&gt; subTasks = new ArrayList&lt;CountTask&gt;(); long pos = start; for (int i = 0; i &lt; 100; i++) &#123; long lastOne = pos + step; if (lastOne &gt; end) lastOne = end; CountTask subTask = new CountTask(pos, lastOne); pos += step + 1; subTasks.add(subTask); subTask.fork(); &#125; for (CountTask subTask : subTasks) &#123; sum += subTask.join(); &#125; &#125; return sum; &#125; public static void main(String[] args) throws InterruptedException, ExecutionException &#123; long forkStart = System.currentTimeMillis(); long endNumber = 99999L; ForkJoinPool forkJoinPool = new ForkJoinPool(); CountTask task = new CountTask(0, endNumber); ForkJoinTask&lt;Long&gt; result = forkJoinPool.submit(task); Long sum = result.get(); System.out.println(\"ForkJoinPool调用 sum=\" + sum); long forkEnd = System.currentTimeMillis(); System.out.println(\"ForkJoinPool调用 花费时间: \" + (forkEnd - forkStart)); sum = 0L; for (long i = 0; i &lt;= endNumber; i++) &#123; sum += i; &#125; System.out.println(\"正常调用 sum=\" + sum); System.out.println(\"正常调用花费时间: \" + (System.currentTimeMillis() - forkEnd)); &#125;&#125; 结果 1234ForkJoinPool调用 sum=4999950000ForkJoinPool调用 花费时间: 4正常调用 sum=4999950000正常调用花费时间: 5","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"Java资源大全","slug":"Java资源大全","date":"2017-05-06T12:55:01.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/05/06/Java资源大全/","link":"","permalink":"https://liyong.ac.cn/2017/05/06/Java资源大全/","excerpt":"https://github.com/shanxiliyong/awesome-java-cn","text":"https://github.com/shanxiliyong/awesome-java-cn 类别日志Apache Log4j 2：使用强大的插件和配置架构进行完全重写。官网SLF4J：日志抽象层，需要与具体的实现配合使用。官网 消息传递Apache Kafka：高吞吐量分布式消息系统。官网Apache ActiveMQ：实现JMS的开源消息代理（broker），可将同步通讯转为异步通讯。官网 构建工具构建及应用依赖关系处理工具。Apache Maven：Maven是一款声明式构建及依赖管理工具，采用约定优于配置方式进行管理。相对Gradle：使用Groovy（非XML）进行增量构建，可以很好地与Maven依赖管理配合工作。 字节码操作编程方式操作字节码的开发库。Javassist：一个简化字节码编辑尝试。官网 代码分析测量代码指标和质量工具。Checkstyle：代码编写规范和标准静态分析工具。FindBugs：通过字节码静态分析查找隐藏bug。PMD：对源代码分析查找不良的编程习惯。官网SonarQube：通过插件集成其它分析组件，对过去一段时间内的数据进行统计。官网 数据库简化数据库交互的相关工具。H2：小型SQL数据库，以可以作为内存数据库使用著称。 时间日期工具库处理时间和日期的开发库。Joda-Time：在Java 8发布前，Joda-Time是实际使用的时间日期库标准。 文档处理工具处理Office文档的开发库。Apache POI：支持OOXML规范（XLSX、DOCX、PPTX）以及OLE2规范（XLS、DOC、PPT）。官网documents4j：使用第三方转换器进行文档格式转换，转成类似MS Word这样的格式。官网jOpenDocument：处理OpenDocument格式（由Sun公司提出基于XML的文档格式）。官网docx4：解压的docx包(docx本身是zip包)和解析WordprocessingML格式XML的Java库 ##高性能计算涵盖了从集合到特定开发库的高性能计算相关工具。Disruptor：线程间消息传递开发库。官网 图像处理创建、评价和操作图片的支持库。Imgscalr：纯Java 2D实现，简单、高效、支持硬件加速的图像缩放开发库。官网Thumbnailator：Thumbnailator是一个高质量Java缩略图开发库。官网ZXing：支持多种格式的一维、二维条形码图片处理开发库。官网im4java: 基于ImageMagick或GraphicsMagick命令行的图片处理开发库，基本上ImageMagick能够支持的图片格式和处理方式都能够处理。官网 JSON简化JSON处理的开发库。Gson：谷歌官方推出的JSON处理库，支持在对象与JSON之间双向序列化，性能良好且可以实时调用。官网Jackson：与GSON类似，在频繁使用时性能更佳。官网Fastjson: 一个Java语言编写的高性能功能完善的JSON库。官网 开发流程增强工具从最基本的层面增强开发流程。ADT4J：针对代数数据类型的JSR-269代码生成器。官网AspectJ：面向切面编程（AOP）的无缝扩展。官网Auto：源代码生成器集合。官网DCEVM：通过修改JVM在运行时支持对已加载的类进行无限次重定义。官网HotswapAgent：支持无限次重定义运行时类与资源。官网Immutables：类似Scala的条件类。官网JHipster：基于Spring Boot与AngularJS应用程序的Yeoman源代码生成器。官网JRebel：无需重新部署，可以即时重新加载代码与配置的商业软件。官网Lombok：减少冗余的代码生成器。官网 分布式应用用来编写分布式容错应用的开发库和框架。Akka：用来编写分布式容错并发事件驱动应用程序的工具和运行时。官网Apache Storm：实时计算系统。官网Apache ZooKeeper：针对大型分布式系统的协调服务，支持分布式配置、同步和名称注册。官网 分布式数据库对应用程序而言，在分布式系统中的数据库看起来就像是只有一个数据源。Apache Cassandra：列式数据库，可用性高且没有单点故障。官网Apache HBase：针对大数据的Hadoop数据库。官网Druid：实时和历史OLAP数据存储，在聚集查询和近似查询方面表现不俗。官网Infinispan：针对缓存的高并发键值对数据存储。官网 机器学习Apache Hadoop：在商用硬件集群上用来进行大规模数据存储的开源软件框架。官网Apache Spark：开源数据分析集群计算框架。官网 古董级工具Apache Ant：基于XML的构建管理工具。cglib：字节码生成库GlassFish：应用服务器，由Oracle赞助支持的Java EE参考实现。Hudson：持续集成服务器，目前仍在活跃开发。 ##应用监控工具监控生产环境中应用程序的工具。JavaMelody：性能监测和分析工具。官网Kamon：Kamon用来监测在JVM上运行的应用程序。官网 网络网络编程函数库。Async Http Client：异步HTTP和WebSocket客户端函数库。官网Netty：构建高性能网络应用程序开发框架。官网OkHttp：一个Android和Java应用的HTTP+SPDY客户端。官网 ORM处理对象持久化的API。Hibernate：广泛使用、强健的持久化框架。Hibernate的技术社区非常活跃。官网MyBatis：带有存储过程或者SQL语句的耦合对象（Couples object）。官网 PDF用来帮助创建PDF文件的资源。Apache FOP：从XSL-FO创建PDF。官网Apache PDFBox：用来创建和操作PDF的工具集。官网DynamicReports：JasperReports的精简版。官网flyingsaucer：XML/XHTML和CSS 2.1渲染器。官网iText：一个易于使用的PDF函数库，用来编程创建PDF文件。注意，用于商业用途时需要许可证。官网JasperReports：一个复杂的报表引擎。官网 性能分析性能分析、性能剖析及基准测试工具。jHiccup：提供平台中JVM暂停的日志和记录。官网JMH：JVM基准测试工具。官网JProfiler：商业分析器。官网LatencyUtils：测量和报告延迟的工具。官网VisualVM：对运行中的应用程序信息提供了可视化界面。官网YourKit Java Profiler：商业分析器。官网 安全用于处理安全、认证、授权或会话管理的函数库。Apache Shiro：执行认证、授权、加密和会话管理。官网Bouncy Castle，涵盖了从基础的帮助函数到PGP/SMIME操作。官网：多途加密开发库。支持JCA提供者（JCA provider)Cryptomator：在云上进行客户端跨平台透明加密。官网Keycloak：为浏览器应用和RESTful Web Service集成SSO和IDM。目前还处于beta版本，但是看起来非常有前途。官网PicketLink：PicketLink是一个针对Java应用进行安全和身份认证管理的大型项目（Umbrella Project）。官网 应用服务器用来部署应用程序的服务器。Apache Tomcat：针对Servlet和JSP的应用服务器，健壮性好且适用性强。官网Jetty：轻量级、小巧的应用服务器，通常会嵌入到项目中。官网 模板引擎在模板中替换表达式的工具。Apache Velocity：提供HTML页面模板、email模板和通用开源代码生成器模板。官网FreeMarker：通用模板引擎，不需要任何重量级或自己使用的依赖关系。官网Thymeleaf：旨在替换JSP，支持XML文件的工具。官网 测试测试内容从对象到接口，涵盖性能测试和基准测试工具。Cucumber：BDD测试框架。官网JUnit：通用测试框架。官网PowerMock： 支持模拟静态方法、构造函数、final类和方法、私有方法以及移除静态初始化器的模拟工具。官网Selenium：为Web应用程序提供可移植软件测试框架。官网 通用工具库通用工具类函数库。Apache Commons：提供各种用途的函数，比如配置、验证、集合、文件上传或XML处理等。官网args4j：命令行参数解析器。官网CRaSH：为运行进行提供CLI。官网Guava：集合、缓存、支持基本类型、并发函数库、通用注解、字符串处理、I/O等。官网JADE：构建、调试多租户系统的框架和环境。官网Quartz：强大的任务调度库.官网 网络爬虫用于分析网站内容的函数库。Apache Nutch：可用于生产环境的高度可扩展、可伸缩的网络爬虫。官网Crawler4j：简单的轻量级网络爬虫。官网JSoup：刮取、解析、操作和清理HTML。官网","categories":[{"name":"Java资源大全","slug":"Java资源大全","permalink":"https://liyong.ac.cn/categories/Java资源大全/"}],"tags":[{"name":"Java资源大全","slug":"Java资源大全","permalink":"https://liyong.ac.cn/tags/Java资源大全/"}]},{"title":"大话UML","slug":"大话UML","date":"2017-04-22T08:15:13.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/22/大话UML/","link":"","permalink":"https://liyong.ac.cn/2017/04/22/大话UML/","excerpt":"结构型UML-Structure Diagram类图-Class Diagram对象图-Object Diagram(不常用)包图-Package Diagram构建图-Component Diagram部署图-Deployment Diagram 行为的UML-Behavior Diagram活动图-Activity Diagram状态机图-State Machine Diagram顺序图-Sequence Diagram用例图-Use Case Diagram通信图-Communication Diagram(不常用)时序图-Timing Diagram(不常用)","text":"结构型UML-Structure Diagram类图-Class Diagram对象图-Object Diagram(不常用)包图-Package Diagram构建图-Component Diagram部署图-Deployment Diagram 行为的UML-Behavior Diagram活动图-Activity Diagram状态机图-State Machine Diagram顺序图-Sequence Diagram用例图-Use Case Diagram通信图-Communication Diagram(不常用)时序图-Timing Diagram(不常用) 类图-Class Diagram类之间的关系1、直线1）普通关系2）一对一关系3）一对多关系,*表示0到多个4）一对零到三关系5）角色关系,+代表public -代表private6）导航关系，通过A能找到B 2、包含聚合-Aggregation-若包含-虚心棱组合-Composition-强包含-实心棱3、继承4、依赖5、递归关系6、三角关系 活动图-Activity Diagram基础语法开始、结束、判断、合并 泳道/分区1、开始、结束状态可以画在任意泳道2、判读、合并也可以画在任何泳道，但如果判断由某角色负责的，则应将这个判断的工作抽离为一个活动，将这个活动画在合适的泳道上 顺序图-Sequence Diagram 适合没有分支，没有循环的场景 基础语法1、角色2、生命线-LifeLine：角色或者对象下面的那条虚线，就是生命线3、激活框-Activation Box:激活框也叫会话框，就是生命线中的西高矩形4、消息-Message:1）指向别人2）指向自己3）返回值 用例图-Use Case Diagram1、用于分析不同的角色对需求的不同2、用于分析不同的测试场景 基本语法1、执行者（小人）2、用例-Use Case（圈圈）3、系统边界-System Boundary4、关联-Association5、Include1)以树的方式组织各种用例2）某些用例的一部分可以抽离出来成为子用例6、Extend 活动图和状态图的区别1、活动图将流程分解为一个个活动，通过活动的先后顺序来展示活动2、状态机图通过事务的的状态如何变化来展示流程 状态机图 构件图 部署图","categories":[{"name":"uml","slug":"uml","permalink":"https://liyong.ac.cn/categories/uml/"}],"tags":[{"name":"uml","slug":"uml","permalink":"https://liyong.ac.cn/tags/uml/"}]},{"title":"oracle常用语句","slug":"oracle常用语句","date":"2017-04-18T08:19:57.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/18/oracle常用语句/","link":"","permalink":"https://liyong.ac.cn/2017/04/18/oracle常用语句/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536--创建数据表空间create tablespace xxxxxx_data logging datafile 'D:\\02_tools\\oracle\\datafile\\xxxxxx_data01.dbf'size 512m autoextend on next 50m maxsize 2048m;--创建索引表空间create tablespace xxxxxx_idx logging datafile 'D:\\02_tools\\oracle\\datafile\\xxxxxx_idx01.dbf'size 512m autoextend on next 50m maxsize 2048m;--创建临时表空间create temporary tablespace xxxxxx_temp tempfile 'D:\\02_tools\\oracle\\datafile\\xxxxxx_temp01.dbf'size 512m autoextend on next 50m maxsize 2048m;--创建用户语句create user xxxxxx identified by xxxxxx default tablespace xxxxxx_DATA temporary tablespace xxxxxx_TEMP;--开发环境授权语句grant connect,resource,dba to xxxxxx;--创建dir语句create DIRECTORY dump_dir AS 'E:\\01_work\\dump_dir';--授权dir语句GRANT READ, WRITE ON DIRECTORY dump_dir TO xxxxxx;--导出语句expdp xxxxxx/xxxxxx@ORCL DIRECTORY=dump_dir DUMPFILE=xxxxxx_v1_1_05.DMP--导入dmpimpdp xxxxxx/xxxxxx@ORCL dumpfile=xxxxxx_V1_0_02.dmp directory=DUMP_DIR REMAP_SCHEMA=from_user:to_user remap_tablespace=from_tablespace:to_tablespace","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"},{"name":"oracle","slug":"oracle","permalink":"https://liyong.ac.cn/tags/oracle/"}]},{"title":"CyclicBarrier","slug":"aa_category/se/thread/CyclicBarrier","date":"2017-04-16T10:16:22.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/16/aa_category/se/thread/CyclicBarrier/","link":"","permalink":"https://liyong.ac.cn/2017/04/16/aa_category/se/thread/CyclicBarrier/","excerpt":"一个同步辅助类，它允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。在涉及一组固定大小的线程的程序中，这些线程必须不时地互相等待，此时 CyclicBarrier 很有用。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。","text":"一个同步辅助类，它允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。在涉及一组固定大小的线程的程序中，这些线程必须不时地互相等待，此时 CyclicBarrier 很有用。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package concurrent;import java.util.Random;import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;import java.util.concurrent.locks.ReentrantReadWriteLock;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-4-16 * @update 修改时间：2017-4-16 */public class CyclicBarrierDemo &#123; public static class Soldier implements Runnable &#123; private String soldierName; private CyclicBarrier barrier; private Soldier(String soldierName, CyclicBarrier barrier) &#123; super(); this.soldierName = soldierName; this.barrier = barrier; &#125; public void run() &#123; try &#123; barrier.await(); Thread.sleep(Math.abs(new Random().nextInt() % 1000)); System.out.println(\"士兵\" + soldierName + \"任务完成\"); barrier.await(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; public static class BarrierRun implements Runnable &#123; boolean flag = false; int N = 0; private BarrierRun(int n) &#123; super(); N = n; &#125; public void run() &#123; if (flag) &#123; System.out.println(N + \"个士兵完成任务\"); &#125; else &#123; System.out.println(N + \"个士兵集合完毕\"); flag = true; System.out.println(N + \"开始完成任务\"); &#125; &#125; &#125; public static void main(String[] args) &#123; int n = 10; BarrierRun barrierRun = new BarrierRun(n); CyclicBarrier barrier = new CyclicBarrier(n, barrierRun); System.out.println(\"开始集合队伍\"); ExecutorService service = Executors.newFixedThreadPool(n); for (int i = 0; i &lt; n; i++) &#123; service.submit(new Soldier(\"士兵 \" + i, barrier)); System.out.println(\"士兵 \" + i + \"集合完毕\"); &#125; &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"ReadWriteLock","slug":"aa_category/se/thread/ReadWriteLock","date":"2017-04-16T09:38:43.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/16/aa_category/se/thread/ReadWriteLock/","link":"","permalink":"https://liyong.ac.cn/2017/04/16/aa_category/se/thread/ReadWriteLock/","excerpt":"ReadWriteLock和ReentrantLock相比，在读多写少的场景下，性能有很多的提升 1、读读不互斥2、读写互斥3、写写互斥","text":"ReadWriteLock和ReentrantLock相比，在读多写少的场景下，性能有很多的提升 1、读读不互斥2、读写互斥3、写写互斥 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package concurrent;import java.util.Random;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;import java.util.concurrent.locks.ReentrantReadWriteLock;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-4-16 * @update 修改时间：2017-4-16 */public class ReadWriteLockDemo &#123; Lock lock = new ReentrantLock(); ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock(); Lock readLock = readWriteLock.readLock(); Lock writeLock = readWriteLock.writeLock(); int value = 0; public int read(Lock lock) &#123; try &#123; lock.lock(); Thread.sleep(1 * 1000); System.out.println(\"读到的值为 ：\" + value); return value; &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); return value; &#125; finally &#123; lock.unlock(); &#125; &#125; public void write(Lock lock, int value) &#123; try &#123; lock.lock(); Thread.sleep(1 * 1000); this.value = value; System.out.println(\"写入的值为 ：\" + value); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) &#123; final ReadWriteLockDemo demo = new ReadWriteLockDemo(); Runnable readRun = new Runnable() &#123; public void run() &#123; // Lock lock = demo.lock; Lock lock = demo.readLock; int value = demo.read(lock); &#125; &#125;; Runnable writeRun = new Runnable() &#123; public void run() &#123; // Lock lock = demo.lock; Lock lock = demo.writeLock; demo.write(lock, new Random().nextInt()); &#125; &#125;; for (int i = 0; i &lt; 20; i++) &#123; if (i &gt; 1) &#123; new Thread(readRun, \"\" + i).start(); &#125; else &#123; new Thread(writeRun, \"\" + i).start(); &#125; &#125; &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"CountDownLatch","slug":"aa_category/se/thread/CountDownLatch","date":"2017-04-16T09:00:09.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/16/aa_category/se/thread/CountDownLatch/","link":"","permalink":"https://liyong.ac.cn/2017/04/16/aa_category/se/thread/CountDownLatch/","excerpt":"一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。用给定的计数 初始化 CountDownLatch。由于调用了 countDown() 方法，所以在当前计数到达零之前，await 方法会一直受阻塞。之后，会释放所有等待的线程，await 的所有后续调用都将立即返回。这种现象只出现一次——计数无法被重置","text":"一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。用给定的计数 初始化 CountDownLatch。由于调用了 countDown() 方法，所以在当前计数到达零之前，await 方法会一直受阻塞。之后，会释放所有等待的线程，await 的所有后续调用都将立即返回。这种现象只出现一次——计数无法被重置 12345678910111213141516171819202122232425262728293031323334353637383940414243package concurrent;import java.util.Random;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-4-16 * @update 修改时间：2017-4-16 */public class CountDownLatchDemo implements Runnable &#123; static final CountDownLatch latch = new CountDownLatch(10); public void run() &#123; try &#123; System.out.println(\"thread name is \" + Thread.currentThread().getName()); Thread.sleep(2 * 1000); latch.countDown(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; ExecutorService exec = Executors.newFixedThreadPool(20); CountDownLatchDemo demo = new CountDownLatchDemo(); for (int i = 0; i &lt; 20; i++) &#123; exec.submit(demo); &#125; // 等待检查 latch.await(); System.out.println(\"火箭发射\"); &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"Semaphore","slug":"Semaphore","date":"2017-04-16T08:31:01.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/16/Semaphore/","link":"","permalink":"https://liyong.ac.cn/2017/04/16/Semaphore/","excerpt":"Semaphore实现的功能就类似厕所有5个坑，假如有10个人要上厕所，那么同时只能有多少个人去上厕所呢？同时只能有5个人能够占用，当5个人中 的任何一个人让开后，其中等待的另外5个人中又有一个人可以占用了。另外等待的5个人中可以是随机获得优先机会，也可以是按照先来后到的顺序获得机会，这取决于构造Semaphore对象时传入的参数选项。","text":"Semaphore实现的功能就类似厕所有5个坑，假如有10个人要上厕所，那么同时只能有多少个人去上厕所呢？同时只能有5个人能够占用，当5个人中 的任何一个人让开后，其中等待的另外5个人中又有一个人可以占用了。另外等待的5个人中可以是随机获得优先机会，也可以是按照先来后到的顺序获得机会，这取决于构造Semaphore对象时传入的参数选项。 123456789101112131415161718192021222324252627282930313233343536373839package concurrent;import java.util.concurrent.Semaphore;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-4-16 * @update 修改时间：2017-4-16 */public class SemaPhoreDemo implements Runnable &#123; Semaphore se = new Semaphore(5); public void run() &#123; try &#123; se.acquire(); System.out.println(Thread.currentThread().getName()); Thread.sleep(4 * 1000); se.release(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; SemaPhoreDemo demo = new SemaPhoreDemo(); for (int i = 0; i &lt; 20; i++) &#123; Thread t = new Thread(demo, \"\" + i); t.start(); &#125; &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"Condition","slug":"aa_category/se/thread/Condition","date":"2017-04-16T07:32:44.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/16/aa_category/se/thread/Condition/","link":"","permalink":"https://liyong.ac.cn/2017/04/16/aa_category/se/thread/Condition/","excerpt":"1、Condition 重入锁的好搭档2、调用方法 await 和 signal 之前必须获持有相关的锁，调用之后必须释放锁。","text":"1、Condition 重入锁的好搭档2、调用方法 await 和 signal 之前必须获持有相关的锁，调用之后必须释放锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package concurrent;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.ReentrantLock;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-4-16 * @update 修改时间：2017-4-16 */public class ConditionDemo implements Runnable &#123; ReentrantLock lock = new ReentrantLock(); Condition condition = lock.newCondition(); public void run() &#123; try &#123; lock.lock(); System.out.println(Thread.currentThread().getName() + \" lock.lock();\"); condition.await(); Thread.sleep(1 * 1000); System.out.println(Thread.currentThread().getName() + \" lock.lock();\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; ConditionDemo test = new ConditionDemo(); Thread t1 = new Thread(test, \"aaaaaa\"); Thread t2 = new Thread(test, \"bbbbbb\"); t1.start(); t2.start(); Thread.sleep(2 * 1000); test.lock.lock(); test.condition.signal(); //如果不释放锁则，被唤醒的线程由于无法获取锁而导致无法继续工作 test.lock.unlock(); t1.join(); t2.join(); &#125;&#125; JDKArrayBlockingQueue的部分源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package concurrent;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.ReentrantLock;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-4-16 * @update 修改时间：2017-4-16 */public class ArrayBlockingQueue&lt;E&gt; &#123; /** Main lock guarding all access */ private final ReentrantLock lock = new ReentrantLock(); /** Condition for waiting takes */ private final Condition notEmpty = lock.newCondition(); /** Condition for waiting puts */ private final Condition notFull = lock.newCondition(); /** * Inserts the specified element at the tail of this queue, waiting * for space to become available if the queue is full. * * @throws InterruptedException * &#123;@inheritDoc&#125; * @throws NullPointerException * &#123;@inheritDoc&#125; */ public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; try &#123; while (count == items.length) notFull.await(); &#125; catch (InterruptedException ie) &#123; notFull.signal(); // propagate to non-interrupted thread throw ie; &#125; // notEmpty.signal(); insert(e); &#125; finally &#123; lock.unlock(); &#125; &#125; public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; try &#123; while (count == 0) notEmpty.await(); &#125; catch (InterruptedException ie) &#123; notEmpty.signal(); // propagate to non-interrupted thread throw ie; &#125; // notFull.signal(); E x = extract(); return x; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"},{"name":"lock","slug":"lock","permalink":"https://liyong.ac.cn/tags/lock/"}]},{"title":"LockSupport","slug":"aa_category/se/thread/LockSupport","date":"2017-04-16T03:40:08.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/16/aa_category/se/thread/LockSupport/","link":"","permalink":"https://liyong.ac.cn/2017/04/16/aa_category/se/thread/LockSupport/","excerpt":"1、先执行unpark，后执行park不会导致线程永久挂起2、可以响应中断3、LockSupport采用类似信号量的机制，它为每个线程准备一个许可，执行park的时候，如果许可可用，则立即返回，如果许可不可用，则会阻塞线程。执行unpark则可让许可可用，多次执行upark不能累加(一个线程只能有一个许可)","text":"1、先执行unpark，后执行park不会导致线程永久挂起2、可以响应中断3、LockSupport采用类似信号量的机制，它为每个线程准备一个许可，执行park的时候，如果许可可用，则立即返回，如果许可不可用，则会阻塞线程。执行unpark则可让许可可用，多次执行upark不能累加(一个线程只能有一个许可) demo,证明先执行unpark，后执行park不会导致线程永久挂起12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.concurrent.locks.LockSupport;import java.util.concurrent.locks.ReentrantLock;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-4-16 * @update 修改时间：2017-4-16 */public class LockSupportDemo1 implements Runnable &#123; public void run() &#123; try &#123; Thread.sleep(4 * 1000); System.out.println(Thread.currentThread().getName() + \" park\"); LockSupport.park(this); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; LockSupportDemo1 test = new LockSupportDemo1(); Thread t1 = new Thread(test, \" aaaaaa\"); Thread t2 = new Thread(test, \" bbbbbb\"); t1.start(); t2.start(); LockSupport.unpark(t1); Thread.sleep(10 * 1000); LockSupport.unpark(t2); System.out.println(\"has unpark\"); t1.join(); t2.join(); &#125;&#125; demo 可以响应中断1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.concurrent.locks.LockSupport;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-4-16 * @update 修改时间：2017-4-16 */public class LockSupportDemo2 implements Runnable &#123; public void run() &#123; try &#123; Thread.sleep(2 * 1000); System.out.println(Thread.currentThread().getName() + \" park\"); LockSupport.park(this); if (Thread.interrupted()) &#123; System.out.println(\"线程\" + Thread.currentThread().getName() + \"被中断了\"); &#125; else &#123; System.out.println(\"线程\" + Thread.currentThread().getName() + \"结束了\"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; LockSupportDemo2 test = new LockSupportDemo2(); Thread t1 = new Thread(test, \"aaaaaa\"); Thread t2 = new Thread(test, \"bbbbbb\"); t1.start(); t2.start(); LockSupport.unpark(t1); System.out.println(\"线程\" + t1.getName() + \" has unpark\"); Thread.sleep(4 * 1000); t2.interrupt(); System.out.println(\"线程 \" + t2.getName() + \" 被中断了\"); t1.join(); t2.join(); &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"},{"name":"lock","slug":"lock","permalink":"https://liyong.ac.cn/tags/lock/"}]},{"title":"指令重排序","slug":"aa_category/se/thread/指令重排序","date":"2017-04-15T14:59:18.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/15/aa_category/se/thread/指令重排序/","link":"","permalink":"https://liyong.ac.cn/2017/04/15/aa_category/se/thread/指令重排序/","excerpt":"1、对于一个线程来说，它看到的指令执行顺序是一致的。2、线程writer的指令执行顺序在现场reader看来是没有保证的3、下面的例子可能会出现flag=true的时候a=2还未执行，至于是否发生了指令重排序，如何重排序我们是无法预测的。","text":"1、对于一个线程来说，它看到的指令执行顺序是一致的。2、线程writer的指令执行顺序在现场reader看来是没有保证的3、下面的例子可能会出现flag=true的时候a=2还未执行，至于是否发生了指令重排序，如何重排序我们是无法预测的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package concurrent;/** * @description 类描述：指令重排序 * @author 作者：LIYONG * @create 创建时间：2017-4-16 * @update 修改时间：2017-4-16 */public class OrderExample implements Runnable &#123; static int a = 0; static boolean flag = false; int state = 0; private OrderExample(int state) &#123; super(); this.state = state; &#125; public void writer() &#123; a = 2; flag = true; &#125; public void reader() &#123; if (flag) &#123; a = 10 / a; &#125; &#125; public void run() &#123; if (state == 0) &#123; writer(); &#125; else &#123; reader(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; while (true) &#123; OrderExample writerOrder = new OrderExample(0); OrderExample readerOrder = new OrderExample(1); Thread writer = new Thread(writerOrder, \"writer\"); Thread reader = new Thread(readerOrder, \"reader\"); writer.start(); reader.start(); writer.join(); reader.join(); Thread.sleep(2 * 1000); &#125; &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"},{"name":"性能调优","slug":"性能调优","permalink":"https://liyong.ac.cn/tags/性能调优/"}]},{"title":"jstack","slug":"jstack","date":"2017-04-15T14:42:30.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/15/jstack/","link":"","permalink":"https://liyong.ac.cn/2017/04/15/jstack/","excerpt":"jstack主要用来查看某个Java进程内的线程堆栈信息。语法格式如下： 123jstack [option] pidjstack [option] executable corejstack [option] [server-id@]remote-hostname-or-ip 命令行参数选项说明如下： 1-l long listings，会打印出额外的锁信息，在发生死锁时可以用jstack -l pid来观察锁 1、jstack定位到堆栈信息2、根据堆栈信息可以堆栈信息可以定位到具体代码，所以在JVM性能调优中使用的的相当多","text":"jstack主要用来查看某个Java进程内的线程堆栈信息。语法格式如下： 123jstack [option] pidjstack [option] executable corejstack [option] [server-id@]remote-hostname-or-ip 命令行参数选项说明如下： 1-l long listings，会打印出额外的锁信息，在发生死锁时可以用jstack -l pid来观察锁 1、jstack定位到堆栈信息2、根据堆栈信息可以堆栈信息可以定位到具体代码，所以在JVM性能调优中使用的的相当多 Java代码查看堆栈信息123456789101112131415161718public static void main(String[] args) throws InterruptedException &#123; Map&lt;Thread, StackTraceElement[]&gt; traces = Thread.getAllStackTraces(); for (Map.Entry&lt;Thread, StackTraceElement[]&gt; trace : traces.entrySet()) &#123; Thread thread = trace.getKey(); if (thread.equals(Thread.currentThread())) &#123; continue; &#125; StringBuilder sb = new StringBuilder(); sb.append(\"线程 \" + thread.getName() + \"\\r\\n\"); for (StackTraceElement element : trace.getValue()) &#123; sb.append(\"--\" + element + \"\\r\\n\"); &#125; System.out.println(sb); &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"性能调优","slug":"性能调优","permalink":"https://liyong.ac.cn/tags/性能调优/"}]},{"title":"ThreadGoup","slug":"aa_category/se/thread/ThreadGoup","date":"2017-04-15T14:28:02.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/15/aa_category/se/thread/ThreadGoup/","link":"","permalink":"https://liyong.ac.cn/2017/04/15/aa_category/se/thread/ThreadGoup/","excerpt":"分门别类的管理，线程组","text":"分门别类的管理，线程组 DEMO12345678910111213141516171819202122232425262728293031323334353637package concurrent;/** * @description 类描述： * @author 作者：LIYONG * @create 创建时间：2017-4-14 * @update 修改时间：2017-4-14 */public class ThreadGoupTest implements Runnable &#123; public void run() &#123; String groupName = Thread.currentThread().getThreadGroup().getName()+\"&amp;&amp;\"+Thread.currentThread().getName(); while(true)&#123; System.out.println(groupName); try &#123; Thread.sleep(4*1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; ThreadGroup group = new ThreadGroup(\"group\"); ThreadGoupTest test = new ThreadGoupTest(); Thread t1 = new Thread(group, test, \"aaaaaa\"); t1.setPriority(10); t1.start(); Thread t2 = new Thread(group, test, \"bbbbbb\"); t2.setPriority(1); t2.start(); &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"ReentrantLock","slug":"aa_category/se/thread/ReentrantLock","date":"2017-04-15T14:16:28.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/15/aa_category/se/thread/ReentrantLock/","link":"","permalink":"https://liyong.ac.cn/2017/04/15/aa_category/se/thread/ReentrantLock/","excerpt":"ReentrantLock比 synchronized的优势1、RenntrantLock性能远远好于synchronized，但从JDK 1.6 开始，JDK在synchronized做了大量的优化，使得两者性能差距并不大2、RenntrantLock支持中断响应3、ReentrantLock支持指定限时等待锁4、ReenTrantLock支持公平锁 ReentrantLock实现三要素 原子性，使用cas保证原子性 等待队列，没有请求道锁的线程会进入等待队列，如果某个线程释放锁，则从等待队列中唤醒一个线程，继续工作 阻塞 park,恢复unpark","text":"ReentrantLock比 synchronized的优势1、RenntrantLock性能远远好于synchronized，但从JDK 1.6 开始，JDK在synchronized做了大量的优化，使得两者性能差距并不大2、RenntrantLock支持中断响应3、ReentrantLock支持指定限时等待锁4、ReenTrantLock支持公平锁 ReentrantLock实现三要素 原子性，使用cas保证原子性 等待队列，没有请求道锁的线程会进入等待队列，如果某个线程释放锁，则从等待队列中唤醒一个线程，继续工作 阻塞 park,恢复unpark 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package concurrent;import java.util.Map;import java.util.concurrent.locks.ReentrantLock;public class ReentrantLockTest2 implements Runnable &#123; static ReentrantLock lock1 = new ReentrantLock(); static ReentrantLock lock2 = new ReentrantLock(); int lock = 0; private ReentrantLockTest2(int lock) &#123; super(); this.lock = lock; &#125; public void run() &#123; if (lock == 1) &#123; try &#123; lock1.lockInterruptibly(); Thread.sleep(1 * 1000); System.out.println(\"lock 1\"); lock2.lockInterruptibly(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; if (lock1.isHeldByCurrentThread()) &#123; lock1.unlock(); &#125; if (lock2.isHeldByCurrentThread()) &#123; lock2.unlock(); &#125; &#125; &#125; else &#123; try &#123; lock2.lockInterruptibly(); Thread.sleep(1 * 1000); System.out.println(\"lock 0\"); lock1.lockInterruptibly(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; if (lock1.isHeldByCurrentThread()) &#123; lock1.unlock(); &#125; if (lock2.isHeldByCurrentThread()) &#123; lock2.unlock(); &#125; &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; ReentrantLockTest2 intLock1 = new ReentrantLockTest2(1); ReentrantLockTest2 intLock0 = new ReentrantLockTest2(0); Thread t1 = new Thread(intLock0, \"intLock0\"); Thread t2 = new Thread(intLock1, \"intLock1\"); t1.start(); t2.start(); Thread.sleep(1000); t2.interrupt(); &#125;&#125;","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"},{"name":"lock","slug":"lock","permalink":"https://liyong.ac.cn/tags/lock/"}]},{"title":"Oracle分区相关表","slug":"Oracle分区相关表","date":"2017-04-06T06:09:16.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/04/06/Oracle分区相关表/","link":"","permalink":"https://liyong.ac.cn/2017/04/06/Oracle分区相关表/","excerpt":"","text":"12345678910 --显示当前用户所有分区表的信息：select * from USER_PART_TABLES; --显示当前用户所有分区表的详细分区信息：select * from USER_TAB_PARTITIONS;--显示当前用户所有组合分区表的子分区信息：select * from USER_TAB_SUBPARTITIONS;--显示当前用户所有分区表的分区列信息：select * from USER_PART_KEY_COLUMNS;--怎样查询出oracle数据库中所有的的分区表;select * from user_tables a where a.partitioned='YES'","categories":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/categories/db/"}],"tags":[{"name":"db","slug":"db","permalink":"https://liyong.ac.cn/tags/db/"},{"name":"oracle","slug":"oracle","permalink":"https://liyong.ac.cn/tags/oracle/"}]},{"title":"设计模式总结","slug":"设计模式总结","date":"2017-03-10T08:44:20.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/03/10/设计模式总结/","link":"","permalink":"https://liyong.ac.cn/2017/03/10/设计模式总结/","excerpt":"先看如下两幅图片 设计模式之间的关系：","text":"先看如下两幅图片 设计模式之间的关系： 设计模式总概况： 一、设计原则1、单一职责原则一个类，只有一个引起它变化的原因。应该只有一个职责。每一个职责都是变化的一个轴线，如果一个类有一个以上的职责，这些职责就耦合在了一起。这会导致脆弱的设计。当一个职责发生变化时，可能会影响其它的职责。另外，多个职责耦合在一起，会影响复用性。例如：要实现逻辑和界面的分离。from：百度百科 2、开闭原则（Open Close Principle）开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类，后面的具体设计中我们会提到这点。 3、里氏代换原则（Liskov Substitution Principle）里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。from：百度百科 4、依赖倒转原则（Dependence Inversion Principle）所谓依赖倒置原则（Dependence Inversion Principle）就是要依赖于抽象，不要依赖于具体。简单的说就是要求对抽象进行编程，不要对实现进行编程，这样就降低了客户与实现模块间的耦合。 实现开闭原则的关键是抽象化，并且从抽象化导出具体化实现，如果说开闭原则是面向对象设计的目标的话，那么依赖倒转原则就是面向对象设计的主要手段。 from：百度百科 5、接口隔离原则（Interface Segregation Principle）这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。还是一个降低类之间的耦合度的意思，从这儿我们看出，其实设计模式就是一个软件的设计思想，从大型软件架构出发，为了升级和维护方便。所以上文中多次出现：降低依赖，降低耦合。 6、合成复用原则（Composite Reuse Principle）合成复用原则就是指在一个新的对象里通过关联关系（包括组合关系和聚合关系）来使用一些已有的对象，使之成为新对象的一部分；新对象通过委派调用已有对象的方法达到复用其已有功能的目的。简言之：要尽量使用组合/聚合关系，少用继承。 7、迪米特法则（最少知道原则）（Demeter Principle）为什么叫最少知道原则，就是说：一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。也就是说一个软件实体应当尽可能少的与其他实体发生相互作用。这样，当一个模块修改时，就会尽量少的影响其他的模块，扩展会相对容易，这是对软件实体之间通信的限制，它要求限制软件实体之间通信的宽度和深度。 适配器模式、代理模式、装饰模式 适配器模式，一个适配允许通常因为接口不兼容而不能在一起工作的类工作在一起，做法是将类自己的接口包裹在一个已存在的类中。 装饰器模式，原有的不能满足现有的需求，对原有的进行增强 代理模式，同一个类而去调用另一个类的方法，不对这个方法进行直接操作，起到隔离效果 代理模式一定是自身持有这个对象，不需要从外部传入。而装饰模式的一定是从外部传入，并且可以没有顺序，按照代码的实际需求随意挑换顺序，就如你吃火锅先放白菜还是先放丸子都可以","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://liyong.ac.cn/tags/设计模式/"}]},{"title":"如何提高Java并行程序性能","slug":"aa_category/se/thread/如何提高Java并行程序性能","date":"2017-03-10T01:19:24.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/03/10/aa_category/se/thread/如何提高Java并行程序性能/","link":"","permalink":"https://liyong.ac.cn/2017/03/10/aa_category/se/thread/如何提高Java并行程序性能/","excerpt":"在Java程序中，多线程几乎已经无处不在。与单线程相比，多线程程序的设计和实现略微困难，但通过多线程，我们却可以获得多核CPU带来的性能飞跃，从这个角度说，多线程是一种值得尝试的技术。那么如何写出高效的多线程程序呢？","text":"在Java程序中，多线程几乎已经无处不在。与单线程相比，多线程程序的设计和实现略微困难，但通过多线程，我们却可以获得多核CPU带来的性能飞跃，从这个角度说，多线程是一种值得尝试的技术。那么如何写出高效的多线程程序呢？ 有关多线程的误区：线程越多，性能越好不少初学者可能认为，线程数量越多，那么性能应该越好。因为程序给我们的直观感受总是这样。一个两个线程可能跑的很难，线程一多可能就快了。但事实并非如此。因为一个物理CPU一次只能执行一个线程，多个线程则意味着必须进行线程的上下文切换，而这个代价是很高的。因此，线程数量必须适量，最好的情况应该是N个CPU使用N个线程，并且让每个CPU的占有率都达到100%，这种情况下，系统的吞吐量才发挥到极致。但现实中，不太可能让单线程独占CPU达到100%，一个普遍的愿意是因为IO操作，无论是磁盘IO还是网络IO都是很慢的。线程在执行中会等待，因此效率就下来了。这也就是为什么在一个物理核上执行多个线程会感觉效率高了，对于程序调度来说，一个线程等待时，也正是其它线程执行的大好机会，因此，CPU资源得到了充分的利用。 尽可能不要挂起线程多线程程序免不了要同步，最直接的方法就是使用锁。每次只允许一个线程进入临界区，让其它相关线程等待。等待有2种，一种是直接使用操作系统指令挂起线程，另外一种是自旋等待。在操作系统直接挂起，是一种简单粗暴的实现，性能较差，不太适用于高并发的场景，因为随之而来的问题就是大量的线程上下文切换。如果可以，尝试一下进行有限的自旋等待，等待不成功再去挂起线程也不迟。这样很有可能可以避免一些无谓的开销。JDK中ConcurrentHashMap的实现里就有一些自旋等待的实现。此外Java虚拟机层面，对synchronized关键字也有自旋等待的优化。 善用“无锁”阻塞线程会带来性能开销，因此，一种提供性能的方案就是使用无锁的CAS操作。JDK中的原子类，如AtomicInteger正是使用了这种方案。在高并发环境中，冲突较多的情况下，它们的性能远远好于传统的锁操作（《实战Java高并发程序设计》 P158）。 处理好“伪共享”问题大家知道，CPU有一个高速缓存Cache。在Cache中，读写数据的最小单位是缓存行，如果2个变量存在一个缓存行中，那么在多线程访问中，可能会相互影响彼此的性能。因此将变量存放于独立的缓存行中，也有助于变量在多线程访问是的性能提升（《实战Java高并发程序设计》 P200），大量的高并发库都会采用这种技术。","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"concurrent","slug":"concurrent","permalink":"https://liyong.ac.cn/tags/concurrent/"}]},{"title":"Java中字符串内存位置浅析","slug":"Java中字符串内存位置浅析","date":"2017-03-09T01:45:24.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/03/09/Java中字符串内存位置浅析/","link":"","permalink":"https://liyong.ac.cn/2017/03/09/Java中字符串内存位置浅析/","excerpt":"前言之前面试被问，Java里面String存放的位置，之前只记得String是一个不变的量，应该是要存放在常量池里面的，但是后来问到new一个String出来应该是放到哪里的，这个应该是放到堆里面的，后来又问到String的引用是放在什么地方的，当时傻逼的说也是放在堆里面的，现在总结一下：基本类型的变量数据和对象的引用都是放在栈里面的，对象本身放在堆里面，显式的String常量放在常量池，String对象放在堆中。","text":"前言之前面试被问，Java里面String存放的位置，之前只记得String是一个不变的量，应该是要存放在常量池里面的，但是后来问到new一个String出来应该是放到哪里的，这个应该是放到堆里面的，后来又问到String的引用是放在什么地方的，当时傻逼的说也是放在堆里面的，现在总结一下：基本类型的变量数据和对象的引用都是放在栈里面的，对象本身放在堆里面，显式的String常量放在常量池，String对象放在堆中。 常量池的说明常量池之前是放在方法区里面的，也就是在永久代里面的，从JDK7开始移到了堆里面。这一改变我们可以从oracle的release version的notes里的 Important RFEs Addressed in JDK 7 看到。 1234567891011Area: HotSpotSynopsis: In JDK 7, interned strings are no longer allocated in the permanent generation of the Java heap, but are instead allocated in the main part of the Java heap (known as the young and old generations), along with the other objects created by the application. This change will result in more data residing in the main Java heap, and less data in the permanent generation, and thus may require heap sizes to be adjusted. Most applications will see only relatively small differences in heap usage due to this change, but larger applications that load many classes or make heavy use of the String.intern() method will see more significant differences.RFE: 6962931 String内存位置说明1.显式的String常量 String a = “holten”;String b = “holten”;第一句代码执行后就在常量池中创建了一个值为holten的String对象；第二句执行时，因为常量池中存在holten所以就不再创建新的String对象了。此时该字符串的引用在虚拟机栈里面。2.String对象 String a = new String(“holtenObj”);String b = new String(“holtenObj”);Class被加载时就在常量池中创建了一个值为holtenObj的String对象，第一句执行时会在堆里创建new String(“holtenObj”)对象；第二句执行时，因为常量池中存在holtenObj所以就不再创建新的String对象了，直接在堆里创建new String(“holtenObj”)对象。 验证一下1234567891011public class Main &#123; public static void main(String[] args)&#123; String str1 = \"高小天\"; String str2 = \"高小天\"; System.out.println(str1==str2);//true String str3 = new String(\"高大天\"); String str4 = new String(\"高大天\"); System.out.println(str3==str4);//false &#125;&#125; 返回结果： 12truefalse","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"memory","slug":"memory","permalink":"https://liyong.ac.cn/tags/memory/"}]},{"title":"JVM内存区域划分","slug":"JVM内存区域划分","date":"2017-03-09T01:11:19.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/03/09/JVM内存区域划分/","link":"","permalink":"https://liyong.ac.cn/2017/03/09/JVM内存区域划分/","excerpt":"学习Java不可避免的要学习Java虚拟机，也就是JVM，Java虚拟机规范里面规定了程序运行期间会使用到的运行时数据区，这也是JVM的内存区域划分的模型，应该可以这么理解吧。 这其中有一些是随着虚拟机的启动和退出而创建和销毁的，这些区域是线程共享的，另外有一些是跟各个线程绑定的，随着线程的开始和结束而创建和销毁，我们可以称这些区域是线程私有的。 根据《Java虚拟机规范Java SE 7版》和《Java虚拟机规范Java SE 8版》的规定，我们可以划分出如下几个运行时数据区，如下图所示：","text":"学习Java不可避免的要学习Java虚拟机，也就是JVM，Java虚拟机规范里面规定了程序运行期间会使用到的运行时数据区，这也是JVM的内存区域划分的模型，应该可以这么理解吧。 这其中有一些是随着虚拟机的启动和退出而创建和销毁的，这些区域是线程共享的，另外有一些是跟各个线程绑定的，随着线程的开始和结束而创建和销毁，我们可以称这些区域是线程私有的。 根据《Java虚拟机规范Java SE 7版》和《Java虚拟机规范Java SE 8版》的规定，我们可以划分出如下几个运行时数据区，如下图所示： Java堆在Java虚拟机中，堆是可供各个线程共享的运行时内存区域，也是供所有类实例和数组对象分配内存的区域。这块区域随着虚拟机的启动而创建，它的唯一使命就是存放对象实例，这块区域也是GC主要关注的地方。 下面的就是我的笔记本上的JVM堆的划分情况，可以看到分为新生代、老年代和永久代，新生代里面有可以分为Eden Space、From Survivor Space和To Survivor Space。 123456789101112131415161718192021222324252627Heap Usage:PS Young GenerationEden Space: capacity = 17301504 (16.5MB) used = 2483088 (2.3680572509765625MB) free = 14818416 (14.131942749023438MB) 14.351862127130682% usedFrom Space: capacity = 2621440 (2.5MB) used = 2615312 (2.4941558837890625MB) free = 6128 (0.0058441162109375MB) 99.7662353515625% usedTo Space: capacity = 6291456 (6.0MB) used = 0 (0.0MB) free = 6291456 (6.0MB) 0.0% usedPS Old Generation capacity = 44564480 (42.5MB) used = 13316368 (12.699478149414062MB) free = 31248112 (29.800521850585938MB) 29.88112505744485% usedPS Perm Generation capacity = 22020096 (21.0MB) used = 14907008 (14.2164306640625MB) free = 7113088 (6.7835693359375MB) 67.6972888764881% used 根据虚拟机规范的规定，Java堆可以是固定的大小也可以是按照需求动态扩展的，而且不需要保证是连续的。 存放内容：所有的对象实例和数组。 方法区方法区是一个线程共享的区域，它用于存储已被虚拟机加载的类信息、常量、静态变量。方法区是堆的逻辑组成部分，Hotspot用永久代实现了方法区。方法区还包含运行时常量池(JDK1.7以后移到堆中)，用于存放编译时生成的各种字面量和符号引用，但是不要求常量一定是在编译时期产生的，运行期间也可以将新的常量放入池中，比如String的intern()方法便是利用了这一特性。 存放内容：类的结构信息，如类的字段、方法、接口、构造函数，还有运行时常量池等。 程序计数寄存器这块区域是每个线程独立拥有的，也就是线程私有的，我们可以把它看作是当前线程所执行的字节码的行号指示器。 这块区域时虚拟机规范里面唯一一个没有规定任何OutOfMemoryError情况的区域。 存放内容：如果线程执行的是一个Java方法，那么寄存器里面记录的就是正在执行的虚拟机字节码指令的地址，如果线程执行的是一个native方法，那么寄存器记录的值为undefined。 虚拟机栈虚拟机栈也是线程私有的内存区域。每个方法在执行的时候都会创建一个栈帧用于存储局部变量表、操作数栈、方法出口等信息，每一个方法从调用到执行完成就是一个栈帧入栈和出栈的过程。 局部变量表存放了编译时期可知的各种基本数据类型、对象引用和指向了一条字节码指令的地址。 存放内容：局部变量表、操作数栈、方法出口等信息。 本地方法栈和虚拟机栈类似，存储Native方法的相关信息。存放内容：局部变量表、操作数栈、方法出口等信息","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"memory","slug":"memory","permalink":"https://liyong.ac.cn/tags/memory/"}]},{"title":"工厂方法模式","slug":"工厂方法模式","date":"2017-03-03T07:43:38.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/03/03/工厂方法模式/","link":"","permalink":"https://liyong.ac.cn/2017/03/03/工厂方法模式/","excerpt":"普通工厂模式 就是建立一个工厂类，对实现了同一接口的一些类进行实例的创建。首先看下关系图：","text":"普通工厂模式 就是建立一个工厂类，对实现了同一接口的一些类进行实例的创建。首先看下关系图： 举例如下：（我们举一个发送邮件和短信的例子） 首先，创建二者的共同接口： 123public interface Sender &#123; public void Send(); &#125; 其次，创建实现类： 1234567891011121314public class MailSender implements Sender &#123; @Override public void Send() &#123; System.out.println(\"this is mailsender!\"); &#125; &#125;public class SmsSender implements Sender &#123; @Override public void Send() &#123; System.out.println(\"this is sms sender!\"); &#125; &#125; 最后，建工厂类： 12345678910111213public class SendFactory &#123; public Sender produce(String type) &#123; if (\"mail\".equals(type)) &#123; return new MailSender(); &#125; else if (\"sms\".equals(type)) &#123; return new SmsSender(); &#125; else &#123; System.out.println(\"请输入正确的类型!\"); return null; &#125; &#125; &#125; 我们来测试下： 12345678public class FactoryTest &#123; public static void main(String[] args) &#123; SendFactory factory = new SendFactory(); Sender sender = factory.produce(\"sms\"); sender.Send(); &#125; &#125; 输出：this is sms sender! 多个工厂方法模式 是对普通工厂方法模式的改进，在普通工厂方法模式中，如果传递的字符串出错，则不能正确创建对象，而多个工厂方法模式是提供多个工厂方法，分别创建对象。关系图： 将上面的代码做下修改，改动下SendFactory类就行，如下： 12345678public Sender produceMail()&#123; return new MailSender(); &#125; public Sender produceSms()&#123; return new SmsSender(); &#125; &#125; 测试类如下： 12345678public class FactoryTest &#123; public static void main(String[] args) &#123; SendFactory factory = new SendFactory(); Sender sender = factory.produceMail(); sender.Send(); &#125; &#125; 输出：this is mailsender! 静态工厂方法模式 将上面的多个工厂方法模式里的方法置为静态的，不需要创建实例，直接调用即可。 12345678910public class SendFactory &#123; public static Sender produceMail()&#123; return new MailSender(); &#125; public static Sender produceSms()&#123; return new SmsSender(); &#125; &#125; 1234567public class FactoryTest &#123; public static void main(String[] args) &#123; Sender sender = SendFactory.produceMail(); sender.Send(); &#125; &#125; 输出：this is mailsender! 总体来说，工厂模式适合：凡是出现了大量的产品需要创建，并且具有共同的接口时，可以通过工厂方法模式进行创建。在以上的三种模式中，第一种如果传入的字符串有误，不能正确创建对象，第三种相对于第二种，不需要实例化工厂类，所以，大多数情况下，我们会选用第三种——静态工厂方法模式。","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://liyong.ac.cn/tags/设计模式/"}]},{"title":"KeyTool","slug":"KeyTool","date":"2017-03-01T11:52:38.000Z","updated":"2019-09-10T11:57:17.000Z","comments":true,"path":"2017/03/01/KeyTool/","link":"","permalink":"https://liyong.ac.cn/2017/03/01/KeyTool/","excerpt":"","text":"1keytool -genkey -alias www.lee.com -keyalg RSA -keystore f:/keys/lee 1keytool -export -file f:/keys/lee.crt -alias www.lee.com -keystore f:/keys/lee 123keytool -import -keystore %JAVA_HOME%\\jre\\lib\\security\\cacerts -file f:/keys/mag.crt -alias www.lee.comkeytool -list -v -alias www.lee.com -keystore %JAVA_HOME%\\jre\\lib\\security\\cacertsKeytool –delete –alias s1as –keystore keystopre.jks –storepass changeit","categories":[{"name":"se","slug":"se","permalink":"https://liyong.ac.cn/categories/se/"}],"tags":[{"name":"plugins","slug":"plugins","permalink":"https://liyong.ac.cn/tags/plugins/"}]}]}